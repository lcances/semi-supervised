{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1703.01780.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/lcances/.miniconda3/envs/dct/bin/python'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, dotdict\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import JensenShanon\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"esc10\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.003, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_s = parser.add_argument_group(\"Student teacher parameters\")\n",
    "group_s.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_s.add_argument(\"--warmup_length\", default=50, type=int)\n",
    "group_s.add_argument(\"--lambda_cost_max\", default=1, type=float)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"mean-teacher\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"mean-teacher\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args=parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'batch_size': 64,\n 'checkpoint_path': 'student-teacher',\n 'checkpoint_root': '../../model_save/',\n 'dataset': 'esc10',\n 'dataset_root': '../../datasets',\n 'ema_alpha': 0.999,\n 'from_config': '',\n 'lambda_cost_max': 1,\n 'learning_rate': 0.003,\n 'model': 'wideresnet28_2',\n 'nb_epoch': 200,\n 'num_classes': 10,\n 'resume': False,\n 'seed': 1234,\n 'supervised_ratio': 0.1,\n 'tensorboard_path': 'student-teacher',\n 'tensorboard_root': '../../tensorboard/',\n 'tensorboard_sufix': '',\n 'train_folds': [1, 2, 3, 4],\n 'val_folds': [5],\n 'warmup_length': 50}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): MelSpectrogram(\n",
       "    (spectrogram): Spectrogram()\n",
       "    (mel_scale): MelScale()\n",
       "  )\n",
       "  (1): AmplitudeToDB()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"mean-teacher\")\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already downloaded and verified.\n",
      "Dataset already downloaded and verified.\n",
      "s_batch_size:  6\n",
      "u_batch_size:  58\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"mean-teacher\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(64, 431)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "input_shape = tuple(train_loader._iterables[0].dataset[0][0].shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "student = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "teacher = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "\n",
    "student = student.cuda()\n",
    "teacher = teacher.cuda()\n",
    "\n",
    "# We do not need gradient for the teacher model\n",
    "for p in teacher.parameters():\n",
    "    p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 32, 64, 431]             864\n       BatchNorm2d-2          [-1, 32, 64, 431]              64\n              ReLU-3          [-1, 32, 64, 431]               0\n         MaxPool2d-4          [-1, 32, 32, 216]               0\n            Conv2d-5          [-1, 32, 32, 216]           9,216\n       BatchNorm2d-6          [-1, 32, 32, 216]              64\n              ReLU-7          [-1, 32, 32, 216]               0\n            Conv2d-8          [-1, 32, 32, 216]           9,216\n       BatchNorm2d-9          [-1, 32, 32, 216]              64\n             ReLU-10          [-1, 32, 32, 216]               0\n       BasicBlock-11          [-1, 32, 32, 216]               0\n           Conv2d-12          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-13          [-1, 32, 32, 216]              64\n             ReLU-14          [-1, 32, 32, 216]               0\n           Conv2d-15          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-16          [-1, 32, 32, 216]              64\n             ReLU-17          [-1, 32, 32, 216]               0\n       BasicBlock-18          [-1, 32, 32, 216]               0\n           Conv2d-19          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-20          [-1, 32, 32, 216]              64\n             ReLU-21          [-1, 32, 32, 216]               0\n           Conv2d-22          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-23          [-1, 32, 32, 216]              64\n             ReLU-24          [-1, 32, 32, 216]               0\n       BasicBlock-25          [-1, 32, 32, 216]               0\n           Conv2d-26          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-27          [-1, 32, 32, 216]              64\n             ReLU-28          [-1, 32, 32, 216]               0\n           Conv2d-29          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-30          [-1, 32, 32, 216]              64\n             ReLU-31          [-1, 32, 32, 216]               0\n       BasicBlock-32          [-1, 32, 32, 216]               0\n           Conv2d-33          [-1, 64, 16, 108]          18,432\n      BatchNorm2d-34          [-1, 64, 16, 108]             128\n             ReLU-35          [-1, 64, 16, 108]               0\n           Conv2d-36          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-37          [-1, 64, 16, 108]             128\n           Conv2d-38          [-1, 64, 16, 108]           2,048\n      BatchNorm2d-39          [-1, 64, 16, 108]             128\n             ReLU-40          [-1, 64, 16, 108]               0\n       BasicBlock-41          [-1, 64, 16, 108]               0\n           Conv2d-42          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-43          [-1, 64, 16, 108]             128\n             ReLU-44          [-1, 64, 16, 108]               0\n           Conv2d-45          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-46          [-1, 64, 16, 108]             128\n             ReLU-47          [-1, 64, 16, 108]               0\n       BasicBlock-48          [-1, 64, 16, 108]               0\n           Conv2d-49          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-50          [-1, 64, 16, 108]             128\n             ReLU-51          [-1, 64, 16, 108]               0\n           Conv2d-52          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-53          [-1, 64, 16, 108]             128\n             ReLU-54          [-1, 64, 16, 108]               0\n       BasicBlock-55          [-1, 64, 16, 108]               0\n           Conv2d-56          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-57          [-1, 64, 16, 108]             128\n             ReLU-58          [-1, 64, 16, 108]               0\n           Conv2d-59          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-60          [-1, 64, 16, 108]             128\n             ReLU-61          [-1, 64, 16, 108]               0\n       BasicBlock-62          [-1, 64, 16, 108]               0\n           Conv2d-63           [-1, 128, 8, 54]          73,728\n      BatchNorm2d-64           [-1, 128, 8, 54]             256\n             ReLU-65           [-1, 128, 8, 54]               0\n           Conv2d-66           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-67           [-1, 128, 8, 54]             256\n           Conv2d-68           [-1, 128, 8, 54]           8,192\n      BatchNorm2d-69           [-1, 128, 8, 54]             256\n             ReLU-70           [-1, 128, 8, 54]               0\n       BasicBlock-71           [-1, 128, 8, 54]               0\n           Conv2d-72           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-73           [-1, 128, 8, 54]             256\n             ReLU-74           [-1, 128, 8, 54]               0\n           Conv2d-75           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-76           [-1, 128, 8, 54]             256\n             ReLU-77           [-1, 128, 8, 54]               0\n       BasicBlock-78           [-1, 128, 8, 54]               0\n           Conv2d-79           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-80           [-1, 128, 8, 54]             256\n             ReLU-81           [-1, 128, 8, 54]               0\n           Conv2d-82           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-83           [-1, 128, 8, 54]             256\n             ReLU-84           [-1, 128, 8, 54]               0\n       BasicBlock-85           [-1, 128, 8, 54]               0\n           Conv2d-86           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-87           [-1, 128, 8, 54]             256\n             ReLU-88           [-1, 128, 8, 54]               0\n           Conv2d-89           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-90           [-1, 128, 8, 54]             256\n             ReLU-91           [-1, 128, 8, 54]               0\n       BasicBlock-92           [-1, 128, 8, 54]               0\nAdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n           Linear-94                   [-1, 10]           1,290\n================================================================\nTotal params: 1,472,554\nTrainable params: 1,472,554\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.11\nForward/backward pass size (MB): 107.11\nParams size (MB): 5.62\nEstimated Total Size (MB): 112.83\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(student, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../../tensorboard/esc10/student-teacher/wideresnet28_2/0.1S/2020-10-30_22:07:06_wideresnet28_2_JS\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__)\n",
    "tensorboard_title = \"%s/%sS/%s_%s_JS\" % title_element\n",
    "\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__)\n",
    "checkpoint_title = \"%s/%sS/%s_%s_JS\" % title_element\n",
    "\n",
    "tensorboard = mSummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(args.dataset, \"mean-teacher\", student=student, teacher=teacher)\n",
    "callbacks = load_callbacks(args.dataset, \"mean-teacher\", optimizer=optimizer, nb_epoch=args.nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\") # Supervised loss\n",
    "consistency_cost = nn.MSELoss(reduction=\"mean\") # Unsupervised loss\n",
    "consistency_cost = JensenShanon\n",
    "\n",
    "lambda_cost = Warmup(args.lambda_cost_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cost]\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(student, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculator():\n",
    "    def c(logits, y):\n",
    "        with torch.no_grad():\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            arg = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            acc = c.fn.acc(arg, y).mean\n",
    "            f1 = c.fn.f1(pred, y_one_hot).mean\n",
    "            \n",
    "            return acc, f1,\n",
    "            \n",
    "    c.fn = dotdict(\n",
    "        acc = CategoricalAccuracy(),\n",
    "        f1 = FScore(),\n",
    "    )\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_student_s_metrics = metrics_calculator()\n",
    "calc_student_u_metrics = metrics_calculator()\n",
    "calc_teacher_s_metrics = metrics_calculator()\n",
    "calc_teacher_u_metrics = metrics_calculator()\n",
    "\n",
    "avg_Sce = ContinueAverage()\n",
    "avg_Tce = ContinueAverage()\n",
    "avg_ccost = ContinueAverage()\n",
    "\n",
    "softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "def reset_metrics():\n",
    "    for d in [calc_student_s_metrics.fn, calc_student_u_metrics.fn, calc_teacher_s_metrics.fn, calc_teacher_u_metrics.fn]:\n",
    "        for fn in d.values():\n",
    "            fn.reset()\n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} | {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} - {:<8.6}\"\n",
    "value_form  = \"{:<8.8} {:<6d} - {:<6d} - {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} | {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} - {:<8.4f}\"\n",
    "header = header_form.format(\".               \", \"Epoch\",  \"%\", \"Student:\", \"ce\", \"ccost\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\", \"Teacher:\", \"ce\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\" , \"Time\")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "    \n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "    \n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data,  alpha = 1-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    reset_metrics()\n",
    "    student.train()\n",
    "\n",
    "    for i, (S, U) in enumerate(train_loader):        \n",
    "        x_s, y_s = S\n",
    "        x_u, y_u = U\n",
    "        \n",
    "        x_s, x_u = x_s.cuda(), x_u.cuda()\n",
    "        y_s, y_u = y_s.cuda(), y_u.cuda()\n",
    "        \n",
    "        # Predictions\n",
    "        student_s_logits = student(x_s)        \n",
    "        student_u_logits = student(x_u)\n",
    "        teacher_s_logits = teacher(x_s)\n",
    "        teacher_u_logits = teacher(x_u)\n",
    "        \n",
    "        # Calculate supervised loss (only student on S)\n",
    "        loss = loss_ce(student_s_logits, y_s)\n",
    "        \n",
    "        # Calculate consistency cost (mse(student(x), teacher(x))) x is S + U\n",
    "        student_logits = torch.cat((student_s_logits, student_u_logits), dim=0)\n",
    "        teacher_logits = torch.cat((teacher_s_logits, teacher_u_logits), dim=0)\n",
    "        ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "\n",
    "        total_loss = loss + lambda_cost() * ccost\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # Teacher prediction (for metrics purpose)\n",
    "            _teacher_loss = loss_ce(teacher_s_logits, y_s)\n",
    "            \n",
    "            # Update teacher\n",
    "            update_teacher_model(student, teacher, args.ema_alpha, epoch*nb_batch + i)\n",
    "            \n",
    "            # Compute the metrics for the student\n",
    "            student_s_metrics = calc_student_s_metrics(student_s_logits, y_s)\n",
    "            student_u_metrics = calc_student_u_metrics(student_u_logits, y_u)\n",
    "            student_s_acc, student_u_acc, student_s_f1, student_u_f1 = *student_s_metrics, *student_u_metrics\n",
    "            \n",
    "            # Compute the metrics for the teacher\n",
    "            teacher_s_metrics = calc_teacher_s_metrics(teacher_s_logits, y_s)\n",
    "            teacher_u_metrics = calc_teacher_u_metrics(teacher_u_logits, y_u)\n",
    "            teacher_s_acc, teacher_u_acc, teacher_s_f1, teacher_u_f1 = *teacher_s_metrics, *teacher_u_metrics\n",
    "            \n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \", epoch + 1, int(100 * (i + 1) / nb_batch),\n",
    "                \"\", student_running_loss, running_ccost, *student_s_metrics, *student_u_metrics,\n",
    "                \"\", teacher_running_loss, *teacher_s_metrics, *teacher_u_metrics,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_acc_s\", student_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_acc_u\", student_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_s\", student_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_u\", student_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/teacher_acc_s\", teacher_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_u\", teacher_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_s\", teacher_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_u\", teacher_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/consistency_cost\", running_ccost, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    student.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Predictions\n",
    "            student_logits = student(X)        \n",
    "            teacher_logits = teacher(X)\n",
    "\n",
    "            # Calculate supervised loss (only student on S)\n",
    "            loss = loss_ce(student_logits, y)\n",
    "            _teacher_loss = loss_ce(teacher_logits, y) # for metrics only\n",
    "            ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "            \n",
    "            # Compute the metrics\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            # ---- student ----\n",
    "            student_metrics = calc_student_s_metrics(student_logits, y)\n",
    "            student_acc, student_f1 = student_metrics\n",
    "            \n",
    "            # ---- teacher ----\n",
    "            teacher_metrics = calc_teacher_s_metrics(teacher_logits, y)\n",
    "            teacher_acc, teacher_f1 = teacher_metrics\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \", epoch + 1, int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", student_running_loss, running_ccost, *student_metrics, 0.0, 0.0,\n",
    "                \"\", teacher_running_loss, *teacher_metrics, 0.0, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/student_acc\", student_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_f1\", student_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_acc\", teacher_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_f1\", teacher_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/consistency_cost\", running_ccost, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    tensorboard.add_scalar(\"hyperparameters/lambda_cost_max\", lambda_cost(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/student_acc\", maximum_tracker(\"student_acc\", student_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_acc\", maximum_tracker(\"teacher_acc\", teacher_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/student_f1\", maximum_tracker(\"student_f1\", student_f1), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_f1\", maximum_tracker(\"teacher_f1\", teacher_f1), epoch )\n",
    "\n",
    "    checkpoint.step(teacher_acc)\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9667   0.6345   0.6417   |            0.8710   0.9000   0.9152   0.6069   0.6035   - 1.5835  \n",
      "\u001b[1;4mValidati 138    - 100    -            2.3323   0.0143   0.5938   0.5913   0.0000   0.0000   |            0.8720   0.5703   0.5806   0.0000   0.0000   - 0.1065  \u001b[0m\n",
      "Training 139    - 100    -            2.3203   0.0143   1.0000   1.0000   0.6379   0.6370   |            0.8682   1.0000   0.9636   0.6241   0.6242   - 1.5825  \n",
      "\u001b[1;4mValidati 139    - 100    -            2.3190   0.0143   0.5938   0.5730   0.0000   0.0000   |            0.8690   0.6094   0.6012   0.0000   0.0000   - 0.1059  \u001b[0m\n",
      "Training 140    - 100    -            2.3072   0.0143   1.0000   1.0000   0.6621   0.6634   |            0.8650   1.0000   0.9818   0.6414   0.6187   - 1.5932  \n",
      "\u001b[1;4mValidati 140    - 100    -            2.3058   0.0143   0.6172   0.6220   0.0000   0.0000   |            0.8660   0.5234   0.5155   0.0000   0.0000   - 0.1083  \u001b[0m\n",
      "Training 141    - 100    -            2.2943   0.0142   1.0000   1.0000   0.6517   0.6562   |            0.8622   1.0000   0.9818   0.6276   0.6318   - 1.5905  \n",
      "\u001b[1;4mValidati 141    - 100    -            2.2921   0.0142   0.6875   0.6920   0.0000   0.0000   |            0.8622   0.7344   0.7271   0.0000   0.0000   - 0.1058  \u001b[0m\n",
      "Training 142    - 100    -            2.2806   0.0142   1.0000   1.0000   0.6655   0.6582   |            0.8584   0.9667   0.9667   0.6103   0.6173   - 1.5817  \n",
      "\u001b[1;4mValidati 142    - 100    -            2.2791   0.0142   0.5703   0.5754   0.0000   0.0000   |            0.8595   0.6016   0.6032   0.0000   0.0000   - 0.1059  \u001b[0m\n",
      "Training 143    - 100    -            2.2678   0.0142   1.0000   1.0000   0.6759   0.6703   |            0.8557   1.0000   1.0000   0.6069   0.6073   - 1.5816  \n",
      "\u001b[1;4mValidati 143    - 100    -            2.2658   0.0141   0.6641   0.6784   0.0000   0.0000   |            0.8560   0.7031   0.6960   0.0000   0.0000   - 0.1061  \u001b[0m\n",
      "Training 144    - 100    -            2.2545   0.0141   1.0000   1.0000   0.6552   0.6534   |            0.8522   1.0000   0.9818   0.6138   0.6124   - 1.5834  \n",
      "\u001b[1;4mValidati 144    - 100    -            2.2538   0.0141   0.5938   0.5998   0.0000   0.0000   |            0.8531   0.6484   0.6252   0.0000   0.0000   - 0.1059  \u001b[0m\n",
      "Training 145    - 100    -            2.2432   0.0141   0.9667   0.9818   0.6586   0.6555   |            0.8501   0.9333   0.9333   0.6172   0.6156   - 1.5813  \n",
      "\u001b[1;4mValidati 145    - 100    -            2.2415   0.0141   0.6406   0.6492   0.0000   0.0000   |            0.8505   0.6641   0.6635   0.0000   0.0000   - 0.1058  \u001b[0m\n",
      "Training 146    - 100    -            2.2307   0.0140   1.0000   1.0000   0.6483   0.6431   |            0.8469   0.9667   0.9667   0.6207   0.6229   - 1.5806  \n",
      "\n",
      "Training 147    - 100    -            2.2194   0.0140   1.0000   1.0000   0.6483   0.6477   |            0.8446   0.9667   0.9667   0.6448   0.6269   - 1.5814  \n",
      "\u001b[1;4mValidati 147    - 100    -            2.2183   0.0140   0.5938   0.6083   0.0000   0.0000   |            0.8453   0.6250   0.6290   0.0000   0.0000   - 0.1063  \u001b[0m\n",
      "Training 148    - 100    -            2.2077   0.0140   1.0000   1.0000   0.6414   0.6479   |            0.8420   0.9333   0.9333   0.6172   0.6091   - 1.5824  \n",
      "\u001b[1;4mValidati 148    - 100    -            2.2068   0.0140   0.6094   0.6165   0.0000   0.0000   |            0.8430   0.5156   0.5323   0.0000   0.0000   - 0.1061  \u001b[0m\n",
      "Training 149    - 100    -            2.1962   0.0139   1.0000   1.0000   0.6586   0.6570   |            0.8394   0.9667   0.9818   0.6276   0.6262   - 1.5827  \n",
      "\u001b[1;4mValidati 149    - 100    -            2.1952   0.0139   0.6719   0.6766   0.0000   0.0000   |            0.8399   0.7031   0.6895   0.0000   0.0000   - 0.1062  \u001b[0m\n",
      "Training 150    - 100    -            2.1848   0.0139   1.0000   1.0000   0.6483   0.6480   |            0.8364   1.0000   1.0000   0.6103   0.6190   - 1.5834  \n",
      "\u001b[1;4mValidati 150    - 100    -            2.1852   0.0139   0.5234   0.5288   0.0000   0.0000   |            0.8379   0.6016   0.5858   0.0000   0.0000   - 0.1055  \u001b[0m\n",
      "Training 151    - 100    -            2.1749   0.0138   1.0000   1.0000   0.6241   0.6279   |            0.8346   0.9667   0.9485   0.6207   0.6303   - 1.5842  \n",
      "\u001b[1;4mValidati 151    - 100    -            2.1738   0.0138   0.6016   0.6066   0.0000   0.0000   |            0.8355   0.6094   0.6065   0.0000   0.0000   - 0.1061  \u001b[0m\n",
      "Training 152    - 100    -            2.1637   0.0138   1.0000   1.0000   0.6552   0.6367   |            0.8324   0.9667   0.9636   0.6138   0.6148   - 1.5850  \n",
      "\u001b[1;4mValidati 152    - 100    -            2.1628   0.0138   0.6328   0.6454   0.0000   0.0000   |            0.8333   0.6328   0.6397   0.0000   0.0000   - 0.1061  \u001b[0m\n",
      "Training 153    - 100    -            2.1527   0.0138   1.0000   1.0000   0.6724   0.6730   |            0.8297   1.0000   1.0000   0.6414   0.6461   - 1.5835  \n",
      "\u001b[1;4mValidati 153    - 100    -            2.1522   0.0138   0.6094   0.6281   0.0000   0.0000   |            0.8306   0.6094   0.6329   0.0000   0.0000   - 0.1060  \u001b[0m\n",
      "Training 154    - 100    -            2.1427   0.0137   0.9667   0.9667   0.6276   0.6343   |            0.8284   0.9333   0.9018   0.6241   0.6144   - 1.5882  \n",
      "\u001b[1;4mValidati 154    - 100    -            2.1416   0.0137   0.6406   0.6508   0.0000   0.0000   |            0.8285   0.7344   0.7524   0.0000   0.0000   - 0.1069  \u001b[0m\n",
      "Training 155    - 100    -            2.1318   0.0137   1.0000   1.0000   0.6103   0.6172   |            0.8252   1.0000   0.9818   0.6103   0.6075   - 1.5836  \n",
      "\u001b[1;4mValidati 155    - 100    -            2.1309   0.0137   0.6250   0.6283   0.0000   0.0000   |            0.8259   0.6328   0.6296   0.0000   0.0000   - 0.1059  \u001b[0m\n",
      "Training 156    - 100    -            2.1214   0.0137   0.9667   0.9818   0.6517   0.6464   |            0.8226   0.9667   0.9667   0.6276   0.6277   - 1.5854  \n",
      "\u001b[1;4mValidati 156    - 100    -            2.1206   0.0136   0.6562   0.6527   0.0000   0.0000   |            0.8233   0.6484   0.6527   0.0000   0.0000   - 0.1060  \u001b[0m\n",
      "Training 157    - 100    -            2.1112   0.0136   1.0000   1.0000   0.6138   0.6166   |            0.8204   0.9333   0.9485   0.6069   0.6021   - 1.5856  \n",
      "\u001b[1;4mValidati 157    - 100    -            2.1105   0.0136   0.6172   0.6245   0.0000   0.0000   |            0.8215   0.6250   0.5772   0.0000   0.0000   - 0.1057  \u001b[0m\n",
      "Training 158    - 100    -            2.1010   0.0136   1.0000   1.0000   0.6276   0.6286   |            0.8181   1.0000   1.0000   0.6172   0.6151   - 1.5835  \n",
      "\u001b[1;4mValidati 158    - 100    -            2.1004   0.0136   0.6250   0.6375   0.0000   0.0000   |            0.8192   0.6094   0.6157   0.0000   0.0000   - 0.1058  \u001b[0m\n",
      "Training 159    - 100    -            2.0911   0.0135   1.0000   1.0000   0.6552   0.6543   |            0.8163   1.0000   0.9818   0.6138   0.6099   - 1.5871  \n",
      "\u001b[1;4mValidati 159    - 100    -            2.0898   0.0135   0.7188   0.7255   0.0000   0.0000   |            0.8168   0.7109   0.7234   0.0000   0.0000   - 0.1060  \u001b[0m\n",
      "Training 160    - 100    -            2.0805   0.0135   1.0000   1.0000   0.6138   0.6207   |            0.8134   1.0000   1.0000   0.6138   0.6124   - 1.5850  \n",
      "\u001b[1;4mValidati 160    - 100    -            2.0805   0.0135   0.5781   0.5947   0.0000   0.0000   |            0.8144   0.6250   0.6146   0.0000   0.0000   - 0.1059  \u001b[0m\n",
      "Training 161    - 100    -            2.0714   0.0134   1.0000   1.0000   0.6414   0.6548   |            0.8114   0.9667   0.9818   0.6241   0.6216   - 1.5807  \n",
      "\u001b[1;4mValidati 161    - 100    -            2.0705   0.0134   0.6797   0.6790   0.0000   0.0000   |            0.8118   0.7266   0.7258   0.0000   0.0000   - 0.1058  \u001b[0m\n",
      "Training 162    - 100    -            2.0614   0.0134   1.0000   1.0000   0.6586   0.6628   |            0.8089   0.9333   0.9485   0.6414   0.6374   - 1.5993  \n",
      "\u001b[1;4mValidati 162    - 100    -            2.0606   0.0134   0.6328   0.6228   0.0000   0.0000   |            0.8096   0.6094   0.6183   0.0000   0.0000   - 0.1077  \u001b[0m\n",
      "Training 163    - 100    -            2.0517   0.0133   1.0000   1.0000   0.6345   0.6368   |            0.8064   0.9667   0.9667   0.6103   0.6050   - 1.5831  \n",
      "\u001b[1;4mValidati 163    - 100    -            2.0509   0.0133   0.6875   0.6895   0.0000   0.0000   |            0.8070   0.6484   0.6330   0.0000   0.0000   - 0.1061  \u001b[0m\n",
      "Training 164    - 100    -            2.0421   0.0133   1.0000   1.0000   0.6172   0.6203   |            0.8044   0.9000   0.9152   0.6103   0.6070   - 1.5831  \n",
      "\u001b[1;4mValidati 164    - 100    -            2.0415   0.0133   0.6641   0.6587   0.0000   0.0000   |            0.8053   0.6328   0.6478   0.0000   0.0000   - 0.1054  \u001b[0m\n",
      "Training 165    - 100    -            2.0327   0.0133   1.0000   1.0000   0.6517   0.6513   |            0.8021   1.0000   1.0000   0.6345   0.6381   - 1.5822  \n",
      "\u001b[1;4mValidati 165    - 100    -            2.0318   0.0132   0.7109   0.6991   0.0000   0.0000   |            0.8024   0.7422   0.7302   0.0000   0.0000   - 0.1081  \u001b[0m\n",
      "Training 166    - 100    -            2.0231   0.0132   1.0000   1.0000   0.6310   0.6337   |            0.7992   1.0000   1.0000   0.6310   0.6281   - 1.5862  \n",
      "\n",
      "Training 167    - 100    -            2.0151   0.0132   0.9667   0.9667   0.6586   0.6471   |            0.7989   0.9000   0.9000   0.6276   0.6230   - 1.5878  \n",
      "\u001b[1;4mValidati 167    - 100    -            2.0149   0.0131   0.5391   0.5164   0.0000   0.0000   |            0.8003   0.6250   0.6159   0.0000   0.0000   - 0.1055  \u001b[0m\n",
      "Training 168    - 100    -            2.0065   0.0131   1.0000   1.0000   0.6241   0.6324   |            0.7977   1.0000   0.9636   0.6172   0.6194   - 1.5851  \n",
      "\u001b[1;4mValidati 168    - 100    -            2.0058   0.0131   0.5781   0.5773   0.0000   0.0000   |            0.7986   0.5859   0.5833   0.0000   0.0000   - 0.1060  \u001b[0m\n",
      "Training 169    - 100    -            1.9975   0.0131   1.0000   0.9818   0.6207   0.6272   |            0.7961   0.9333   0.9303   0.6241   0.6226   - 1.5787  \n",
      "\u001b[1;4mValidati 169    - 100    -            1.9974   0.0131   0.5625   0.5563   0.0000   0.0000   |            0.7970   0.6172   0.6000   0.0000   0.0000   - 0.1057  \u001b[0m\n",
      "Training 170    - 100    -            1.9892   0.0130   1.0000   1.0000   0.6414   0.6397   |            0.7949   0.9333   0.9333   0.6276   0.6172   - 1.6142  \n",
      "\u001b[1;4mValidati 170    - 100    -            1.9888   0.0130   0.5938   0.6026   0.0000   0.0000   |            0.7959   0.6641   0.6558   0.0000   0.0000   - 0.1081  \u001b[0m\n",
      "Training 171    - 100    -            1.9807   0.0130   1.0000   1.0000   0.6379   0.6351   |            0.7944   0.9000   0.8539   0.6241   0.6178   - 1.5886  \n",
      "\u001b[1;4mValidati 171    - 100    -            1.9805   0.0130   0.6172   0.6071   0.0000   0.0000   |            0.7952   0.6406   0.6405   0.0000   0.0000   - 0.1067  \u001b[0m\n",
      "Training 172    - 100    -            1.9723   0.0130   1.0000   1.0000   0.6138   0.6165   |            0.7923   1.0000   1.0000   0.6379   0.6275   - 1.5858  \n",
      "\u001b[1;4mValidati 172    - 100    -            1.9722   0.0129   0.5703   0.5806   0.0000   0.0000   |            0.7932   0.6328   0.6263   0.0000   0.0000   - 0.1059  \u001b[0m\n",
      "Training 173    - 100    -            1.9641   0.0129   1.0000   1.0000   0.6310   0.6232   |            0.7902   1.0000   1.0000   0.6138   0.6100   - 1.5814  \n",
      "\u001b[1;4mValidati 173    - 100    -            1.9634   0.0129   0.6406   0.6242   0.0000   0.0000   |            0.7911   0.6250   0.6155   0.0000   0.0000   - 0.1063  \u001b[0m\n",
      "Training 174    - 100    -            1.9554   0.0129   1.0000   1.0000   0.6448   0.6449   |            0.7883   1.0000   0.9818   0.6276   0.6250   - 1.5858  \n",
      "\u001b[1;4mValidati 174    - 100    -            1.9546   0.0129   0.6406   0.6398   0.0000   0.0000   |            0.7891   0.6562   0.6668   0.0000   0.0000   - 0.1057  \u001b[0m\n",
      "Training 175    - 100    -            1.9467   0.0128   1.0000   1.0000   0.6310   0.6270   |            0.7862   0.9667   0.9667   0.6138   0.6067   - 1.5863  \n",
      "\u001b[1;4mValidati 175    - 100    -            1.9466   0.0128   0.5703   0.5781   0.0000   0.0000   |            0.7873   0.6562   0.6486   0.0000   0.0000   - 0.1061  \u001b[0m\n",
      "Training 176    - 100    -            1.9388   0.0128   1.0000   1.0000   0.6621   0.6504   |            0.7848   0.9333   0.9636   0.6310   0.6296   - 1.5925  \n",
      "\n",
      "Training 177    - 100    -            1.9307   0.0127   1.0000   1.0000   0.6379   0.6362   |            0.7839   0.9333   0.9273   0.6310   0.6286   - 1.5865  \n",
      "\u001b[1;4mValidati 177    - 100    -            1.9302   0.0127   0.6328   0.6210   0.0000   0.0000   |            0.7845   0.6875   0.6900   0.0000   0.0000   - 0.1047  \u001b[0m\n",
      "Training 178    - 100    -            1.9225   0.0127   1.0000   1.0000   0.6483   0.6467   |            0.7819   0.9667   0.9667   0.6379   0.6383   - 1.5729  \n",
      "\u001b[1;4mValidati 178    - 100    -            1.9221   0.0127   0.6406   0.6398   0.0000   0.0000   |            0.7826   0.6953   0.6950   0.0000   0.0000   - 0.1043  \u001b[0m\n",
      "Training 179    - 100    -            1.9144   0.0127   1.0000   1.0000   0.6414   0.6508   |            0.7797   1.0000   1.0000   0.6310   0.6327   - 1.5738  \n",
      "\u001b[1;4mValidati 179    - 100    -            1.9140   0.0127   0.6406   0.6453   0.0000   0.0000   |            0.7804   0.6719   0.6506   0.0000   0.0000   - 0.1047  \u001b[0m\n",
      "Training 180    - 100    -            1.9064   0.0126   1.0000   1.0000   0.6207   0.6179   |            0.7776   0.9667   0.9667   0.6276   0.6194   - 1.6838  \n",
      "\u001b[1;4mValidati 180    - 100    -            1.9054   0.0126   0.6641   0.6603   0.0000   0.0000   |            0.7781   0.6875   0.6766   0.0000   0.0000   - 0.1126  \u001b[0m\n",
      "Training 181    - 100    -            1.8979   0.0126   1.0000   1.0000   0.6379   0.6314   |            0.7753   1.0000   1.0000   0.6241   0.6225   - 1.6131  \n",
      "\u001b[1;4mValidati 181    - 100    -            1.8979   0.0126   0.6172   0.6023   0.0000   0.0000   |            0.7764   0.6328   0.6146   0.0000   0.0000   - 0.1055  \u001b[0m\n",
      "Training 182    - 100    -            1.8905   0.0125   1.0000   1.0000   0.6276   0.6319   |            0.7735   1.0000   1.0000   0.6276   0.6291   - 1.5707  \n",
      "\u001b[1;4mValidati 182    - 100    -            1.8901   0.0125   0.6406   0.6218   0.0000   0.0000   |            0.7743   0.7031   0.6719   0.0000   0.0000   - 0.1045  \u001b[0m\n",
      "Training 183    - 100    -            1.8828   0.0125   1.0000   1.0000   0.6241   0.6199   |            0.7719   0.9333   0.9333   0.6069   0.6127   - 1.5740  \n",
      "\u001b[1;4mValidati 183    - 100    -            1.8821   0.0125   0.6641   0.6728   0.0000   0.0000   |            0.7725   0.6875   0.6870   0.0000   0.0000   - 0.1050  \u001b[0m\n",
      "Training 184    - 100    -            1.8748   0.0125   1.0000   1.0000   0.6483   0.6466   |            0.7697   1.0000   1.0000   0.6241   0.6274   - 1.5757  \n",
      "\u001b[1;4mValidati 184    - 100    -            1.8747   0.0125   0.6172   0.6245   0.0000   0.0000   |            0.7708   0.6875   0.6171   0.0000   0.0000   - 0.1045  \u001b[0m\n",
      "Training 185    - 100    -            1.8676   0.0124   1.0000   1.0000   0.6138   0.6265   |            0.7682   1.0000   1.0000   0.6000   0.6082   - 1.5731  \n",
      "\u001b[1;4mValidati 185    - 100    -            1.8672   0.0124   0.6172   0.6245   0.0000   0.0000   |            0.7690   0.6641   0.6666   0.0000   0.0000   - 0.1047  \u001b[0m\n",
      "Training 186    - 100    -            1.8602   0.0124   1.0000   1.0000   0.6345   0.6352   |            0.7668   0.9333   0.9152   0.6172   0.6185   - 1.5749  \n",
      "\u001b[1;4mValidati 186    - 100    -            1.8600   0.0124   0.5938   0.5987   0.0000   0.0000   |            0.7678   0.6172   0.6225   0.0000   0.0000   - 0.1048  \u001b[0m\n",
      "Training 187    - 100    -            1.8529   0.0124   1.0000   1.0000   0.6414   0.6477   |            0.7652   1.0000   1.0000   0.6310   0.6223   - 1.5741  \n",
      "\u001b[1;4mValidati 187    - 100    -            1.8523   0.0124   0.6875   0.6751   0.0000   0.0000   |            0.7658   0.6094   0.5895   0.0000   0.0000   - 0.1054  \u001b[0m\n",
      "Training 188    - 100    -            1.8453   0.0123   1.0000   1.0000   0.6414   0.6363   |            0.7633   1.0000   1.0000   0.6414   0.6466   - 1.5722  \n",
      "\u001b[1;4mValidati 188    - 100    -            1.8451   0.0123   0.6406   0.6398   0.0000   0.0000   |            0.7642   0.6016   0.6093   0.0000   0.0000   - 0.1052  \u001b[0m\n",
      "Training 189    - 100    -            1.8382   0.0123   1.0000   1.0000   0.6241   0.6303   |            0.7616   1.0000   1.0000   0.6207   0.6207   - 1.5729  \n",
      "\u001b[1;4mValidati 189    - 100    -            1.8380   0.0123   0.6172   0.6141   0.0000   0.0000   |            0.7626   0.6641   0.6346   0.0000   0.0000   - 0.1046  \u001b[0m\n",
      "Training 190    - 100    -            1.8312   0.0122   1.0000   1.0000   0.6448   0.6504   |            0.7600   1.0000   0.9818   0.6517   0.6583   - 1.5775  \n",
      "\u001b[1;4mValidati 190    - 100    -            1.8312   0.0122   0.5938   0.5987   0.0000   0.0000   |            0.7612   0.5938   0.5915   0.0000   0.0000   - 0.1048  \u001b[0m\n",
      "Training 191    - 100    -            1.8244   0.0122   1.0000   1.0000   0.6276   0.6348   |            0.7596   0.9000   0.9000   0.6241   0.6231   - 1.5717  \n",
      "\u001b[1;4mValidati 191    - 100    -            1.8239   0.0122   0.6641   0.6607   0.0000   0.0000   |            0.7602   0.6641   0.6593   0.0000   0.0000   - 0.1051  \u001b[0m\n",
      "Training 192    - 100    -            1.8171   0.0122   1.0000   1.0000   0.6241   0.6277   |            0.7576   1.0000   0.9818   0.6345   0.6311   - 1.5721  \n",
      "\u001b[1;4mValidati 192    - 100    -            1.8171   0.0122   0.5938   0.5755   0.0000   0.0000   |            0.7589   0.6328   0.6101   0.0000   0.0000   - 0.1045  \u001b[0m\n",
      "Training 193    - 100    -            1.8105   0.0121   1.0000   1.0000   0.6448   0.6350   |            0.7564   1.0000   1.0000   0.6207   0.6296   - 1.5862  \n",
      "\u001b[1;4mValidati 193    - 100    -            1.8110   0.0121   0.5703   0.5780   0.0000   0.0000   |            0.7578   0.5312   0.5208   0.0000   0.0000   - 0.1067  \u001b[0m\n",
      "Training 194    - 100    -            1.8044   0.0121   1.0000   1.0000   0.6414   0.6513   |            0.7552   1.0000   0.9818   0.6276   0.6343   - 1.5756  \n",
      "\u001b[1;4mValidati 194    - 100    -            1.8043   0.0121   0.6484   0.6502   0.0000   0.0000   |            0.7563   0.6094   0.6146   0.0000   0.0000   - 0.1047  \u001b[0m\n",
      "Training 195    - 100    -            1.7978   0.0121   1.0000   1.0000   0.6483   0.6407   |            0.7538   1.0000   1.0000   0.6241   0.6318   - 1.5704  \n",
      "\u001b[1;4mValidati 195    - 100    -            1.7985   0.0121   0.5703   0.5780   0.0000   0.0000   |            0.7552   0.6562   0.6617   0.0000   0.0000   - 0.1051  \u001b[0m\n",
      "Training 196    - 100    -            1.7919   0.0120   1.0000   1.0000   0.6448   0.6420   |            0.7526   1.0000   1.0000   0.6379   0.6391   - 1.5735  \n",
      "\u001b[1;4mValidati 196    - 100    -            1.7921   0.0120   0.6172   0.6095   0.0000   0.0000   |            0.7536   0.6641   0.6639   0.0000   0.0000   - 0.1047  \u001b[0m\n",
      "Training 197    - 100    -            1.7856   0.0120   1.0000   1.0000   0.6448   0.6525   |            0.7510   1.0000   1.0000   0.6483   0.6350   - 1.5759  \n",
      "\u001b[1;4mValidati 197    - 100    -            1.7849   0.0120   0.6875   0.6942   0.0000   0.0000   |            0.7515   0.6953   0.7016   0.0000   0.0000   - 0.1054  \u001b[0m\n",
      "Training 198    - 100    -            1.7785   0.0119   1.0000   1.0000   0.6414   0.6374   |            0.7489   1.0000   1.0000   0.6414   0.6357   - 1.5886  \n",
      "\n",
      "Training 199    - 100    -            1.7727   0.0119   1.0000   1.0000   0.6586   0.6509   |            0.7477   1.0000   0.9818   0.6483   0.6327   - 1.5735  \n",
      "\u001b[1;4mValidati 199    - 100    -            1.7727   0.0119   0.5703   0.5536   0.0000   0.0000   |            0.7488   0.5859   0.5482   0.0000   0.0000   - 0.1057  \u001b[0m\n",
      "Training 200    - 100    -            1.7664   0.0119   1.0000   1.0000   0.6379   0.6384   |            0.7463   1.0000   1.0000   0.6310   0.6262   - 1.5776  \n"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the hyper parameters and the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "    \n",
    "final_metrics = {\n",
    "    \"max_student_acc\": maximum_tracker.max[\"student_acc\"],\n",
    "    \"max_teacher_acc\": maximum_tracker.max[\"teacher_acc\"],\n",
    "    \"max_student_f1\": maximum_tracker.max[\"student_f1\"],\n",
    "    \"max_teacher_f1\": maximum_tracker.max[\"teacher_f1\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 2160x1008 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"377.005398pt\" version=\"1.1\" viewBox=\"0 0 1711.303125 377.005398\" width=\"1711.303125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 377.005398 \nL 1711.303125 377.005398 \nL 1711.303125 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 353.127273 \nL 522.456066 353.127273 \nL 522.456066 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m52c7acbc3f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.482804\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(49.301554 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.713154\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(102.350654 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.943503\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(158.581003 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.173853\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(214.811353 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"277.404203\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(267.860453 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.634552\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(324.090802 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"389.864902\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 150 -->\n      <g transform=\"translate(380.321152 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"446.095251\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 175 -->\n      <g transform=\"translate(436.551501 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"502.325601\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(492.781851 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7ca26c7801\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7ca26c7801\" y=\"302.729943\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 306.529162)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7ca26c7801\" y=\"251.123077\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 254.922296)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7ca26c7801\" y=\"199.516211\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(7.2 203.31543)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7ca26c7801\" y=\"147.909345\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 151.708564)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7ca26c7801\" y=\"96.302479\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(7.2 100.101698)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7ca26c7801\" y=\"44.695613\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.7 -->\n      <g transform=\"translate(7.2 48.494832)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p705ebc2c88)\" d=\"M 52.482804 337.403306 \nL 54.732018 337.403306 \nL 56.981232 289.021869 \nL 61.47966 289.021869 \nL 63.728874 236.608646 \nL 65.978088 236.608646 \nL 68.227302 176.13185 \nL 83.9718 176.13185 \nL 86.221014 139.845772 \nL 88.470228 139.845772 \nL 90.719442 111.623268 \nL 95.21787 111.623268 \nL 97.467084 91.464336 \nL 115.460796 91.464336 \nL 117.71001 83.400763 \nL 203.180141 83.400763 \nL 205.429355 55.178258 \nL 275.154989 55.178258 \nL 277.404203 43.082899 \nL 284.151845 43.082899 \nL 286.401059 30.98754 \nL 500.076387 30.98754 \nL 500.076387 30.98754 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p705ebc2c88)\" d=\"M 52.482804 272.894723 \nL 56.981232 272.894723 \nL 59.230446 256.767578 \nL 61.47966 256.767578 \nL 63.728874 240.640432 \nL 65.978088 240.640432 \nL 68.227302 228.545073 \nL 70.476516 155.972918 \nL 101.965512 155.972918 \nL 104.214726 147.909345 \nL 108.713154 147.909345 \nL 110.962368 143.877559 \nL 122.208438 143.877559 \nL 124.457652 135.813986 \nL 137.952936 135.813986 \nL 140.20215 103.559695 \nL 160.445075 103.559695 \nL 162.694289 83.400763 \nL 167.192717 83.400763 \nL 169.441931 79.368976 \nL 207.678569 79.368976 \nL 209.927783 75.33719 \nL 250.413635 75.33719 \nL 252.662849 67.273617 \nL 266.158133 67.273617 \nL 268.407347 43.082899 \nL 365.123548 43.082899 \nL 367.372762 26.955753 \nL 419.104684 26.955753 \nL 421.353898 22.923967 \nL 500.076387 22.923967 \nL 500.076387 22.923967 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 353.127273 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 522.456066 353.127273 \nL 522.456066 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 353.127273 \nL 522.456066 353.127273 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 522.456066 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 37.103125 45.1125 \nL 223.5 45.1125 \nQ 225.5 45.1125 225.5 43.1125 \nL 225.5 14.2 \nQ 225.5 12.2 223.5 12.2 \nL 37.103125 12.2 \nQ 35.103125 12.2 35.103125 14.2 \nL 35.103125 43.1125 \nQ 35.103125 45.1125 37.103125 45.1125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 39.103125 20.298437 \nL 59.103125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_16\">\n     <!-- max/student_acc = 0.7265625 -->\n     <defs>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n      <path d=\"M 25.390625 72.90625 \nL 33.6875 72.90625 \nL 8.296875 -9.28125 \nL 0 -9.28125 \nz\n\" id=\"DejaVuSans-47\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n     </defs>\n     <g transform=\"translate(67.103125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"158.691406\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"217.871094\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"251.5625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"303.662109\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"342.871094\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"406.25\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"469.726562\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"531.25\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"594.628906\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"633.837891\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"683.837891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"745.117188\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"800.097656\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"855.078125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"886.865234\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"970.654297\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1002.441406\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1066.064453\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1097.851562\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1161.474609\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1225.097656\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1288.720703\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1352.34375\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1415.966797\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1479.589844\" xlink:href=\"#DejaVuSans-53\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 39.103125 35.254687 \nL 59.103125 35.254687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_17\">\n     <!-- max/teacher_acc = 0.7421875 -->\n     <defs>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n     </defs>\n     <g transform=\"translate(67.103125 38.754687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"158.691406\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"217.871094\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"251.5625\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"290.771484\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"352.294922\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"413.574219\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"468.554688\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"531.933594\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"593.457031\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"634.570312\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"684.570312\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"745.849609\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"800.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"855.810547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"887.597656\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"971.386719\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1003.173828\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1066.796875\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1098.583984\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1162.207031\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"1225.830078\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1289.453125\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"1353.076172\" xlink:href=\"#DejaVuSans-56\"/>\n      <use x=\"1416.699219\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1480.322266\" xlink:href=\"#DejaVuSans-53\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 1211.750184 353.127273 \nL 1704.103125 353.127273 \nL 1704.103125 7.2 \nL 1211.750184 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1234.129863\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0 -->\n      <g transform=\"translate(1230.948613 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1290.360213\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 25 -->\n      <g transform=\"translate(1283.997713 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1346.590562\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 50 -->\n      <g transform=\"translate(1340.228062 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1402.820912\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 75 -->\n      <g transform=\"translate(1396.458412 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1459.051261\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 100 -->\n      <g transform=\"translate(1449.507511 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1515.281611\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 125 -->\n      <g transform=\"translate(1505.737861 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1571.511961\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 150 -->\n      <g transform=\"translate(1561.968211 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1627.74231\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 175 -->\n      <g transform=\"translate(1618.19856 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1683.97266\" xlink:href=\"#m52c7acbc3f\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 200 -->\n      <g transform=\"translate(1674.42891 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m7ca26c7801\" y=\"337.480913\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 0.0000 -->\n      <g transform=\"translate(1169.759559 341.280132)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m7ca26c7801\" y=\"285.054756\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0.0005 -->\n      <g transform=\"translate(1169.759559 288.853974)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m7ca26c7801\" y=\"232.628598\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 0.0010 -->\n      <g transform=\"translate(1169.759559 236.427817)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m7ca26c7801\" y=\"180.20244\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 0.0015 -->\n      <g transform=\"translate(1169.759559 184.001659)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m7ca26c7801\" y=\"127.776282\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0.0020 -->\n      <g transform=\"translate(1169.759559 131.575501)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m7ca26c7801\" y=\"75.350125\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 0.0025 -->\n      <g transform=\"translate(1169.759559 79.149343)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m7ca26c7801\" y=\"22.923967\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 0.0030 -->\n      <g transform=\"translate(1169.759559 26.723186)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p0d814d385f)\" d=\"M 1234.129863 22.94337 \nL 1238.628291 22.94337 \nL 1243.126719 23.098566 \nL 1247.625147 23.408804 \nL 1252.123575 23.873779 \nL 1256.622003 24.493031 \nL 1261.120431 25.265949 \nL 1265.618859 26.191772 \nL 1272.366501 27.865159 \nL 1279.114143 29.876774 \nL 1285.861785 32.222149 \nL 1292.609427 34.896078 \nL 1299.357069 37.892623 \nL 1306.10471 41.205131 \nL 1312.852352 44.826248 \nL 1319.599994 48.747934 \nL 1326.347636 52.961482 \nL 1333.095278 57.457537 \nL 1339.84292 62.226116 \nL 1346.590562 67.256632 \nL 1353.338204 72.537916 \nL 1360.085846 78.058243 \nL 1366.833488 83.805355 \nL 1375.830344 91.798825 \nL 1384.8272 100.141183 \nL 1393.824056 108.799507 \nL 1402.820912 117.739626 \nL 1414.066982 129.257203 \nL 1425.313052 141.088874 \nL 1438.808336 155.598666 \nL 1459.051261 177.732017 \nL 1486.041829 207.243186 \nL 1499.537113 221.70399 \nL 1510.783183 233.478623 \nL 1522.029253 244.92479 \nL 1531.026109 253.797727 \nL 1540.022965 262.380217 \nL 1549.019821 270.638388 \nL 1558.016677 278.53965 \nL 1564.764319 284.212561 \nL 1571.511961 289.654542 \nL 1578.259603 294.853513 \nL 1585.007245 299.797929 \nL 1591.754886 304.476814 \nL 1598.502528 308.879778 \nL 1605.25017 312.997047 \nL 1611.997812 316.819478 \nL 1618.745454 320.338586 \nL 1625.493096 323.546556 \nL 1632.240738 326.436266 \nL 1638.98838 329.001301 \nL 1645.736022 331.235965 \nL 1652.483664 333.135296 \nL 1659.231306 334.695079 \nL 1663.729734 335.544554 \nL 1668.228162 336.240725 \nL 1672.72659 336.782906 \nL 1677.225018 337.17056 \nL 1681.723446 337.403306 \nL 1681.723446 337.403306 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 1211.750184 353.127273 \nL 1211.750184 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 1704.103125 353.127273 \nL 1704.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 1211.750184 353.127273 \nL 1704.103125 353.127273 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 1211.750184 7.2 \nL 1704.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 1465.215625 30.15625 \nL 1697.103125 30.15625 \nQ 1699.103125 30.15625 1699.103125 28.15625 \nL 1699.103125 14.2 \nQ 1699.103125 12.2 1697.103125 12.2 \nL 1465.215625 12.2 \nQ 1463.215625 12.2 1463.215625 14.2 \nL 1463.215625 28.15625 \nQ 1463.215625 30.15625 1465.215625 30.15625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_39\">\n     <path d=\"M 1467.215625 20.298437 \nL 1487.215625 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_40\"/>\n    <g id=\"text_34\">\n     <!-- hyperparameters/learning_rate = 0.003 -->\n     <defs>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     </defs>\n     <g transform=\"translate(1495.215625 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"63.378906\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"122.558594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"186.035156\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"247.558594\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"288.671875\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"352.148438\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"413.427734\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"454.541016\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"515.820312\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"613.232422\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"674.755859\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"713.964844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"775.488281\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"816.601562\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"868.701172\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"902.392578\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"930.175781\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"991.699219\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1052.978516\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1094.076172\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1157.455078\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1185.238281\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1248.617188\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"1312.09375\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"1362.09375\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1403.207031\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1464.486328\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1503.695312\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1565.21875\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1597.005859\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1680.794922\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1712.582031\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1776.205078\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1807.992188\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1871.615234\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1935.238281\" xlink:href=\"#DejaVuSans-51\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p705ebc2c88\">\n   <rect height=\"345.927273\" width=\"492.352941\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p0d814d385f\">\n   <rect height=\"345.927273\" width=\"492.352941\" x=\"1211.750184\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"max/student_acc\")\n",
    "pp(\"max/teacher_acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (only for speechcommand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset.lower() == \"speechcommand\":\n",
    "    \n",
    "    from DCT.dataset_loader.speechcommand import SpeechCommands\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.nn import Sequential\n",
    "    from DCT.util.transforms import PadUpTo\n",
    "    from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "    transform = Sequential(\n",
    "        PadUpTo(target_length=16000, mode=\"constant\", value=0),\n",
    "        MelSpectrogram(sample_rate=16000, n_fft=2048, hop_length=512, n_mels=64),\n",
    "        AmplitudeToDB(),\n",
    "    )\n",
    "\n",
    "    test_dataset = SpeechCommands(root=args.dataset_root, subset=\"testing\", download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-f4374d55d915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(header)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\")\n",
    "reset_metrics()\n",
    "model.eval()\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    for i, (X, y) in enumerate(test_loader):\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        # metrics\n",
    "        pred = torch.softmax(logits, dim=1)\n",
    "        pred_arg = torch.argmax(logits, dim=1)\n",
    "        y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "        acc = acc_fn(pred_arg, y).mean\n",
    "        fscore = fscore_fn(pred, y_one_hot).mean\n",
    "        avg_ce = avg(loss.item()).mean\n",
    "\n",
    "        # logs\n",
    "        print(val_form.format(\n",
    "            \"Testing: \",\n",
    "            1,\n",
    "            int(100 * (i + 1) / len(val_loader)),\n",
    "            \"\", avg_ce,\n",
    "            \"\", acc, fscore,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .llll||=||llll."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}