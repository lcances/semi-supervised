{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1703.01780.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, dotdict\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import JensenShanon\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"esc10\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.003, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_s = parser.add_argument_group(\"Student teacher parameters\")\n",
    "group_s.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_s.add_argument(\"--warmup_length\", default=50, type=int)\n",
    "group_s.add_argument(\"--lambda_cost_max\", default=1, type=float)\n",
    "group_s.add_argument(\"--teacher_noise\", default=2, type=float)\n",
    "group_s.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"mean-teacher\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"mean-teacher\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args=parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'batch_size': 64,\n 'checkpoint_path': 'mean-teacher',\n 'checkpoint_root': '../../model_save/',\n 'dataset': 'esc10',\n 'dataset_root': '../../datasets',\n 'ema_alpha': 0.999,\n 'epsilon': 0.02,\n 'from_config': '',\n 'lambda_cost_max': 1,\n 'learning_rate': 0.003,\n 'model': 'wideresnet28_2',\n 'nb_epoch': 200,\n 'num_classes': 10,\n 'resume': False,\n 'seed': 1234,\n 'supervised_ratio': 0.1,\n 'teacher_noise': 2,\n 'tensorboard_path': 'mean-teacher',\n 'tensorboard_root': '../../tensorboard/',\n 'tensorboard_sufix': '',\n 'train_folds': [1, 2, 3, 4],\n 'val_folds': [5],\n 'warmup_length': 50}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): MelSpectrogram(\n",
       "    (spectrogram): Spectrogram()\n",
       "    (mel_scale): MelScale()\n",
       "  )\n",
       "  (1): AmplitudeToDB()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"mean-teacher\")\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already downloaded and verified.\n",
      "Dataset already downloaded and verified.\n",
      "s_batch_size:  6\n",
      "u_batch_size:  58\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"mean-teacher\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(64, 431)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "input_shape = tuple(train_loader._iterables[0].dataset[0][0].shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "student = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "teacher = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "\n",
    "student = student.cuda()\n",
    "teacher = teacher.cuda()\n",
    "\n",
    "# We do not need gradient for the teacher model\n",
    "for p in teacher.parameters():\n",
    "    p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 32, 64, 431]             864\n       BatchNorm2d-2          [-1, 32, 64, 431]              64\n              ReLU-3          [-1, 32, 64, 431]               0\n         MaxPool2d-4          [-1, 32, 32, 216]               0\n            Conv2d-5          [-1, 32, 32, 216]           9,216\n       BatchNorm2d-6          [-1, 32, 32, 216]              64\n              ReLU-7          [-1, 32, 32, 216]               0\n            Conv2d-8          [-1, 32, 32, 216]           9,216\n       BatchNorm2d-9          [-1, 32, 32, 216]              64\n             ReLU-10          [-1, 32, 32, 216]               0\n       BasicBlock-11          [-1, 32, 32, 216]               0\n           Conv2d-12          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-13          [-1, 32, 32, 216]              64\n             ReLU-14          [-1, 32, 32, 216]               0\n           Conv2d-15          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-16          [-1, 32, 32, 216]              64\n             ReLU-17          [-1, 32, 32, 216]               0\n       BasicBlock-18          [-1, 32, 32, 216]               0\n           Conv2d-19          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-20          [-1, 32, 32, 216]              64\n             ReLU-21          [-1, 32, 32, 216]               0\n           Conv2d-22          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-23          [-1, 32, 32, 216]              64\n             ReLU-24          [-1, 32, 32, 216]               0\n       BasicBlock-25          [-1, 32, 32, 216]               0\n           Conv2d-26          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-27          [-1, 32, 32, 216]              64\n             ReLU-28          [-1, 32, 32, 216]               0\n           Conv2d-29          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-30          [-1, 32, 32, 216]              64\n             ReLU-31          [-1, 32, 32, 216]               0\n       BasicBlock-32          [-1, 32, 32, 216]               0\n           Conv2d-33          [-1, 64, 16, 108]          18,432\n      BatchNorm2d-34          [-1, 64, 16, 108]             128\n             ReLU-35          [-1, 64, 16, 108]               0\n           Conv2d-36          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-37          [-1, 64, 16, 108]             128\n           Conv2d-38          [-1, 64, 16, 108]           2,048\n      BatchNorm2d-39          [-1, 64, 16, 108]             128\n             ReLU-40          [-1, 64, 16, 108]               0\n       BasicBlock-41          [-1, 64, 16, 108]               0\n           Conv2d-42          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-43          [-1, 64, 16, 108]             128\n             ReLU-44          [-1, 64, 16, 108]               0\n           Conv2d-45          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-46          [-1, 64, 16, 108]             128\n             ReLU-47          [-1, 64, 16, 108]               0\n       BasicBlock-48          [-1, 64, 16, 108]               0\n           Conv2d-49          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-50          [-1, 64, 16, 108]             128\n             ReLU-51          [-1, 64, 16, 108]               0\n           Conv2d-52          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-53          [-1, 64, 16, 108]             128\n             ReLU-54          [-1, 64, 16, 108]               0\n       BasicBlock-55          [-1, 64, 16, 108]               0\n           Conv2d-56          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-57          [-1, 64, 16, 108]             128\n             ReLU-58          [-1, 64, 16, 108]               0\n           Conv2d-59          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-60          [-1, 64, 16, 108]             128\n             ReLU-61          [-1, 64, 16, 108]               0\n       BasicBlock-62          [-1, 64, 16, 108]               0\n           Conv2d-63           [-1, 128, 8, 54]          73,728\n      BatchNorm2d-64           [-1, 128, 8, 54]             256\n             ReLU-65           [-1, 128, 8, 54]               0\n           Conv2d-66           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-67           [-1, 128, 8, 54]             256\n           Conv2d-68           [-1, 128, 8, 54]           8,192\n      BatchNorm2d-69           [-1, 128, 8, 54]             256\n             ReLU-70           [-1, 128, 8, 54]               0\n       BasicBlock-71           [-1, 128, 8, 54]               0\n           Conv2d-72           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-73           [-1, 128, 8, 54]             256\n             ReLU-74           [-1, 128, 8, 54]               0\n           Conv2d-75           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-76           [-1, 128, 8, 54]             256\n             ReLU-77           [-1, 128, 8, 54]               0\n       BasicBlock-78           [-1, 128, 8, 54]               0\n           Conv2d-79           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-80           [-1, 128, 8, 54]             256\n             ReLU-81           [-1, 128, 8, 54]               0\n           Conv2d-82           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-83           [-1, 128, 8, 54]             256\n             ReLU-84           [-1, 128, 8, 54]               0\n       BasicBlock-85           [-1, 128, 8, 54]               0\n           Conv2d-86           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-87           [-1, 128, 8, 54]             256\n             ReLU-88           [-1, 128, 8, 54]               0\n           Conv2d-89           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-90           [-1, 128, 8, 54]             256\n             ReLU-91           [-1, 128, 8, 54]               0\n       BasicBlock-92           [-1, 128, 8, 54]               0\nAdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n           Linear-94                   [-1, 10]           1,290\n================================================================\nTotal params: 1,472,554\nTrainable params: 1,472,554\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.11\nForward/backward pass size (MB): 107.11\nParams size (MB): 5.62\nEstimated Total Size (MB): 112.83\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(student, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../../tensorboard/esc10/mean-teacher/wideresnet28_2/0.1S/2020-11-03_16:15:14_wideresnet28_2_JS\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__)\n",
    "tensorboard_title = \"%s/%sS/%s_%s_JS\" % title_element\n",
    "\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__)\n",
    "checkpoint_title = \"%s/%sS/%s_%s_JS\" % title_element\n",
    "\n",
    "tensorboard = mSummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "source": [
    "## Adversarial generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator = GradientSignAttack(\n",
    "    student, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-numpy.inf, clip_max=numpy.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(args.dataset, \"mean-teacher\", student=student, teacher=teacher)\n",
    "callbacks = load_callbacks(args.dataset, \"mean-teacher\", optimizer=optimizer, nb_epoch=args.nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\") # Supervised loss\n",
    "consistency_cost = nn.MSELoss(reduction=\"mean\") # Unsupervised loss\n",
    "# consistency_cost = JensenShanon\n",
    "\n",
    "lambda_cost = Warmup(args.lambda_cost_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cost]\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(student, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculator():\n",
    "    def c(logits, y):\n",
    "        with torch.no_grad():\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            arg = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            acc = c.fn.acc(arg, y).mean\n",
    "            f1 = c.fn.f1(pred, y_one_hot).mean\n",
    "            \n",
    "            return acc, f1,\n",
    "            \n",
    "    c.fn = dotdict(\n",
    "        acc = CategoricalAccuracy(),\n",
    "        f1 = FScore(),\n",
    "    )\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_student_s_metrics = metrics_calculator()\n",
    "calc_student_u_metrics = metrics_calculator()\n",
    "calc_teacher_s_metrics = metrics_calculator()\n",
    "calc_teacher_u_metrics = metrics_calculator()\n",
    "\n",
    "avg_Sce = ContinueAverage()\n",
    "avg_Tce = ContinueAverage()\n",
    "avg_ccost = ContinueAverage()\n",
    "\n",
    "softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "def reset_metrics():\n",
    "    for d in [calc_student_s_metrics.fn, calc_student_u_metrics.fn, calc_teacher_s_metrics.fn, calc_teacher_u_metrics.fn]:\n",
    "        for fn in d.values():\n",
    "            fn.reset()\n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} | {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} - {:<8.6}\"\n",
    "value_form  = \"{:<8.8} {:<6d} - {:<6d} - {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} | {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} - {:<8.4f}\"\n",
    "header = header_form.format(\".               \", \"Epoch\",  \"%\", \"Student:\", \"ce\", \"ccost\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\", \"Teacher:\", \"ce\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\" , \"Time\")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "    \n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "    \n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data,  alpha = 1-alpha)\n",
    "\n",
    "class Noisify(nn.Module):\n",
    "    def __init__(self, noise_level: int = 15):\n",
    "        super().__init__()\n",
    "\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + (torch.rand(x.shape).cuda() * self.noise_level)\n",
    "\n",
    "noise_fn = lambda x: x\n",
    "if args.teacher_noise != 0:\n",
    "    noise_fn = Noisify(noise_level=args.teacher_noise)\n",
    "    noise_fn = noise_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    reset_metrics()\n",
    "    student.train()\n",
    "\n",
    "    for i, (S, U) in enumerate(train_loader):        \n",
    "        x_s, y_s = S\n",
    "        x_u, y_u = U\n",
    "        \n",
    "        x_s, x_u = x_s.cuda(), x_u.cuda()\n",
    "        y_s, y_u = y_s.cuda(), y_u.cuda()\n",
    "        \n",
    "        # Student predictions\n",
    "        with autocast():\n",
    "            student_s_logits = student(x_s)        \n",
    "            student_u_logits = student(x_u)\n",
    "\n",
    "        # ---- adversarial examples ----\n",
    "        # for U using student model and pseudo label\n",
    "        student.eval()\n",
    "        pseudo_pred_u = torch.argmax(student_u_logits, dim=1)\n",
    "        adv_u = adv_generator.perturb(x_u, pseudo_pred_u)\n",
    "        student.train()\n",
    "\n",
    "        # Teacher prediction on adv U\n",
    "        with autocast():\n",
    "            teacher_adv_u_logits = teacher(adv_u)\n",
    "        \n",
    "        # ---- losses ----\n",
    "        with autocast():\n",
    "            loss = loss_ce(student_s_logits, y_s)\n",
    "            ccost = consistency_cost(softmax_fn(student_u_logits), softmax_fn(teacher_adv_u_logits))\n",
    "            total_loss = loss + lambda_cost() * ccost\n",
    "        \n",
    "        # step\n",
    "        for p in student.parameters(): p.grad = None\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # Teacher prediction (for metrics purpose)\n",
    "            with autocast():\n",
    "                teacher_s_logits = teacher(x_s)\n",
    "                teacher_u_logits = teacher(x_u)\n",
    "                _teacher_loss = loss_ce(teacher_s_logits, y_s)\n",
    "            \n",
    "            # Update teacher\n",
    "            update_teacher_model(student, teacher, args.ema_alpha, epoch*nb_batch + i)\n",
    "            \n",
    "            # Compute the metrics for the student\n",
    "            student_s_metrics = calc_student_s_metrics(student_s_logits, y_s)\n",
    "            student_u_metrics = calc_student_u_metrics(student_u_logits, y_u)\n",
    "            student_s_acc, student_s_f1, student_u_acc, student_u_f1 = *student_s_metrics, *student_u_metrics\n",
    "            \n",
    "            # Compute the metrics for the teacher\n",
    "            teacher_s_metrics = calc_teacher_s_metrics(teacher_s_logits, y_s)\n",
    "            teacher_u_metrics = calc_teacher_u_metrics(teacher_u_logits, y_u)\n",
    "            teacher_s_acc, teacher_s_f1, teacher_u_acc, teacher_u_f1 = *teacher_s_metrics, *teacher_u_metrics\n",
    "            \n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \", epoch + 1, int(100 * (i + 1) / nb_batch),\n",
    "                \"\", student_running_loss, running_ccost, *student_s_metrics, *student_u_metrics,\n",
    "                \"\", teacher_running_loss, *teacher_s_metrics, *teacher_u_metrics,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_acc_s\", student_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_acc_u\", student_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_s\", student_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_u\", student_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/teacher_acc_s\", teacher_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_u\", teacher_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_s\", teacher_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_u\", teacher_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/consistency_cost\", running_ccost, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    student.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Predictions\n",
    "            with autocast():\n",
    "                student_logits = student(X)        \n",
    "                teacher_logits = teacher(X)\n",
    "\n",
    "                # Calculate supervised loss (only student on S)\n",
    "                loss = loss_ce(student_logits, y)\n",
    "                _teacher_loss = loss_ce(teacher_logits, y) # for metrics only\n",
    "                ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "            \n",
    "            # Compute the metrics\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            # ---- student ----\n",
    "            student_metrics = calc_student_s_metrics(student_logits, y)\n",
    "            student_acc, student_f1 = student_metrics\n",
    "            \n",
    "            # ---- teacher ----\n",
    "            teacher_metrics = calc_teacher_s_metrics(teacher_logits, y)\n",
    "            teacher_acc, teacher_f1 = teacher_metrics\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \", epoch + 1, int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", student_running_loss, running_ccost, *student_metrics, 0.0, 0.0,\n",
    "                \"\", teacher_running_loss, *teacher_metrics, 0.0, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/student_acc\", student_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_f1\", student_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_acc\", teacher_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_f1\", teacher_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/consistency_cost\", running_ccost, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    tensorboard.add_scalar(\"hyperparameters/lambda_cost_max\", lambda_cost(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/student_acc\", maximum_tracker(\"student_acc\", student_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_acc\", maximum_tracker(\"teacher_acc\", teacher_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/student_f1\", maximum_tracker(\"student_f1\", student_f1), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_f1\", maximum_tracker(\"teacher_f1\", teacher_f1), epoch )\n",
    "\n",
    "    checkpoint.step(teacher_acc)\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0.6464   |            0.7438   1.0000   1.0000   0.6276   0.6194   - 2.0415  \n",
      "\u001b[1;4mValidati 140    - 100    -            6.9365   0.0137   0.5703   0.5863   0.0000   0.0000   |            0.7451   0.5781   0.5756   0.0000   0.0000   - 0.0847  \u001b[0m\n",
      "Training 141    - 100    -            6.9014   0.0137   1.0000   1.0000   0.6414   0.6350   |            0.7418   1.0000   1.0000   0.6103   0.6119   - 2.0433  \n",
      "\u001b[1;4mValidati 141    - 100    -            6.8901   0.0137   0.6719   0.6761   0.0000   0.0000   |            0.7422   0.7266   0.7289   0.0000   0.0000   - 0.0806  \u001b[0m\n",
      "Training 142    - 100    -            6.8555   0.0137   1.0000   1.0000   0.6379   0.6425   |            0.7388   1.0000   1.0000   0.6310   0.6229   - 2.0363  \n",
      "\u001b[1;4mValidati 142    - 100    -            6.8450   0.0137   0.5391   0.5603   0.0000   0.0000   |            0.7406   0.4844   0.5016   0.0000   0.0000   - 0.0803  \u001b[0m\n",
      "Training 143    - 100    -            6.8108   0.0137   1.0000   1.0000   0.6517   0.6512   |            0.7373   1.0000   1.0000   0.6345   0.6324   - 2.0356  \n",
      "\u001b[1;4mValidati 143    - 100    -            6.7997   0.0137   0.6406   0.6575   0.0000   0.0000   |            0.7381   0.5938   0.6268   0.0000   0.0000   - 0.0807  \u001b[0m\n",
      "Training 144    - 100    -            6.7660   0.0136   1.0000   1.0000   0.6414   0.6452   |            0.7348   1.0000   1.0000   0.6345   0.6261   - 2.0360  \n",
      "\u001b[1;4mValidati 144    - 100    -            6.7567   0.0136   0.4844   0.4915   0.0000   0.0000   |            0.7366   0.5391   0.5507   0.0000   0.0000   - 0.0854  \u001b[0m\n",
      "Training 145    - 100    -            6.7238   0.0136   0.9667   0.9667   0.6241   0.6296   |            0.7340   0.9333   0.9333   0.6276   0.6236   - 2.0493  \n",
      "\u001b[1;4mValidati 145    - 100    -            6.7136   0.0136   0.5859   0.5857   0.0000   0.0000   |            0.7350   0.6484   0.6514   0.0000   0.0000   - 0.0812  \u001b[0m\n",
      "Training 146    - 100    -            6.6808   0.0135   1.0000   1.0000   0.6414   0.6410   |            0.7318   1.0000   1.0000   0.6276   0.6307   - 2.0360  \n",
      "\u001b[1;4mValidati 146    - 100    -            6.6718   0.0135   0.4922   0.4835   0.0000   0.0000   |            0.7330   0.6562   0.6645   0.0000   0.0000   - 0.0813  \u001b[0m\n",
      "Training 147    - 100    -            6.6394   0.0135   1.0000   1.0000   0.6207   0.6206   |            0.7301   0.9667   0.9667   0.6241   0.6355   - 2.0361  \n",
      "\u001b[1;4mValidati 147    - 100    -            6.6300   0.0135   0.5625   0.5625   0.0000   0.0000   |            0.7315   0.5547   0.5749   0.0000   0.0000   - 0.0810  \u001b[0m\n",
      "Training 148    - 100    -            6.5981   0.0135   1.0000   1.0000   0.6345   0.6244   |            0.7285   0.9667   0.9667   0.6172   0.6107   - 2.0335  \n",
      "\u001b[1;4mValidati 148    - 100    -            6.5887   0.0135   0.5312   0.5278   0.0000   0.0000   |            0.7299   0.5234   0.5183   0.0000   0.0000   - 0.0801  \u001b[0m\n",
      "Training 149    - 100    -            6.5571   0.0134   1.0000   1.0000   0.6414   0.6505   |            0.7268   1.0000   1.0000   0.6345   0.6282   - 2.0433  \n",
      "\u001b[1;4mValidati 149    - 100    -            6.5485   0.0134   0.5625   0.5554   0.0000   0.0000   |            0.7279   0.6250   0.6295   0.0000   0.0000   - 0.0848  \u001b[0m\n",
      "Training 150    - 100    -            6.5174   0.0134   0.9667   0.9667   0.6414   0.6454   |            0.7249   1.0000   1.0000   0.6310   0.6340   - 2.0470  \n",
      "\u001b[1;4mValidati 150    - 100    -            6.5096   0.0134   0.4453   0.4443   0.0000   0.0000   |            0.7270   0.5391   0.5440   0.0000   0.0000   - 0.0809  \u001b[0m\n",
      "Training 151    - 100    -            6.4789   0.0133   1.0000   1.0000   0.6241   0.6247   |            0.7240   1.0000   1.0000   0.6276   0.6247   - 2.0534  \n",
      "\u001b[1;4mValidati 151    - 100    -            6.4700   0.0133   0.5000   0.4793   0.0000   0.0000   |            0.7258   0.5391   0.5651   0.0000   0.0000   - 0.0864  \u001b[0m\n",
      "Training 152    - 100    -            6.4396   0.0133   1.0000   1.0000   0.6241   0.6290   |            0.7229   1.0000   0.9818   0.6310   0.6172   - 2.0499  \n",
      "\u001b[1;4mValidati 152    - 100    -            6.4308   0.0133   0.5156   0.5119   0.0000   0.0000   |            0.7243   0.5547   0.5670   0.0000   0.0000   - 0.0812  \u001b[0m\n",
      "Training 153    - 100    -            6.4008   0.0133   1.0000   1.0000   0.6483   0.6492   |            0.7212   1.0000   1.0000   0.6276   0.6247   - 2.0421  \n",
      "\u001b[1;4mValidati 153    - 100    -            6.3924   0.0132   0.5156   0.5258   0.0000   0.0000   |            0.7228   0.5703   0.5984   0.0000   0.0000   - 0.0886  \u001b[0m\n",
      "Training 154    - 100    -            6.3636   0.0132   0.9667   0.9667   0.6414   0.6543   |            0.7207   0.9667   0.9455   0.6241   0.6235   - 2.0701  \n",
      "\u001b[1;4mValidati 154    - 100    -            6.3548   0.0132   0.5703   0.5765   0.0000   0.0000   |            0.7215   0.6719   0.6532   0.0000   0.0000   - 0.0831  \u001b[0m\n",
      "Training 155    - 100    -            6.3256   0.0132   1.0000   1.0000   0.6379   0.6327   |            0.7186   1.0000   1.0000   0.6448   0.6487   - 2.0470  \n",
      "\u001b[1;4mValidati 155    - 100    -            6.3175   0.0132   0.5703   0.5685   0.0000   0.0000   |            0.7199   0.6406   0.6210   0.0000   0.0000   - 0.0815  \u001b[0m\n",
      "Training 156    - 100    -            6.2886   0.0131   1.0000   1.0000   0.6310   0.6334   |            0.7169   1.0000   1.0000   0.6448   0.6366   - 2.0523  \n",
      "\u001b[1;4mValidati 156    - 100    -            6.2803   0.0131   0.5234   0.5242   0.0000   0.0000   |            0.7179   0.6250   0.6364   0.0000   0.0000   - 0.0836  \u001b[0m\n",
      "Training 157    - 100    -            6.2519   0.0131   1.0000   1.0000   0.6276   0.6271   |            0.7154   0.9333   0.9485   0.6276   0.6281   - 2.0543  \n",
      "\u001b[1;4mValidati 157    - 100    -            6.2438   0.0131   0.5312   0.5357   0.0000   0.0000   |            0.7171   0.6172   0.6335   0.0000   0.0000   - 0.0831  \u001b[0m\n",
      "Training 158    - 100    -            6.2156   0.0131   1.0000   1.0000   0.6172   0.6235   |            0.7141   1.0000   1.0000   0.6276   0.6293   - 2.0427  \n",
      "\u001b[1;4mValidati 158    - 100    -            6.2081   0.0131   0.5547   0.5522   0.0000   0.0000   |            0.7157   0.5625   0.5726   0.0000   0.0000   - 0.0812  \u001b[0m\n",
      "Training 159    - 100    -            6.1804   0.0130   1.0000   1.0000   0.6345   0.6408   |            0.7131   0.9667   0.9485   0.6172   0.6229   - 2.0382  \n",
      "\u001b[1;4mValidati 159    - 100    -            6.1723   0.0130   0.6250   0.6186   0.0000   0.0000   |            0.7142   0.6406   0.6532   0.0000   0.0000   - 0.0814  \u001b[0m\n",
      "Training 160    - 100    -            6.1447   0.0130   1.0000   1.0000   0.6310   0.6303   |            0.7113   1.0000   1.0000   0.6310   0.6344   - 2.0692  \n",
      "\u001b[1;4mValidati 160    - 100    -            6.1374   0.0130   0.4766   0.4895   0.0000   0.0000   |            0.7128   0.5234   0.5430   0.0000   0.0000   - 0.0846  \u001b[0m\n",
      "Training 161    - 100    -            6.1103   0.0129   1.0000   1.0000   0.6448   0.6422   |            0.7101   0.9667   0.9818   0.6345   0.6263   - 2.0608  \n",
      "\u001b[1;4mValidati 161    - 100    -            6.1021   0.0129   0.5938   0.5957   0.0000   0.0000   |            0.7109   0.6406   0.6455   0.0000   0.0000   - 0.0860  \u001b[0m\n",
      "Training 162    - 100    -            6.0753   0.0129   1.0000   1.0000   0.6379   0.6430   |            0.7081   1.0000   0.9818   0.6448   0.6389   - 2.0575  \n",
      "\u001b[1;4mValidati 162    - 100    -            6.0676   0.0129   0.5547   0.5511   0.0000   0.0000   |            0.7095   0.5312   0.5426   0.0000   0.0000   - 0.0855  \u001b[0m\n",
      "Training 163    - 100    -            6.0412   0.0129   0.9667   0.9667   0.6069   0.6154   |            0.7068   0.9667   0.9667   0.6310   0.6287   - 2.0423  \n",
      "\u001b[1;4mValidati 163    - 100    -            6.0335   0.0129   0.5938   0.6018   0.0000   0.0000   |            0.7077   0.6016   0.6110   0.0000   0.0000   - 0.0806  \u001b[0m\n",
      "Training 164    - 100    -            6.0073   0.0128   1.0000   1.0000   0.6379   0.6399   |            0.7053   0.9333   0.9455   0.6172   0.6187   - 2.0452  \n",
      "\u001b[1;4mValidati 164    - 100    -            6.0001   0.0128   0.5547   0.5358   0.0000   0.0000   |            0.7066   0.5547   0.5970   0.0000   0.0000   - 0.0817  \u001b[0m\n",
      "Training 165    - 100    -            5.9742   0.0128   1.0000   1.0000   0.6414   0.6405   |            0.7038   1.0000   0.9818   0.6379   0.6192   - 2.0363  \n",
      "\u001b[1;4mValidati 165    - 100    -            5.9665   0.0128   0.5938   0.6108   0.0000   0.0000   |            0.7045   0.6719   0.6827   0.0000   0.0000   - 0.0812  \u001b[0m\n",
      "Training 166    - 100    -            5.9409   0.0127   1.0000   1.0000   0.6379   0.6372   |            0.7017   1.0000   1.0000   0.6345   0.6373   - 2.0355  \n",
      "\u001b[1;4mValidati 166    - 100    -            5.9341   0.0128   0.5078   0.5195   0.0000   0.0000   |            0.7035   0.5469   0.5454   0.0000   0.0000   - 0.0817  \u001b[0m\n",
      "Training 167    - 100    -            5.9095   0.0127   0.9667   0.9667   0.6414   0.6384   |            0.7020   0.9000   0.9000   0.6310   0.6346   - 2.0532  \n",
      "\u001b[1;4mValidati 167    - 100    -            5.9029   0.0127   0.5156   0.5180   0.0000   0.0000   |            0.7043   0.4922   0.4677   0.0000   0.0000   - 0.0843  \u001b[0m\n",
      "Training 168    - 100    -            5.8780   0.0127   1.0000   1.0000   0.6345   0.6318   |            0.7019   1.0000   0.9818   0.6310   0.6296   - 2.0459  \n",
      "\u001b[1;4mValidati 168    - 100    -            5.8711   0.0127   0.5781   0.5765   0.0000   0.0000   |            0.7032   0.5391   0.5260   0.0000   0.0000   - 0.0826  \u001b[0m\n",
      "Training 169    - 100    -            5.8466   0.0126   0.9667   0.9818   0.6172   0.6208   |            0.7011   0.9333   0.9303   0.6207   0.6228   - 2.0369  \n",
      "\u001b[1;4mValidati 169    - 100    -            5.8404   0.0126   0.5156   0.5124   0.0000   0.0000   |            0.7026   0.5625   0.5892   0.0000   0.0000   - 0.0807  \u001b[0m\n",
      "Training 170    - 100    -            5.8167   0.0126   0.9333   0.9333   0.6414   0.6441   |            0.7008   0.9333   0.9333   0.6414   0.6395   - 2.0397  \n",
      "\u001b[1;4mValidati 170    - 100    -            5.8102   0.0126   0.5312   0.5335   0.0000   0.0000   |            0.7023   0.6406   0.6202   0.0000   0.0000   - 0.0812  \u001b[0m\n",
      "Training 171    - 100    -            5.7869   0.0126   0.9333   0.9333   0.6448   0.6458   |            0.7010   0.9000   0.8933   0.6276   0.6265   - 2.0593  \n",
      "\u001b[1;4mValidati 171    - 100    -            5.7803   0.0126   0.5391   0.5413   0.0000   0.0000   |            0.7022   0.6016   0.6094   0.0000   0.0000   - 0.0841  \u001b[0m\n",
      "Training 172    - 100    -            5.7564   0.0125   1.0000   1.0000   0.6414   0.6424   |            0.6997   0.9667   0.9667   0.6483   0.6461   - 2.0475  \n",
      "\u001b[1;4mValidati 172    - 100    -            5.7501   0.0125   0.5391   0.5413   0.0000   0.0000   |            0.7008   0.6172   0.6008   0.0000   0.0000   - 0.0808  \u001b[0m\n",
      "Training 173    - 100    -            5.7264   0.0125   1.0000   1.0000   0.6276   0.6259   |            0.6982   1.0000   1.0000   0.6207   0.6047   - 2.0367  \n",
      "\u001b[1;4mValidati 173    - 100    -            5.7196   0.0125   0.6094   0.6136   0.0000   0.0000   |            0.6995   0.5312   0.5426   0.0000   0.0000   - 0.0810  \u001b[0m\n",
      "Training 174    - 100    -            5.6962   0.0125   1.0000   1.0000   0.6172   0.6268   |            0.6969   1.0000   1.0000   0.6241   0.6209   - 2.0378  \n",
      "\u001b[1;4mValidati 174    - 100    -            5.6899   0.0125   0.6094   0.6136   0.0000   0.0000   |            0.6982   0.6172   0.6231   0.0000   0.0000   - 0.0810  \u001b[0m\n",
      "Training 175    - 100    -            5.6668   0.0124   1.0000   1.0000   0.6241   0.6251   |            0.6956   0.9667   0.9667   0.6034   0.6101   - 2.0382  \n",
      "\u001b[1;4mValidati 175    - 100    -            5.6610   0.0124   0.5312   0.5357   0.0000   0.0000   |            0.6971   0.5781   0.6065   0.0000   0.0000   - 0.0811  \u001b[0m\n",
      "Training 176    - 100    -            5.6382   0.0124   1.0000   1.0000   0.6379   0.6402   |            0.6949   0.9667   0.9818   0.6310   0.6204   - 2.0370  \n",
      "\u001b[1;4mValidati 176    - 100    -            5.6321   0.0124   0.5625   0.5835   0.0000   0.0000   |            0.6964   0.5078   0.5237   0.0000   0.0000   - 0.0813  \u001b[0m\n",
      "Training 177    - 100    -            5.6095   0.0124   1.0000   1.0000   0.6517   0.6504   |            0.6944   0.9000   0.9418   0.6483   0.6455   - 2.0367  \n",
      "\u001b[1;4mValidati 177    - 100    -            5.6034   0.0124   0.6328   0.6369   0.0000   0.0000   |            0.6952   0.6797   0.6613   0.0000   0.0000   - 0.0814  \u001b[0m\n",
      "Training 178    - 100    -            5.5811   0.0123   1.0000   1.0000   0.6552   0.6454   |            0.6931   0.9667   0.9667   0.6483   0.6590   - 2.0369  \n",
      "\u001b[1;4mValidati 178    - 100    -            5.5752   0.0123   0.5625   0.5670   0.0000   0.0000   |            0.6942   0.6250   0.6364   0.0000   0.0000   - 0.0845  \u001b[0m\n",
      "Training 179    - 100    -            5.5530   0.0123   1.0000   1.0000   0.6552   0.6612   |            0.6917   1.0000   1.0000   0.6379   0.6458   - 2.0539  \n",
      "\u001b[1;4mValidati 179    - 100    -            5.5471   0.0123   0.5781   0.5889   0.0000   0.0000   |            0.6929   0.6094   0.6142   0.0000   0.0000   - 0.0811  \u001b[0m\n",
      "Training 180    - 100    -            5.5251   0.0123   1.0000   1.0000   0.6483   0.6505   |            0.6904   0.9667   0.9667   0.6448   0.6479   - 2.0433  \n",
      "\u001b[1;4mValidati 180    - 100    -            5.5189   0.0123   0.5781   0.6032   0.0000   0.0000   |            0.6915   0.6484   0.6683   0.0000   0.0000   - 0.0820  \u001b[0m\n",
      "Training 181    - 100    -            5.4971   0.0122   1.0000   1.0000   0.6414   0.6334   |            0.6890   1.0000   1.0000   0.6207   0.6159   - 2.0407  \n",
      "\u001b[1;4mValidati 181    - 100    -            5.4917   0.0122   0.5078   0.5124   0.0000   0.0000   |            0.6905   0.5703   0.5984   0.0000   0.0000   - 0.0813  \u001b[0m\n",
      "Training 182    - 100    -            5.4702   0.0122   1.0000   1.0000   0.6379   0.6387   |            0.6880   1.0000   1.0000   0.6345   0.6350   - 2.0375  \n",
      "\u001b[1;4mValidati 182    - 100    -            5.4645   0.0122   0.6094   0.6136   0.0000   0.0000   |            0.6892   0.5625   0.5783   0.0000   0.0000   - 0.0811  \u001b[0m\n",
      "Training 183    - 100    -            5.4433   0.0122   1.0000   1.0000   0.6138   0.6156   |            0.6871   0.9667   0.9667   0.6310   0.6138   - 2.0370  \n",
      "\u001b[1;4mValidati 183    - 100    -            5.4374   0.0122   0.6172   0.6226   0.0000   0.0000   |            0.6881   0.6406   0.6532   0.0000   0.0000   - 0.0809  \u001b[0m\n",
      "Training 184    - 100    -            5.4163   0.0121   1.0000   1.0000   0.6310   0.6279   |            0.6857   1.0000   1.0000   0.6241   0.6148   - 2.0526  \n",
      "\u001b[1;4mValidati 184    - 100    -            5.4112   0.0121   0.5391   0.5413   0.0000   0.0000   |            0.6872   0.5391   0.5603   0.0000   0.0000   - 0.0831  \u001b[0m\n",
      "Training 185    - 100    -            5.3903   0.0121   1.0000   1.0000   0.6172   0.6209   |            0.6848   1.0000   1.0000   0.5966   0.5965   - 2.0387  \n",
      "\u001b[1;4mValidati 185    - 100    -            5.3850   0.0121   0.5625   0.5415   0.0000   0.0000   |            0.6858   0.6250   0.6452   0.0000   0.0000   - 0.0809  \u001b[0m\n",
      "Training 186    - 100    -            5.3645   0.0121   0.9667   0.9667   0.6276   0.6209   |            0.6837   0.9333   0.9273   0.6138   0.6149   - 2.0356  \n",
      "\u001b[1;4mValidati 186    - 100    -            5.3594   0.0121   0.5547   0.5635   0.0000   0.0000   |            0.6853   0.5156   0.5274   0.0000   0.0000   - 0.0809  \u001b[0m\n",
      "Training 187    - 100    -            5.3389   0.0120   1.0000   1.0000   0.6276   0.6271   |            0.6830   1.0000   1.0000   0.6241   0.6279   - 2.0350  \n",
      "\u001b[1;4mValidati 187    - 100    -            5.3334   0.0120   0.6250   0.6431   0.0000   0.0000   |            0.6840   0.6094   0.6157   0.0000   0.0000   - 0.0867  \u001b[0m\n",
      "Training 188    - 100    -            5.3132   0.0120   1.0000   1.0000   0.6621   0.6571   |            0.6817   1.0000   1.0000   0.6621   0.6618   - 2.0624  \n",
      "\u001b[1;4mValidati 188    - 100    -            5.3082   0.0120   0.5625   0.5692   0.0000   0.0000   |            0.6832   0.5391   0.5403   0.0000   0.0000   - 0.0851  \u001b[0m\n",
      "Training 189    - 100    -            5.2882   0.0120   1.0000   1.0000   0.6379   0.6400   |            0.6809   0.9667   0.9667   0.6276   0.6237   - 2.0545  \n",
      "\u001b[1;4mValidati 189    - 100    -            5.2829   0.0120   0.5859   0.5881   0.0000   0.0000   |            0.6824   0.6641   0.6350   0.0000   0.0000   - 0.0817  \u001b[0m\n",
      "Training 190    - 100    -            5.2631   0.0119   1.0000   1.0000   0.6414   0.6426   |            0.6800   1.0000   1.0000   0.6379   0.6355   - 2.0420  \n",
      "\u001b[1;4mValidati 190    - 100    -            5.2581   0.0119   0.5469   0.5437   0.0000   0.0000   |            0.6817   0.5469   0.5484   0.0000   0.0000   - 0.0811  \u001b[0m\n",
      "Training 191    - 100    -            5.2388   0.0119   0.9333   0.9333   0.6448   0.6387   |            0.6802   0.9000   0.9000   0.6345   0.6446   - 2.0398  \n",
      "\u001b[1;4mValidati 191    - 100    -            5.2334   0.0119   0.6172   0.6215   0.0000   0.0000   |            0.6812   0.6328   0.6371   0.0000   0.0000   - 0.0829  \u001b[0m\n",
      "Training 192    - 100    -            5.2140   0.0119   1.0000   1.0000   0.6276   0.6260   |            0.6790   1.0000   0.9818   0.6310   0.6257   - 2.0418  \n",
      "\u001b[1;4mValidati 192    - 100    -            5.2089   0.0119   0.5625   0.5647   0.0000   0.0000   |            0.6803   0.5391   0.5517   0.0000   0.0000   - 0.0856  \u001b[0m\n",
      "Training 193    - 100    -            5.1897   0.0118   1.0000   1.0000   0.6310   0.6302   |            0.6782   0.9667   0.9667   0.6345   0.6311   - 2.0507  \n",
      "\u001b[1;4mValidati 193    - 100    -            5.1857   0.0119   0.5703   0.5773   0.0000   0.0000   |            0.6800   0.5703   0.5429   0.0000   0.0000   - 0.0810  \u001b[0m\n",
      "Training 194    - 100    -            5.1666   0.0118   1.0000   1.0000   0.6483   0.6457   |            0.6777   1.0000   1.0000   0.6379   0.6421   - 2.0469  \n",
      "\u001b[1;4mValidati 194    - 100    -            5.1621   0.0118   0.5703   0.5840   0.0000   0.0000   |            0.6793   0.5547   0.5720   0.0000   0.0000   - 0.0836  \u001b[0m\n",
      "Training 195    - 100    -            5.1432   0.0118   1.0000   1.0000   0.6448   0.6392   |            0.6769   1.0000   1.0000   0.6241   0.6230   - 2.0401  \n",
      "\u001b[1;4mValidati 195    - 100    -            5.1395   0.0118   0.5156   0.5171   0.0000   0.0000   |            0.6787   0.5781   0.5828   0.0000   0.0000   - 0.0811  \u001b[0m\n",
      "Training 196    - 100    -            5.1208   0.0118   1.0000   1.0000   0.6345   0.6400   |            0.6764   1.0000   1.0000   0.6207   0.6270   - 2.0499  \n",
      "\u001b[1;4mValidati 196    - 100    -            5.1164   0.0118   0.5078   0.5195   0.0000   0.0000   |            0.6781   0.5625   0.5373   0.0000   0.0000   - 0.0829  \u001b[0m\n",
      "Training 197    - 100    -            5.0979   0.0117   1.0000   1.0000   0.6379   0.6357   |            0.6757   1.0000   1.0000   0.6483   0.6385   - 2.0391  \n",
      "\u001b[1;4mValidati 197    - 100    -            5.0926   0.0117   0.6250   0.6310   0.0000   0.0000   |            0.6764   0.6406   0.6492   0.0000   0.0000   - 0.0850  \u001b[0m\n",
      "Training 198    - 100    -            5.0742   0.0117   1.0000   1.0000   0.6586   0.6561   |            0.6741   1.0000   1.0000   0.6414   0.6434   - 2.0471  \n",
      "\u001b[1;4mValidati 198    - 100    -            5.0703   0.0117   0.4922   0.5006   0.0000   0.0000   |            0.6758   0.4922   0.4940   0.0000   0.0000   - 0.0810  \u001b[0m\n",
      "Training 199    - 100    -            5.0523   0.0117   1.0000   1.0000   0.6414   0.6365   |            0.6736   1.0000   1.0000   0.6241   0.6260   - 2.0385  \n",
      "\u001b[1;4mValidati 199    - 100    -            5.0479   0.0117   0.5469   0.5549   0.0000   0.0000   |            0.6750   0.4297   0.4450   0.0000   0.0000   - 0.0807  \u001b[0m\n",
      "Training 200    - 100    -            5.0299   0.0116   1.0000   1.0000   0.6310   0.6277   |            0.6728   1.0000   1.0000   0.6207   0.6190   - 2.0368  \n"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "if args.resume:\n",
    "    checkpoint.load_last()\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the hyper parameters and the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "    \n",
    "final_metrics = {\n",
    "    \"max_student_acc\": maximum_tracker.max[\"student_acc\"],\n",
    "    \"max_teacher_acc\": maximum_tracker.max[\"teacher_acc\"],\n",
    "    \"max_student_f1\": maximum_tracker.max[\"student_f1\"],\n",
    "    \"max_teacher_f1\": maximum_tracker.max[\"teacher_f1\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 2160x1008 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"377.005398pt\" version=\"1.1\" viewBox=\"0 0 1711.303125 377.005398\" width=\"1711.303125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 377.005398 \nL 1711.303125 377.005398 \nL 1711.303125 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 353.127273 \nL 522.456066 353.127273 \nL 522.456066 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m195ec03db2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.482804\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(49.301554 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.713154\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(102.350654 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.943503\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(158.581003 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.173853\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(214.811353 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"277.404203\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(267.860453 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.634552\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(324.090802 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"389.864902\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 150 -->\n      <g transform=\"translate(380.321152 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"446.095251\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 175 -->\n      <g transform=\"translate(436.551501 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"502.325601\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(492.781851 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m42be647cd2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m42be647cd2\" y=\"302.279639\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 306.078858)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m42be647cd2\" y=\"250.002554\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 253.801773)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m42be647cd2\" y=\"197.72547\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(7.2 201.524688)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m42be647cd2\" y=\"145.448385\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 149.247603)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m42be647cd2\" y=\"93.1713\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(7.2 96.970519)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m42be647cd2\" y=\"40.894215\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.7 -->\n      <g transform=\"translate(7.2 44.693434)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p94ffb51e9a)\" d=\"M 52.482804 337.403306 \nL 54.732018 337.403306 \nL 56.981232 329.235011 \nL 59.230446 251.636213 \nL 61.47966 227.13133 \nL 63.728874 227.13133 \nL 65.978088 206.710594 \nL 70.476516 206.710594 \nL 72.72573 198.542299 \nL 74.974944 125.027648 \nL 79.473372 125.027648 \nL 81.722586 120.943501 \nL 83.9718 120.943501 \nL 86.221014 22.923967 \nL 500.076387 22.923967 \nL 500.076387 22.923967 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p94ffb51e9a)\" d=\"M 52.482804 316.982569 \nL 54.732018 247.552066 \nL 61.47966 247.552066 \nL 63.728874 198.542299 \nL 65.978088 198.542299 \nL 68.227302 178.121563 \nL 70.476516 125.027648 \nL 77.224158 125.027648 \nL 79.473372 116.859354 \nL 83.9718 116.859354 \nL 86.221014 112.775207 \nL 101.965512 112.775207 \nL 104.214726 67.849587 \nL 207.678569 67.849587 \nL 209.927783 51.512998 \nL 250.413635 51.512998 \nL 252.662849 47.42885 \nL 266.158133 47.42885 \nL 268.407347 31.092261 \nL 365.123548 31.092261 \nL 367.372762 27.008114 \nL 500.076387 27.008114 \nL 500.076387 27.008114 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 353.127273 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 522.456066 353.127273 \nL 522.456066 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 353.127273 \nL 522.456066 353.127273 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 522.456066 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 329.059191 348.127273 \nL 515.456066 348.127273 \nQ 517.456066 348.127273 517.456066 346.127273 \nL 517.456066 317.214773 \nQ 517.456066 315.214773 515.456066 315.214773 \nL 329.059191 315.214773 \nQ 327.059191 315.214773 327.059191 317.214773 \nL 327.059191 346.127273 \nQ 327.059191 348.127273 329.059191 348.127273 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 331.059191 323.31321 \nL 351.059191 323.31321 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_16\">\n     <!-- max/student_acc = 0.734375 -->\n     <defs>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n      <path d=\"M 25.390625 72.90625 \nL 33.6875 72.90625 \nL 8.296875 -9.28125 \nL 0 -9.28125 \nz\n\" id=\"DejaVuSans-47\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n     </defs>\n     <g transform=\"translate(359.059191 326.81321)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"158.691406\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"217.871094\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"251.5625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"303.662109\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"342.871094\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"406.25\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"469.726562\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"531.25\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"594.628906\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"633.837891\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"683.837891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"745.117188\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"800.097656\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"855.078125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"886.865234\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"970.654297\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1002.441406\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1066.064453\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1097.851562\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1161.474609\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1225.097656\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"1288.720703\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1352.34375\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1415.966797\" xlink:href=\"#DejaVuSans-53\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 331.059191 338.26946 \nL 351.059191 338.26946 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_17\">\n     <!-- max/teacher_acc = 0.7265625 -->\n     <defs>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     </defs>\n     <g transform=\"translate(359.059191 341.76946)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"158.691406\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"217.871094\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"251.5625\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"290.771484\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"352.294922\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"413.574219\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"468.554688\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"531.933594\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"593.457031\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"634.570312\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"684.570312\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"745.849609\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"800.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"855.810547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"887.597656\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"971.386719\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1003.173828\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1066.796875\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1098.583984\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1162.207031\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1225.830078\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1289.453125\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1353.076172\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1416.699219\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1480.322266\" xlink:href=\"#DejaVuSans-53\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 1211.750184 353.127273 \nL 1704.103125 353.127273 \nL 1704.103125 7.2 \nL 1211.750184 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1234.129863\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0 -->\n      <g transform=\"translate(1230.948613 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1290.360213\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 25 -->\n      <g transform=\"translate(1283.997713 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1346.590562\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 50 -->\n      <g transform=\"translate(1340.228062 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1402.820912\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 75 -->\n      <g transform=\"translate(1396.458412 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1459.051261\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 100 -->\n      <g transform=\"translate(1449.507511 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1515.281611\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 125 -->\n      <g transform=\"translate(1505.737861 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1571.511961\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 150 -->\n      <g transform=\"translate(1561.968211 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1627.74231\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 175 -->\n      <g transform=\"translate(1618.19856 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1683.97266\" xlink:href=\"#m195ec03db2\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 200 -->\n      <g transform=\"translate(1674.42891 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m42be647cd2\" y=\"337.480913\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 0.0000 -->\n      <g transform=\"translate(1169.759559 341.280132)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m42be647cd2\" y=\"285.054756\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0.0005 -->\n      <g transform=\"translate(1169.759559 288.853974)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m42be647cd2\" y=\"232.628598\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 0.0010 -->\n      <g transform=\"translate(1169.759559 236.427817)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m42be647cd2\" y=\"180.20244\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 0.0015 -->\n      <g transform=\"translate(1169.759559 184.001659)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m42be647cd2\" y=\"127.776282\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0.0020 -->\n      <g transform=\"translate(1169.759559 131.575501)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m42be647cd2\" y=\"75.350125\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 0.0025 -->\n      <g transform=\"translate(1169.759559 79.149343)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m42be647cd2\" y=\"22.923967\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 0.0030 -->\n      <g transform=\"translate(1169.759559 26.723186)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p4dcc272eb6)\" d=\"M 1234.129863 22.94337 \nL 1238.628291 22.94337 \nL 1243.126719 23.098566 \nL 1247.625147 23.408804 \nL 1252.123575 23.873779 \nL 1256.622003 24.493031 \nL 1261.120431 25.265949 \nL 1265.618859 26.191772 \nL 1272.366501 27.865159 \nL 1279.114143 29.876774 \nL 1285.861785 32.222149 \nL 1292.609427 34.896078 \nL 1299.357069 37.892623 \nL 1306.10471 41.205131 \nL 1312.852352 44.826248 \nL 1319.599994 48.747934 \nL 1326.347636 52.961482 \nL 1333.095278 57.457537 \nL 1339.84292 62.226116 \nL 1346.590562 67.256632 \nL 1353.338204 72.537916 \nL 1360.085846 78.058243 \nL 1366.833488 83.805355 \nL 1375.830344 91.798825 \nL 1384.8272 100.141183 \nL 1393.824056 108.799507 \nL 1402.820912 117.739626 \nL 1414.066982 129.257203 \nL 1425.313052 141.088874 \nL 1438.808336 155.598666 \nL 1459.051261 177.732017 \nL 1486.041829 207.243186 \nL 1499.537113 221.70399 \nL 1510.783183 233.478623 \nL 1522.029253 244.92479 \nL 1531.026109 253.797727 \nL 1540.022965 262.380217 \nL 1549.019821 270.638388 \nL 1558.016677 278.53965 \nL 1564.764319 284.212561 \nL 1571.511961 289.654542 \nL 1578.259603 294.853513 \nL 1585.007245 299.797929 \nL 1591.754886 304.476814 \nL 1598.502528 308.879778 \nL 1605.25017 312.997047 \nL 1611.997812 316.819478 \nL 1618.745454 320.338586 \nL 1625.493096 323.546556 \nL 1632.240738 326.436266 \nL 1638.98838 329.001301 \nL 1645.736022 331.235965 \nL 1652.483664 333.135296 \nL 1659.231306 334.695079 \nL 1663.729734 335.544554 \nL 1668.228162 336.240725 \nL 1672.72659 336.782906 \nL 1677.225018 337.17056 \nL 1681.723446 337.403306 \nL 1681.723446 337.403306 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 1211.750184 353.127273 \nL 1211.750184 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 1704.103125 353.127273 \nL 1704.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 1211.750184 353.127273 \nL 1704.103125 353.127273 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 1211.750184 7.2 \nL 1704.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 1465.215625 30.15625 \nL 1697.103125 30.15625 \nQ 1699.103125 30.15625 1699.103125 28.15625 \nL 1699.103125 14.2 \nQ 1699.103125 12.2 1697.103125 12.2 \nL 1465.215625 12.2 \nQ 1463.215625 12.2 1463.215625 14.2 \nL 1463.215625 28.15625 \nQ 1463.215625 30.15625 1465.215625 30.15625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_39\">\n     <path d=\"M 1467.215625 20.298437 \nL 1487.215625 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_40\"/>\n    <g id=\"text_34\">\n     <!-- hyperparameters/learning_rate = 0.003 -->\n     <defs>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     </defs>\n     <g transform=\"translate(1495.215625 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"63.378906\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"122.558594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"186.035156\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"247.558594\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"288.671875\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"352.148438\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"413.427734\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"454.541016\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"515.820312\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"613.232422\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"674.755859\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"713.964844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"775.488281\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"816.601562\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"868.701172\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"902.392578\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"930.175781\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"991.699219\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1052.978516\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1094.076172\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1157.455078\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1185.238281\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1248.617188\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"1312.09375\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"1362.09375\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1403.207031\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1464.486328\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1503.695312\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1565.21875\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1597.005859\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1680.794922\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1712.582031\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1776.205078\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1807.992188\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1871.615234\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1935.238281\" xlink:href=\"#DejaVuSans-51\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p94ffb51e9a\">\n   <rect height=\"345.927273\" width=\"492.352941\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p4dcc272eb6\">\n   <rect height=\"345.927273\" width=\"492.352941\" x=\"1211.750184\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"max/student_acc\")\n",
    "pp(\"max/teacher_acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (only for speechcommand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset.lower() == \"speechcommand\":\n",
    "    \n",
    "    from DCT.dataset_loader.speechcommand import SpeechCommands\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.nn import Sequential\n",
    "    from DCT.util.transforms import PadUpTo\n",
    "    from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "    transform = Sequential(\n",
    "        PadUpTo(target_length=16000, mode=\"constant\", value=0),\n",
    "        MelSpectrogram(sample_rate=16000, n_fft=2048, hop_length=512, n_mels=64),\n",
    "        AmplitudeToDB(),\n",
    "    )\n",
    "\n",
    "    test_dataset = SpeechCommands(root=args.dataset_root, subset=\"testing\", download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-f4374d55d915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(header)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\")\n",
    "reset_metrics()\n",
    "model.eval()\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    for i, (X, y) in enumerate(test_loader):\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        # metrics\n",
    "        pred = torch.softmax(logits, dim=1)\n",
    "        pred_arg = torch.argmax(logits, dim=1)\n",
    "        y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "        acc = acc_fn(pred_arg, y).mean\n",
    "        fscore = fscore_fn(pred, y_one_hot).mean\n",
    "        avg_ce = avg(loss.item()).mean\n",
    "\n",
    "        # logs\n",
    "        print(val_form.format(\n",
    "            \"Testing: \",\n",
    "            1,\n",
    "            int(100 * (i + 1) / len(val_loader)),\n",
    "            \"\", avg_ce,\n",
    "            \"\", acc, fscore,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .llll||=||llll."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}