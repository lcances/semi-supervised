{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "from typing import Union\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, DotDict, cache_to_disk, get_train_format, get_lr\n",
    "from SSL.util.mixup import MixUpBatchShuffle\n",
    "from SSL.loss import FixMatchLoss\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from metric_utils.metrics import BinaryAccuracy, FScore, ContinueAverage, MAP\n",
    "from augmentation_utils.spec_augmentations import SpecAugment\n",
    "from torch.cuda import empty_cache\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DotDict(\n",
    "    hardware=DotDict(\n",
    "        nb_cpu=10,\n",
    "        nb_gpu=2,\n",
    "    ),\n",
    "    \n",
    "    dataset=DotDict(\n",
    "        dataset='audioset-unbalanced',\n",
    "        num_classes=527,\n",
    "    ),\n",
    "    \n",
    "    model=DotDict(\n",
    "        model=\"MobileNetV2\",\n",
    "    ),\n",
    "    \n",
    "    train_param=DotDict(\n",
    "        supervised_ratio=0.1,\n",
    "        batch_size=64,\n",
    "        nb_iteration=125000,\n",
    "        learning_rate=0.003,\n",
    "        seed=1234,\n",
    "        resume=False,\n",
    "        \n",
    "        train_folds=None,\n",
    "        val_folds=None,\n",
    "    ),\n",
    "    \n",
    "    fixmatch=DotDict(\n",
    "        mask_threshold=0.75,\n",
    "        guess_threshold=0.5,\n",
    "        lambda_s=1.0,\n",
    "        lambda_u=1.0,\n",
    "        warmup_length=50000,\n",
    "    ),\n",
    "    \n",
    "    mixup=DotDict(\n",
    "        use=False,\n",
    "        alpha=1.0,\n",
    "        max=True,\n",
    "        label=True\n",
    "    ),\n",
    "    \n",
    "    specaugment=DotDict(\n",
    "        use=False,\n",
    "        time_drop_width=32,\n",
    "        time_stripe_num=1,\n",
    "        freq_drop_width=4,\n",
    "        freq_stripe_num=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "cfg['path'] = DotDict(\n",
    "    dataset_root='../../datasets',\n",
    "    checkpoint_root='../../model_save',\n",
    "    tensorboard_root='../../tensorboard',\n",
    ")\n",
    "cfg.path['checkpoint_path']=f'{cfg.path.checkpoint_root}/{cfg.dataset.dataset}/fixmatch'\n",
    "cfg.path['tensorboard_path']=f'{cfg.path.tensorboard_root}/{cfg.dataset.dataset}/fixmatch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reset the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(cfg.train_param.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -------- Get the pre-processer --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/samova/lcances/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: fixmatch | audioset-unbalanced\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(cfg.dataset.dataset, \"fixmatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Get the dataset --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/samova/lcances/semi-supervised/SSL/dataset/speechcommands.py\n",
      "loading dataset: fixmatch | audioset-unbalanced\n",
      "creating validation dataset\n",
      "creating weak and strong augmented train dataset\n",
      "spliting the dataset\n",
      "cache path:  .cache_supervised_ratio=0.1_unsupervised_ratio=0.9_batch_size=64_verbose=True\n",
      "split ready, loading cache file\n",
      "Getting all target\n",
      "Sort the classes\n",
      "merging dataloaders\n"
     ]
    }
   ],
   "source": [
    "_, train_loader, val_loader = load_dataset(\n",
    "    cfg.dataset.dataset,\n",
    "    \"fixmatch\",\n",
    "\n",
    "    dataset_root=cfg.path.dataset_root,\n",
    "    supervised_ratio=cfg.train_param.supervised_ratio,\n",
    "    batch_size=cfg.train_param.batch_size,\n",
    "    train_folds=cfg.train_param.train_folds,\n",
    "    val_folds=cfg.train_param.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "\n",
    "    num_workers=cfg.hardware.nb_cpu,  # With the cache enable, it is faster to have only one worker\n",
    "    pin_memory=True,\n",
    "    seed=cfg.train_param.seed,\n",
    "\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/samova/lcances/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/users/samova/lcances/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "# The input shape of the data is used to generate the model\n",
    "input_shape = tuple(train_loader._iterables[0].dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Prepare the model --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 32, 64, 501]             288\n",
      "         AvgPool2d-2          [-1, 32, 32, 250]               0\n",
      "       BatchNorm2d-3          [-1, 32, 32, 250]              64\n",
      "             ReLU6-4          [-1, 32, 32, 250]               0\n",
      "            Conv2d-5          [-1, 32, 32, 250]             288\n",
      "         AvgPool2d-6          [-1, 32, 32, 250]               0\n",
      "       BatchNorm2d-7          [-1, 32, 32, 250]              64\n",
      "             ReLU6-8          [-1, 32, 32, 250]               0\n",
      "            Conv2d-9          [-1, 16, 32, 250]             512\n",
      "      BatchNorm2d-10          [-1, 16, 32, 250]              32\n",
      " InvertedResidual-11          [-1, 16, 32, 250]               0\n",
      "           Conv2d-12          [-1, 96, 32, 250]           1,536\n",
      "      BatchNorm2d-13          [-1, 96, 32, 250]             192\n",
      "            ReLU6-14          [-1, 96, 32, 250]               0\n",
      "           Conv2d-15          [-1, 96, 32, 250]             864\n",
      "        AvgPool2d-16          [-1, 96, 16, 125]               0\n",
      "      BatchNorm2d-17          [-1, 96, 16, 125]             192\n",
      "            ReLU6-18          [-1, 96, 16, 125]               0\n",
      "           Conv2d-19          [-1, 24, 16, 125]           2,304\n",
      "      BatchNorm2d-20          [-1, 24, 16, 125]              48\n",
      " InvertedResidual-21          [-1, 24, 16, 125]               0\n",
      "           Conv2d-22         [-1, 144, 16, 125]           3,456\n",
      "      BatchNorm2d-23         [-1, 144, 16, 125]             288\n",
      "            ReLU6-24         [-1, 144, 16, 125]               0\n",
      "           Conv2d-25         [-1, 144, 16, 125]           1,296\n",
      "        AvgPool2d-26         [-1, 144, 16, 125]               0\n",
      "      BatchNorm2d-27         [-1, 144, 16, 125]             288\n",
      "            ReLU6-28         [-1, 144, 16, 125]               0\n",
      "           Conv2d-29          [-1, 24, 16, 125]           3,456\n",
      "      BatchNorm2d-30          [-1, 24, 16, 125]              48\n",
      " InvertedResidual-31          [-1, 24, 16, 125]               0\n",
      "           Conv2d-32         [-1, 144, 16, 125]           3,456\n",
      "      BatchNorm2d-33         [-1, 144, 16, 125]             288\n",
      "            ReLU6-34         [-1, 144, 16, 125]               0\n",
      "           Conv2d-35         [-1, 144, 16, 125]           1,296\n",
      "        AvgPool2d-36           [-1, 144, 8, 62]               0\n",
      "      BatchNorm2d-37           [-1, 144, 8, 62]             288\n",
      "            ReLU6-38           [-1, 144, 8, 62]               0\n",
      "           Conv2d-39            [-1, 32, 8, 62]           4,608\n",
      "      BatchNorm2d-40            [-1, 32, 8, 62]              64\n",
      " InvertedResidual-41            [-1, 32, 8, 62]               0\n",
      "           Conv2d-42           [-1, 192, 8, 62]           6,144\n",
      "      BatchNorm2d-43           [-1, 192, 8, 62]             384\n",
      "            ReLU6-44           [-1, 192, 8, 62]               0\n",
      "           Conv2d-45           [-1, 192, 8, 62]           1,728\n",
      "        AvgPool2d-46           [-1, 192, 8, 62]               0\n",
      "      BatchNorm2d-47           [-1, 192, 8, 62]             384\n",
      "            ReLU6-48           [-1, 192, 8, 62]               0\n",
      "           Conv2d-49            [-1, 32, 8, 62]           6,144\n",
      "      BatchNorm2d-50            [-1, 32, 8, 62]              64\n",
      " InvertedResidual-51            [-1, 32, 8, 62]               0\n",
      "           Conv2d-52           [-1, 192, 8, 62]           6,144\n",
      "      BatchNorm2d-53           [-1, 192, 8, 62]             384\n",
      "            ReLU6-54           [-1, 192, 8, 62]               0\n",
      "           Conv2d-55           [-1, 192, 8, 62]           1,728\n",
      "        AvgPool2d-56           [-1, 192, 8, 62]               0\n",
      "      BatchNorm2d-57           [-1, 192, 8, 62]             384\n",
      "            ReLU6-58           [-1, 192, 8, 62]               0\n",
      "           Conv2d-59            [-1, 32, 8, 62]           6,144\n",
      "      BatchNorm2d-60            [-1, 32, 8, 62]              64\n",
      " InvertedResidual-61            [-1, 32, 8, 62]               0\n",
      "           Conv2d-62           [-1, 192, 8, 62]           6,144\n",
      "      BatchNorm2d-63           [-1, 192, 8, 62]             384\n",
      "            ReLU6-64           [-1, 192, 8, 62]               0\n",
      "           Conv2d-65           [-1, 192, 8, 62]           1,728\n",
      "        AvgPool2d-66           [-1, 192, 4, 31]               0\n",
      "      BatchNorm2d-67           [-1, 192, 4, 31]             384\n",
      "            ReLU6-68           [-1, 192, 4, 31]               0\n",
      "           Conv2d-69            [-1, 64, 4, 31]          12,288\n",
      "      BatchNorm2d-70            [-1, 64, 4, 31]             128\n",
      " InvertedResidual-71            [-1, 64, 4, 31]               0\n",
      "           Conv2d-72           [-1, 384, 4, 31]          24,576\n",
      "      BatchNorm2d-73           [-1, 384, 4, 31]             768\n",
      "            ReLU6-74           [-1, 384, 4, 31]               0\n",
      "           Conv2d-75           [-1, 384, 4, 31]           3,456\n",
      "        AvgPool2d-76           [-1, 384, 4, 31]               0\n",
      "      BatchNorm2d-77           [-1, 384, 4, 31]             768\n",
      "            ReLU6-78           [-1, 384, 4, 31]               0\n",
      "           Conv2d-79            [-1, 64, 4, 31]          24,576\n",
      "      BatchNorm2d-80            [-1, 64, 4, 31]             128\n",
      " InvertedResidual-81            [-1, 64, 4, 31]               0\n",
      "           Conv2d-82           [-1, 384, 4, 31]          24,576\n",
      "      BatchNorm2d-83           [-1, 384, 4, 31]             768\n",
      "            ReLU6-84           [-1, 384, 4, 31]               0\n",
      "           Conv2d-85           [-1, 384, 4, 31]           3,456\n",
      "        AvgPool2d-86           [-1, 384, 4, 31]               0\n",
      "      BatchNorm2d-87           [-1, 384, 4, 31]             768\n",
      "            ReLU6-88           [-1, 384, 4, 31]               0\n",
      "           Conv2d-89            [-1, 64, 4, 31]          24,576\n",
      "      BatchNorm2d-90            [-1, 64, 4, 31]             128\n",
      " InvertedResidual-91            [-1, 64, 4, 31]               0\n",
      "           Conv2d-92           [-1, 384, 4, 31]          24,576\n",
      "      BatchNorm2d-93           [-1, 384, 4, 31]             768\n",
      "            ReLU6-94           [-1, 384, 4, 31]               0\n",
      "           Conv2d-95           [-1, 384, 4, 31]           3,456\n",
      "        AvgPool2d-96           [-1, 384, 4, 31]               0\n",
      "      BatchNorm2d-97           [-1, 384, 4, 31]             768\n",
      "            ReLU6-98           [-1, 384, 4, 31]               0\n",
      "           Conv2d-99            [-1, 64, 4, 31]          24,576\n",
      "     BatchNorm2d-100            [-1, 64, 4, 31]             128\n",
      "InvertedResidual-101            [-1, 64, 4, 31]               0\n",
      "          Conv2d-102           [-1, 384, 4, 31]          24,576\n",
      "     BatchNorm2d-103           [-1, 384, 4, 31]             768\n",
      "           ReLU6-104           [-1, 384, 4, 31]               0\n",
      "          Conv2d-105           [-1, 384, 4, 31]           3,456\n",
      "       AvgPool2d-106           [-1, 384, 2, 15]               0\n",
      "     BatchNorm2d-107           [-1, 384, 2, 15]             768\n",
      "           ReLU6-108           [-1, 384, 2, 15]               0\n",
      "          Conv2d-109            [-1, 96, 2, 15]          36,864\n",
      "     BatchNorm2d-110            [-1, 96, 2, 15]             192\n",
      "InvertedResidual-111            [-1, 96, 2, 15]               0\n",
      "          Conv2d-112           [-1, 576, 2, 15]          55,296\n",
      "     BatchNorm2d-113           [-1, 576, 2, 15]           1,152\n",
      "           ReLU6-114           [-1, 576, 2, 15]               0\n",
      "          Conv2d-115           [-1, 576, 2, 15]           5,184\n",
      "       AvgPool2d-116           [-1, 576, 2, 15]               0\n",
      "     BatchNorm2d-117           [-1, 576, 2, 15]           1,152\n",
      "           ReLU6-118           [-1, 576, 2, 15]               0\n",
      "          Conv2d-119            [-1, 96, 2, 15]          55,296\n",
      "     BatchNorm2d-120            [-1, 96, 2, 15]             192\n",
      "InvertedResidual-121            [-1, 96, 2, 15]               0\n",
      "          Conv2d-122           [-1, 576, 2, 15]          55,296\n",
      "     BatchNorm2d-123           [-1, 576, 2, 15]           1,152\n",
      "           ReLU6-124           [-1, 576, 2, 15]               0\n",
      "          Conv2d-125           [-1, 576, 2, 15]           5,184\n",
      "       AvgPool2d-126           [-1, 576, 2, 15]               0\n",
      "     BatchNorm2d-127           [-1, 576, 2, 15]           1,152\n",
      "           ReLU6-128           [-1, 576, 2, 15]               0\n",
      "          Conv2d-129            [-1, 96, 2, 15]          55,296\n",
      "     BatchNorm2d-130            [-1, 96, 2, 15]             192\n",
      "InvertedResidual-131            [-1, 96, 2, 15]               0\n",
      "          Conv2d-132           [-1, 576, 2, 15]          55,296\n",
      "     BatchNorm2d-133           [-1, 576, 2, 15]           1,152\n",
      "           ReLU6-134           [-1, 576, 2, 15]               0\n",
      "          Conv2d-135           [-1, 576, 2, 15]           5,184\n",
      "       AvgPool2d-136           [-1, 576, 2, 15]               0\n",
      "     BatchNorm2d-137           [-1, 576, 2, 15]           1,152\n",
      "           ReLU6-138           [-1, 576, 2, 15]               0\n",
      "          Conv2d-139           [-1, 160, 2, 15]          92,160\n",
      "     BatchNorm2d-140           [-1, 160, 2, 15]             320\n",
      "InvertedResidual-141           [-1, 160, 2, 15]               0\n",
      "          Conv2d-142           [-1, 960, 2, 15]         153,600\n",
      "     BatchNorm2d-143           [-1, 960, 2, 15]           1,920\n",
      "           ReLU6-144           [-1, 960, 2, 15]               0\n",
      "          Conv2d-145           [-1, 960, 2, 15]           8,640\n",
      "       AvgPool2d-146           [-1, 960, 2, 15]               0\n",
      "     BatchNorm2d-147           [-1, 960, 2, 15]           1,920\n",
      "           ReLU6-148           [-1, 960, 2, 15]               0\n",
      "          Conv2d-149           [-1, 160, 2, 15]         153,600\n",
      "     BatchNorm2d-150           [-1, 160, 2, 15]             320\n",
      "InvertedResidual-151           [-1, 160, 2, 15]               0\n",
      "          Conv2d-152           [-1, 960, 2, 15]         153,600\n",
      "     BatchNorm2d-153           [-1, 960, 2, 15]           1,920\n",
      "           ReLU6-154           [-1, 960, 2, 15]               0\n",
      "          Conv2d-155           [-1, 960, 2, 15]           8,640\n",
      "       AvgPool2d-156           [-1, 960, 2, 15]               0\n",
      "     BatchNorm2d-157           [-1, 960, 2, 15]           1,920\n",
      "           ReLU6-158           [-1, 960, 2, 15]               0\n",
      "          Conv2d-159           [-1, 160, 2, 15]         153,600\n",
      "     BatchNorm2d-160           [-1, 160, 2, 15]             320\n",
      "InvertedResidual-161           [-1, 160, 2, 15]               0\n",
      "          Conv2d-162           [-1, 960, 2, 15]         153,600\n",
      "     BatchNorm2d-163           [-1, 960, 2, 15]           1,920\n",
      "           ReLU6-164           [-1, 960, 2, 15]               0\n",
      "          Conv2d-165           [-1, 960, 2, 15]           8,640\n",
      "       AvgPool2d-166           [-1, 960, 2, 15]               0\n",
      "     BatchNorm2d-167           [-1, 960, 2, 15]           1,920\n",
      "           ReLU6-168           [-1, 960, 2, 15]               0\n",
      "          Conv2d-169           [-1, 320, 2, 15]         307,200\n",
      "     BatchNorm2d-170           [-1, 320, 2, 15]             640\n",
      "InvertedResidual-171           [-1, 320, 2, 15]               0\n",
      "          Conv2d-172          [-1, 1280, 2, 15]         409,600\n",
      "     BatchNorm2d-173          [-1, 1280, 2, 15]           2,560\n",
      "           ReLU6-174          [-1, 1280, 2, 15]               0\n",
      "          Linear-175                 [-1, 1024]       1,311,744\n",
      "          Linear-176                  [-1, 527]         540,175\n",
      "================================================================\n",
      "Total params: 4,075,215\n",
      "Trainable params: 4,075,215\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 114.07\n",
      "Params size (MB): 15.55\n",
      "Estimated Total Size (MB): 129.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = load_model(cfg.dataset.dataset, cfg.model.model)\n",
    "model = model_func(input_shape=input_shape, num_classes=cfg.dataset.num_classes)\n",
    "model = model.cuda()\n",
    "summary(model, input_shape)\n",
    "\n",
    "if cfg.hardware.nb_gpu > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Tensorboard and checkpoint --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare suffix\n",
    "# normale training parameters\n",
    "sufix_title = ''\n",
    "sufix_title += f'_{cfg.train_param.learning_rate}-lr'\n",
    "sufix_title += f'_{cfg.train_param.supervised_ratio}-sr'\n",
    "sufix_title += f'_{cfg.train_param.batch_size}-bs'\n",
    "sufix_title += f'_{cfg.train_param.seed}-seed'\n",
    "\n",
    "# fixmatch parameters\n",
    "sufix_fm = f'_{cfg.fixmatch.mask_threshold}-mTh'\n",
    "sufix_fm += f'_{cfg.fixmatch.guess_threshold}-gTh'\n",
    "sufix_fm += f'_{cfg.fixmatch.lambda_s}-ls'\n",
    "sufix_fm += f'_{cfg.fixmatch.lambda_u}-lu'\n",
    "\n",
    "# mixup parameters\n",
    "sufix_mixup = ''\n",
    "if cfg.mixup.use:\n",
    "    sufix_mixup = '_mixup'\n",
    "    if cfg.mixup.max: sufix_mixup += \"-max\"\n",
    "    if cfg.mixup.label: sufix_mixup += \"-label\"\n",
    "    sufix_mixup += f\"-{cfg.mixup.alpha}-a\"\n",
    "\n",
    "# SpecAugment parameters\n",
    "sufix_sa = ''\n",
    "if cfg.specaugment.use:\n",
    "    sufix_sa = '_specAugment'\n",
    "    sufix_sa += f'-{cfg.specaugment.time_drop_width}-tdw'\n",
    "    sufix_sa += f'-{cfg.specaugment.time_stripe_num}-tsn'\n",
    "    sufix_sa += f'-{cfg.specaugment.freq_drop_width}-fdw'\n",
    "    sufix_sa += f'-{cfg.specaugment.freq_stripe_num}-fsn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Tensorboard logging --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard log at:  ../../tensorboard/audioset-unbalanced/fixmatch/MobileNetV2/2021-04-29_21:17:52_MobileNetV2__0.003-lr_0.1-sr_64-bs_1234-seed_125000-e_0.75-mTh_0.5-gTh_1.0-ls_1.0-lu__None\n"
     ]
    }
   ],
   "source": [
    "tensorboard_sufix = sufix_title + f'_{cfg.train_param.nb_iteration}-e' + sufix_fm + sufix_sa + f'__{cfg.path.sufix}'\n",
    "tensorboard_title = f'{get_datetime()}_{cfg.model.model}_{tensorboard_sufix}'\n",
    "log_dir = f'{cfg.path.tensorboard_path}/{cfg.model.model}/{tensorboard_title}'\n",
    "print('Tensorboard log at: ', log_dir)\n",
    "\n",
    "tensorboard = mSummaryWriter(log_dir=log_dir, comment=model_func.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Optimizer, callbacks, loss and checkpoint --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: fixmatch | audioset-unbalanced\n",
      "loading dataset: fixmatch | audioset-unbalanced\n",
      "checkpoint initialise at:  /users/samova/lcances/semi-supervised/model_save/audioset-unbalanced/fixmatch/MobileNetV2/MobileNetV2__0.003-lr_0.1-sr_64-bs_1234-seed__None\n",
      "name:  MobileNetV2__0.003-lr_0.1-sr_64-bs_1234-seed__None\n",
      "mode:  max\n"
     ]
    }
   ],
   "source": [
    "optimizer = load_optimizer(cfg.dataset.dataset, 'fixmatch', learning_rate=cfg.train_param.learning_rate, model=model)\n",
    "callbacks = load_callbacks(cfg.dataset.dataset, 'fixmatch', optimizer=optimizer, nb_epoch=cfg.train_param.nb_iteration)\n",
    "loss_sup = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "loss_unsup = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "lambda_u = Warmup(cfg.fixmatch.lambda_u, cfg.fixmatch.warmup_length, sigmoid_rampup)\n",
    "\n",
    "checkpoint_sufix = sufix_title + sufix_mixup + sufix_sa + f'__{cfg.path.sufix}'\n",
    "checkpoint_title = f'{cfg.model.model}_{checkpoint_sufix}'\n",
    "checkpoint_path = f'{cfg.path.checkpoint_path}/{cfg.model.model}/{checkpoint_title}'\n",
    "checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Metrics and print formater --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = DotDict(\n",
    "    fscore_s=FScore(),\n",
    "    fscore_u=FScore(),\n",
    "    acc_s=BinaryAccuracy(),\n",
    "    acc_u=BinaryAccuracy(),\n",
    "    avg_fn=ContinueAverage(),\n",
    ")\n",
    "\n",
    "val_metrics = DotDict(\n",
    "    fscore_s=FScore(),\n",
    "    acc_s=BinaryAccuracy(),\n",
    "    mAP_fn=MAP(),\n",
    "    avg_fn=ContinueAverage(),\n",
    ")\n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, train_formater, val_formater = get_train_format('audioset-fixmatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Augmentations ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_augmenter = SpecAugment(time_drop_width=cfg.specaugment.time_drop_width,\n",
    "                                  time_stripes_num=cfg.specaugment.sa_time_stripe_num,\n",
    "                                  freq_drop_width=cfg.specaugment.freq_drop_width,\n",
    "                                  freq_stripes_num=cfg.specaugment.req_stripe_num)\n",
    "\n",
    "mixup_fn = MixUpBatchShuffle(alpha=cfg.mixup.alpha, apply_max=cfg.mixup.max, mix_labels=cfg.mixup.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Training and Validation function --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_label(x_uw: Tensor) -> (Tensor, Tensor):    \n",
    "    logits_uw = model(x_uw)\n",
    "    pred_uw = activation(logits_uw)\n",
    "\n",
    "    nb_classes = cfg.dataset.num_classes\n",
    "    labels_u = (pred_uw > cfg.fixmatch.guess_threshold).float()\n",
    "#     labels_u = F.one_hot(pred_uw, cfg.dataset.num_classes)\n",
    "    return labels_u, pred_uw\n",
    "\n",
    "\n",
    "def confidence_mask(pred: Tensor, threshold: float, dim: int) -> Tensor:\n",
    "    max_values, _ = pred.max(dim=dim)\n",
    "    return (max_values > threshold).float()\n",
    "\n",
    "\n",
    "m_ = lambda x: x.mean(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, Sw, Uw, Us, start_time):\n",
    "    # aliases\n",
    "    M = metrics\n",
    "    T = tensorboard.add_scalar\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    x_sw, y_sw = Sw  # Supervised weak augmented\n",
    "    x_uw, y_uw = Uw  # Unsupervised weak augmented\n",
    "    x_us, y_us = Us  # Unsupervised strong augmented\n",
    "    \n",
    "    x_sw, y_sw = x_sw.cuda().float(), y_sw.cuda().float()\n",
    "    x_uw, y_uw = x_uw.cuda().float(), y_uw.cuda().float()\n",
    "    x_us, y_us = x_us.cuda().float(), y_us.cuda().float()\n",
    "    \n",
    "    # Apply mixup is needed\n",
    "    if cfg.mixup.use:\n",
    "        pass\n",
    "    \n",
    "    # Apply specaugment if needed\n",
    "    if cfg.specaugment.use:\n",
    "        pass\n",
    "    \n",
    "    # Use guess u label with prediction of weak augmentation of u\n",
    "    with torch.no_grad():\n",
    "        pseudo_y_uw, pred_uw = guess_label(x_uw)\n",
    "        mask = confidence_mask(pseudo_y_uw, cfg.fixmatch.mask_threshold, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute predictions\n",
    "    logits_sw = model(x_sw)\n",
    "    logits_uw = model(x_uw)\n",
    "    logits_us = model(x_us)\n",
    "\n",
    "    # Update model\n",
    "    loss_s = loss_sup(logits_sw, y_sw)\n",
    "    loss_u = loss_unsup(logits_us, pseudo_y_uw)\n",
    "    loss_u = torch.sum(loss_u, dim=-1)\n",
    "    loss_u = torch.sum(loss_u * mask)\n",
    "    loss = loss_s + lambda_u() * loss_u\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute metrics\n",
    "    with torch.no_grad():\n",
    "        fscore_s = M.fscore_s(torch.sigmoid(logits_sw), y_sw)\n",
    "        acc_s = M.acc_s(logits_sw, y_sw)\n",
    "        acc_u = M.acc_u(logits_uw, y_uw)\n",
    "        fscore_uw = M.fscore_u(torch.sigmoid(logits_uw), pseudo_y_uw)  # Use true label for monitoring purpose\n",
    "        avg_ce = M.avg_fn(loss.item())\n",
    "        \n",
    "        # logs\n",
    "        print(train_formater.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            e, cfg.train_param.nb_iteration,\n",
    "            \"\", m_(avg_ce),\n",
    "            \"\", m_(acc_s), m_(fscore_s), m_(acc_u), m_(fscore_uw), 0.0,\n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "        \n",
    "        T(\"train/loss\",   loss.item(), epoch)\n",
    "        T(\"train/loss_s\", loss_s.item(), epoch)\n",
    "        T(\"train/loss_u\", loss_u.item(), epoch)\n",
    "        T(\"train/labels_used\", mask.mean().item(), epoch)\n",
    "        \n",
    "        T('train/fscore_s', m_(fscore_s), epoch)\n",
    "        T('train/fscore_uw', m_(fscore_uw), epoch)\n",
    "        T('train/acc_s', m_(acc_s), epoch)\n",
    "        T('train/acc_u', m_(acc_u), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    # aliases\n",
    "    M = val_metrics\n",
    "    T = tensorboard.add_scalar\n",
    "    nb_batch = len(val_loader)\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y = y.cuda().float()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_sup(logits, y)\n",
    "\n",
    "            pred = torch.sigmoid(logits)\n",
    "            fscore = M.fscore_s(pred, y)\n",
    "            acc = M.acc_s(pred, y)\n",
    "            mAP = M.mAP_fn(pred.cpu().reshape(-1), y.cpu().reshape(-1))\n",
    "            avg_ce = M.avg_fn(loss.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_formater.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                i, nb_batch,\n",
    "                \"\", m_(avg_ce),\n",
    "                \"\", m_(acc), m_(fscore), 0.0, 0.0, m_(mAP),\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    T(\"val/Lce\", m_(avg_ce), epoch)\n",
    "    T(\"val/f1\", m_(fscore), epoch)\n",
    "    T(\"val/acc\", m_(acc), epoch)\n",
    "    T(\"val/mAP\", m_(mAP), epoch)\n",
    "\n",
    "    T(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    T(\"max/acc\", maximum_tracker(\"acc\", m_(acc)), epoch)\n",
    "    T(\"max/f1\", maximum_tracker(\"f1\", m_(fscore)), epoch)\n",
    "    T('max/mAP', maximum_tracker('mAP', m_(mAP)), epoch)\n",
    "\n",
    "    return avg_ce, fscore, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                Epoch -       /       - Losses: ce        - metrics: acc_s        F1_s         acc_u        F1_u         mAP          - Time  \n",
      "\n",
      "\u001b[1;4mValidation:      1     -   318 / 319   -         23225.6818 -          5.094e-01    1.024e-02    0.000e+00    0.000e+00    5.166e-03    - 210.5409\u001b[0m\n",
      "Training:        500   -   499 / 125000 -         417.1603  -          9.973e-01    5.510e-01    9.976e-01    1.000e+00    0.000e+00    - 683.7227\n",
      "\u001b[1;4mValidation:      501   -   318 / 319   -         932.1062  -          9.954e-01    2.471e-01    0.000e+00    0.000e+00    1.532e-01    - 56.2590\u001b[0mm\n",
      "\n",
      " better performance: saving ...\n",
      "Training:        1000  -   999 / 125000 -         392.6789  -          9.974e-01    5.725e-01    9.976e-01    1.000e+00    0.000e+00    - 1067.2493\n",
      "\u001b[1;4mValidation:      1001  -   318 / 319   -         912.0537  -          9.953e-01    2.564e-01    0.000e+00    0.000e+00    1.689e-01    - 56.2886\u001b[0m\n",
      "\n",
      " better performance: saving ...\n",
      "Training:        1500  -  1499 / 125000 -         373.6131  -          9.975e-01    5.891e-01    9.979e-01    1.000e+00    0.000e+00    - 1436.0183\n",
      "\u001b[1;4mValidation:      1501  -   318 / 319   -         843.5739  -          9.958e-01    2.705e-01    0.000e+00    0.000e+00    1.778e-01    - 56.2113\u001b[0m\n",
      "\n",
      " better performance: saving ...\n",
      "Training:        2000  -  1999 / 125000 -         363.2408  -          9.975e-01    5.960e-01    9.979e-01    1.000e+00    0.000e+00    - 1838.9164\n",
      "\u001b[1;4mValidation:      2001  -   318 / 319   -         875.3528  -          9.958e-01    2.777e-01    0.000e+00    0.000e+00    1.820e-01    - 55.1380\u001b[0m\n",
      "\n",
      " better performance: saving ...\n",
      "Training:        2500  -  2499 / 125000 -         356.0270  -          9.975e-01    6.046e-01    9.979e-01    1.000e+00    0.000e+00    - 2263.0464\n",
      "\u001b[1;4mValidation:      2501  -   318 / 319   -         823.8362  -          9.957e-01    2.758e-01    0.000e+00    0.000e+00    1.865e-01    - 55.4754\u001b[0m\n",
      "\n",
      " better performance: saving ...\n",
      "Training:        2923  -  2922 / 125000 -         348.0732  -          9.976e-01    6.101e-01    9.980e-01    1.000e+00    0.000e+00    - 2833.2962\r"
     ]
    }
   ],
   "source": [
    "# -------- Training loop --------\n",
    "if cfg.train_param.resume:\n",
    "    checkpoint.load_last()\n",
    "\n",
    "start_iteration = checkpoint.epoch_counter\n",
    "end_iteration = cfg.train_param.nb_iteration\n",
    "\n",
    "train_iterator = iter(train_loader)\n",
    "start_time = time.time()\n",
    "\n",
    "print(header)\n",
    "\n",
    "for e in range(start_iteration, end_iteration):\n",
    "    # Validation every 500 iteration\n",
    "    if e % 500 == 0:\n",
    "        val_avg_ce, val_fscore, val_mAP = val(e)\n",
    "        print('')\n",
    "        checkpoint.step(m_(val_mAP))\n",
    "        tensorboard.flush()\n",
    "\n",
    "    # Perform train\n",
    "    train(e, *next(train_iterator), start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Save the hyper parameters and the metrics --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'dataset': cfg.dataset.dataset,\n",
    "    'model': cfg.model.model,\n",
    "    'supervised_ratio': cfg.train_param.supervised_ratio,\n",
    "    'batch_size': cfg.train_param.batch_size,\n",
    "    'nb_iteration': cfg.train_param.nb_iteration,\n",
    "    'learning_rate': cfg.train_param.learning_rate,\n",
    "    'seed': cfg.train_param.seed,\n",
    "    \n",
    "    'threshold': cfg.fixmatch.threshold,\n",
    "    'lambda_s': cfg.fixmatch.lambda_s,\n",
    "    'lambda_u': cfg.fixmatch.lambda_u,\n",
    "    \n",
    "    'mixup': cfg.mixup.use,\n",
    "    'mixup-alpha': cfg.mixup.alpha,\n",
    "    'mixup-max': cfg.mixup.max,\n",
    "    'mixup-label': cfg.mixup.label,\n",
    "    \n",
    "    'specaugment(sa)': cfg.specaugment.use,\n",
    "    'sa_time_drop_width': cfg.specaugment.time_drop_width,\n",
    "    'sa_time_stripe_num': cfg.specaugment.time_stripe_num,\n",
    "    'sa_freq_drop_width': cfg.specaugment.freq_drop_width,\n",
    "    'sa_freq_stripe_num': cfg.specaugment.freq_stripe_num,\n",
    "}\n",
    "\n",
    "# convert all value to str\n",
    "hparams = dict(zip(hparams.keys(), map(str, hparams.values())))\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc\": maximum_tracker.max[\"acc\"],\n",
    "    \"max_f1\": maximum_tracker.max[\"f1\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
