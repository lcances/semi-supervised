{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/lcances/.miniconda3/envs/dct/bin/python'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"esc10\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"cnn03\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=1.0, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=100, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.003, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--preload_dataset\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, checkpoint_path='supervised', checkpoint_root='../model_save/', dataset='esc10', dataset_root='../datasets', from_config='', learning_rate=0.003, model='cnn03', nb_epoch=100, num_classes=10, preload_dataset=False, resume=False, seed=1234, supervised_ratio=1.0, tensorboard_path='supervised', tensorboard_root='../tensorboard/', tensorboard_sufix='', train_folds=[1, 2, 3, 4], val_folds=[5])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../datasets'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "args.dataset_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "################################################################################\n### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n################################################################################\n\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already downloaded and verified.\nDataset already downloaded and verified.\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"supervised\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(train_loader.dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from augmentation_utils.spec_augmentations import Noise\n",
    "\n",
    "\n",
    "class NoiseLayer(nn.Module):\n",
    "    def __init__(self, ratio: float = 1.0, init_snr: float = 20):\n",
    "        super().__init__()\n",
    "        self.register_parameter(name='snr', param=Parameter(torch.tensor(20.0)))\n",
    "        self.dropout = nn.Dropout(1.0 - ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.dropout.p == 1.0:\n",
    "            return x\n",
    "\n",
    "        # Create the noise\n",
    "        noise = torch.randn(size=x.size()).cuda()\n",
    "        noise *= self.snr\n",
    "\n",
    "        # Create the application mask\n",
    "        mask = torch.ones(x.size()[0])\n",
    "        mask = self.dropout(mask)\n",
    "        mask = torch.clamp(mask, 0, 1)\n",
    "        mask = mask.cuda()\n",
    "\n",
    "        for i, m in enumerate(mask):\n",
    "            noise[i] = noise[i] * m\n",
    "\n",
    "        y = x + noise\n",
    "        y = torch.clamp(y, x.min(), x.max())\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisier = NoiseLayer()\n",
    "noisier = noisier.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor(20., device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in noisier.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f48a4125a00>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 2160x720 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"146.98344pt\" version=\"1.1\" viewBox=\"0 0 1708.125 146.98344\" width=\"1708.125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 146.98344 \nL 1708.125 146.98344 \nL 1708.125 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 123.105315 \nL 787.834091 123.105315 \nL 787.834091 10.116494 \nL 26.925 10.116494 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p245c8130cd)\">\n    <image height=\"113\" id=\"imagedaee7a5f79\" transform=\"scale(1 -1)translate(0 -113)\" width=\"761\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-10.105315\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfdd0d084fb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"27.807725\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(24.626475 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.080242\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(109.717742 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.352758\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(194.809008 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"292.625274\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(283.081524 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"380.897791\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(371.354041 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.170307\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(459.626557 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"557.442823\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(547.899073 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"645.71534\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 350 -->\n      <g transform=\"translate(636.17159 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"733.987856\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(724.444106 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mfc6459fa9b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfc6459fa9b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfc6459fa9b\" y=\"46.308225\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 50.107444)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfc6459fa9b\" y=\"81.617232\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40 -->\n      <g transform=\"translate(7.2 85.416451)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfc6459fa9b\" y=\"116.926238\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(7.2 120.725457)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 123.105315 \nL 26.925 10.116494 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 787.834091 123.105315 \nL 787.834091 10.116494 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 123.105315 \nL 787.834091 123.105315 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 10.116494 \nL 787.834091 10.116494 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 940.015909 123.105315 \nL 1700.925 123.105315 \nL 1700.925 10.116494 \nL 940.015909 10.116494 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa0a877eeb1)\">\n    <image height=\"113\" id=\"imaged55d192ef3\" transform=\"scale(1 -1)translate(0 -113)\" width=\"761\" x=\"940.015909\" xlink:href=\"data:image/png;base64,\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\" y=\"-10.105315\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"940.898634\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0 -->\n      <g transform=\"translate(937.717384 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1029.171151\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 50 -->\n      <g transform=\"translate(1022.808651 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1117.443667\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 100 -->\n      <g transform=\"translate(1107.899917 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1205.716183\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 150 -->\n      <g transform=\"translate(1196.172433 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1293.9887\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 200 -->\n      <g transform=\"translate(1284.44495 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1382.261216\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 250 -->\n      <g transform=\"translate(1372.717466 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1470.533732\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 300 -->\n      <g transform=\"translate(1460.989982 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1558.806249\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 350 -->\n      <g transform=\"translate(1549.262499 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1647.078765\" xlink:href=\"#mfdd0d084fb\" y=\"123.105315\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 400 -->\n      <g transform=\"translate(1637.535015 137.703752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"940.015909\" xlink:href=\"#mfc6459fa9b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0 -->\n      <g transform=\"translate(926.653409 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"940.015909\" xlink:href=\"#mfc6459fa9b\" y=\"46.308225\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 20 -->\n      <g transform=\"translate(920.290909 50.107444)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"940.015909\" xlink:href=\"#mfc6459fa9b\" y=\"81.617232\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 40 -->\n      <g transform=\"translate(920.290909 85.416451)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"940.015909\" xlink:href=\"#mfc6459fa9b\" y=\"116.926238\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 60 -->\n      <g transform=\"translate(920.290909 120.725457)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 940.015909 123.105315 \nL 940.015909 10.116494 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 1700.925 123.105315 \nL 1700.925 10.116494 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 940.015909 123.105315 \nL 1700.925 123.105315 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 940.015909 10.116494 \nL 1700.925 10.116494 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p245c8130cd\">\n   <rect height=\"112.988821\" width=\"760.909091\" x=\"26.925\" y=\"10.116494\"/>\n  </clipPath>\n  <clipPath id=\"pa0a877eeb1\">\n   <rect height=\"112.988821\" width=\"760.909091\" x=\"940.015909\" y=\"10.116494\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABqwAAACTCAYAAAD7qDjjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3da6wk+Vnf8d9TVX05l7ns7NrrxeuwBjkmBCmAkBE4QlYcJ9yESRQiI4FMhOS8gMgokcDwJuSdFSWIvELaANEmECyLi2whBDEGK0JKwDaYgL3Y6zhgr3fXuzv3c+nuqvr/8+J5qvv0mTmzc+acPj01+/1Io+6uW9f5q7e7n+1fPX/LOQsAAAAAAAAAAABYl2LdJwAAAAAAAAAAAIBXN36wAgAAAAAAAAAAwFrxgxUAAAAAAAAAAADWih+sAAAAAAAAAAAAsFb8YAUAAAAAAAAAAIC14gcrAAAAAAAAAAAArNWJfrAys+80s8+a2efN7H2ndVIAAAAAgAcPNSQAAACAo1jO+d52NCslfU7SOyQ9K+njkn4w5/yZ0zs9AAAAAMCDgBoSAAAAwJ2c5Aqrt0j6fM75CznnmaQPSHrn6ZwWAAAAAOABQw0JAAAA4EjVCfZ9vaQvHXj8rKRvvdMOQxvlsbZO8JQA8Cq2OZYkNdulJKkdxHKT1F0sWx66ajaZrz/I8q3b5tioW5S7Ax9etlisg08VyyzdeshuWe4iEsVi326dNXFKs7idtMr7EwEA7n83dfXlnPNr1n0e6AVqSAA4S9SQAID70J1qyJP8YHX440ta/ujxjczeI+k9kjTWpr7V3n6CpwSAVy/7ur8rSXrh71+QJO0+7m+5ucqyxt+Sm3Otbxxf7Iu9Yn4/R3GRB35bbNXzY6faCxjV8dbemizF/XlBYMv7z2z+rp+Gfqfa9SezJLVDX1dOu23i0FtJFs9T7vnt6Irfnv8bP//zn7uh9OdP39W4AADW6/fzr//Nus8BvUENCQBniBoSAHA/ulMNeZIfrJ6V9IYDjx+X9NzhjXLOT0p6UpLO26V7mzALAKA88oIgxzt3u5FieZKiALAqlk1823S+kQ1iWRQPVsS2hVRW/uW+boulY/v95RRdl3br0nVte+D/OcXdOooODZIU69uuaBm1i2PPogJKfp7lNIqPWezf8nEBAMADiBoSAM4QNSQAoG9OMofVxyW9yczeaGZDSe+S9OHTOS0AAAAAwAOGGhIAAADAke75Cqucc2NmPy7p9ySVkn455/zpUzszAMCSXHrGIAJli7jagT7iFsm1YttbNZikauCptCZaNgyG3uy7qlrt7438UM2ijYNvlKUuwdaF4LrnaYrFtpHGm+9XLFJ6ue1ONA7TpewmpfdF16I1xLzVwzASfJPpHccCAAD0DzUkAJwtakgAQN+cpCWgcs6/I+l3TulcAAB3kuPLflcDxMNcF1K0XrCx357f3pckbY9m890njb/ljysvNsoiaTb2L/U3Nnwy3tnMtynLNC9O6t2YmXfaNTKPA46SVEe/8Sg2cpxUVrnYrpswdxrFR2vzoiTHoZvx8t+k6zuvOBwAAKB/qCEB4AxRQwIAeuYkLQEBAAAAAAAAAACAEzvRFVYAgLNT7HuqrY1EWdcKQaOk0ban3M5vTZb2mTSVcrR92Bp6Um5U+nGuT8fz9g/bkZI7mElrom1Dtenbp2jB0LWHSG2hdhAT7U6XWzeUW7VS7J/3l9dplBYxuG5S3Uj1VfsRpbt0QXrppaMHAwAAAABwR9SQAIC+4QorAAAAAAAAAAAArBVXWAFAT1jrqbQc79y5PDA5bSTgprWvrBtPpDV1qRTrmi3PKAyqmEC3LdSk5dxCE/t1rc4lyQpPrFmOdZGSa6blYlLdbvLekGbl4hixrtiJCX830mLy3UjHWRwzVdHHfMTHEwAAAACcBDUkAKBvuMIKAAAAAAAAAAAAa0X8AAB6Io+88XgaRiou+oIXVVJqPX/QRtptY+S9xvPQVLeeaut6jXeaVGgUSbkUq1Lsn5LJlgNvaqbRKzz5R0c5TIpQm/J0Of+QZ4Vs6Km6eUiuS8QlyeJ8i/1IztW+qpjlOG8+ngAAAADgJKghAQB9w7s5APSE7fs38hQT5haVf5kvLMsK/5J+bsMnzH14Y0+SFxQ7s5Ek6ebEb6vS9+sKDWlRZAwrL2Ams8F8XV17sVINu8IkCoQizQuYpomqoysoskk7UZR0RcpWTIY7apXrrqiJSXXj06hrOaGXD1U6AAAAAIBjoYYEAPQNLQEBAAAAAAAAAACwVlxhBQA9kTY9sda1RSgj5VaWSaOBp9rGkW7brLydw6ytlAeeNKti4tv58bKpjGXtoYlzyyJrb+rPNx4v7zfZH/r+bTlPxw22/fmamX+s5GaRbkujSMxFmk+tSYNo9RCbtUN/fovAns0aLTefAAAAAAAcBzUkAKBvuMIKAAAAAAAAAAAAa8UVVgDQE7mKXt2RKHvonPcY/5oLlzUqPRV3fTZe2ufRjRtqkvcPv14vrzs3mOrqdFOSlOQHncXkumWR1MZsuE0k55rG1w1H3ge9qtp5//CuR3kRyb1srdJmPFFry7e2uG9NTJzrp6+iIRMHAAAAAKeBGhIA0DdcYQUAAAAAAAAAAIC14gorAOiJcs9TadZ67GxvNpivuzTYlSRtlJFci0beX7/5nAZx//n6oiTpau3777QjPbpxw48dfcRvRIJuVDS6uTHyZZG4u7y3JUnaP/C8Xf/ywdifYxYJuulkqK6BeLHh0bccabvcFFL0Jy93PDdRTmJdxCi6JCAAAAAA4N5QQwIA+oYfrACgZ0qfm1Y3XzgnSfrK9q6+4dxzkqRL1VckSdfbzfn2k+zFwbdtPSNJ2jI/wG4e6lrrBcRuGsa2w9h/Q1drX7cz9qLjytgfv7S/7be7W/MJc8cxYe/mKJ50a38+CW/d+u0kipS2LWTR2WG24R9D08ILmnYYRUdJsQEAAAAAp4EaEgDQF7ybAwAAAAAAAAAAYK24wgoAeiJbN8msLS1P2XSh8slznxi+vLSulelcMZG0SMWNo73DpmrtJo+zjQs/5lj1/Jg7kVjbiGWPjHYkSZPWPzoGZasUs/fu1Z6q6ybX3RrM1ERvhptTf46tsT9/m0xNTMybo+XDbOh39h/xfQY7Y3VBOwAAAADA8VFDAgD6hiusAAAAAAAAAAAAsFZcYQUAfRXJst3ZUF838v7j3zbalyRtFp5We77Z0bWuD3hkFAYWk9xKen11TZL0Uuu9zL9cP+T71RfnE+t2iug1/tqxp+Ru1ON5Am6z8uTbLPnHys3ZSHUk4A7vPxy0uhnrZvvekzwCeyqa+NPK5QQgAAAAAOCEqCEBAPc5rrACAAAAAAAAAADAWnGFFQD0TIpe3Rp7pKxpC11rtyRJ19N1SdLAPH02tkKvKX37Onv0bLPo1lW60k4lSeeKK3Hr6bqL5Z5eGnhi7kqzLUkqI1VXxO31wSI9N41U3POTC5Kk7Wo6T87dmHkf853ZKPbP2hx5mm5/OIwDeBouDiPrGpMDAAAAAE6EGhIA0Bf8YAUAfVH5l/ccXRJe9zpvxfDG81f0+emjkqSBeUHxtYPLkqRLRVJ0StD15Dte8VpBmzbVJAqCJP+yX0aPiHPFvurSt9+LSXXbQ9sMrNWNxguJOk5qUPiztdl0rvSJeoexbHvghc2kHejqZMOPVfnJ2F5MBpz82MU0HXd0AAAAAAAHUUMCAHqGloAAAAAAAAAAAABYK66wAoCe6CaRtW5S2eyPHxtfn7dYeKk5L2kxEe61lHWx8HWPH3rHT7lQG0m3vezbjM3bOVwopqrlibevjtlsJ9knt92NtNwkDVTFsa9MvZ1EEcfbqGptFN6yoYlUXtfeYbuYKsW57029ncONh+P5X/LluSJPAQAAAAAnQQ0JAOgb3s0BAAAAAAAAAACwVlxhBQB9Eb25y5knyF667BPa/t+t16iwmBS38rf1/52/VpL0xPAlPVFdlSTtZU+5zSKldjMP9HDhPcG79ELXY3wvVxpbLWnRU7yMBN3N1nuOX6635n3HN0rfdr/1BN2V2aYkn1C3mzC30/Ujl6RB5ffTtkf+0sDTcu3Y7nZUAAAAAAC3Qw0JAOgZrrACAAAAAAAAAADAWnGFFQD0ReGJsXboSbhUezLtSzcu6NGNG5KkRwY7kqStSL1da7f0TPa3+i7tVijND3kzeXKt6y1+udmO/Wc6V3gv8mvJU27Pzh72bZNvWxVJL018+8M9xmdtpRRJuzrWTZvFR87uzI8x65Y1vl859b+tqPMxBgYAAAAAcAtqSABAz7ziFVZm9gYz+0Mze9rMPm1m743ll8zsI2b2TNw+tPrTBQAAAADcz6ghAQAAANyLu7nCqpH0b3LOf2pm5yR90sw+IulHJH005/x+M3ufpPdJ+qnVnSoAvLqlgafM2rEnx0bb0/m6gXnirbC0tE+ppEulJ+YejrRbId9/aEk3I+n25faCJKmOJN0kZ12ut5eO1R37ershSZqmatF3PLaZNX6Os1Tq6sS3GxS+X5s9LXdzMlJKnpdIabnPeNd3PFX0HwcAoMeoIQHgPkANCQDom1f8wSrn/Lyk5+P+TTN7WtLrJb1T0ttis6ckfUwUGwCwMtb6l/ZcebFwcdu/4l/a2FMdbRTq5G/rbTyWSV+qvQ1DXV2TJG3ZTJK0WdTziXYvFnuSpEnpE9a+0Fy45fmvN97WYVzERLrW6mbt7SBm8bxVFBZVkbQ58O0m0bKha91QFUmzKDZyXi4qzOfNVTlZLpoAAEB/UEMCwP2BGhIA0Dev2BLwIDN7QtI3SfpjSY9GIdIVJK89Yp/3mNknzOwTtaa32wQAAAAA8ACihgQAAABwt+6mJaAkycy2Jf2GpJ/IOd8wu7tLbXPOT0p6UpLO2yVmQASAe5QHnjGwxt9/d6eeZPuaC5f1d7aelyR9++YzkqStiJltWqutmGh3ZDEprRbv3220dni53Yt1/nirmOpGTKa7l0a+bOT/w6hL3r3cnNNgo5W0SMztJT+nnWakndb3uzrzVF2XxJu1pXZrX/fynq+bVP64aOJj4i4/YwAAwP2LGhIA1osaEgDQN3d1hZWZDeSFxq/mnH8zFn/FzB6L9Y9JenE1pwgAAAAA6BNqSAAAAADH9YpXWJnH4H5J0tM55587sOrDkt4t6f1x+6GVnCEAQJJkM+/JXU4jHXfdJ6R9buuCdNG3mWSfAPeceVptbItkQsqePNsuPIlWWqE2R5/v0pNvRUx9O7BW4zjGjbgtY8LcmzFh7qQcaLPwXuZXmq2lcx0Vzfz+YOT7TaNH+b4NFkm55BPsXin9cSxWUdN/HACAvqKGBID7AzUkAKBv7qYl4Fsl/bCkvzCzT8Wyn5EXGR80sx+V9EVJP7CaUwQAAAAA9Ag1JAAAAIBje8UfrHLOfyTpqEawbz/d0wEAvBLrgmOFR8mu7m3oc3uvkySl6A3+5vFzkqSLxd58v4uFJ+CuJF82yaXa+du73+4lT75dTlvajb7jV5ptSdLN6Ee+00Zf8naoOke6LXqM70Rf8Uk70LT1j5g2Rd/zYpF42515n/LL1/zYxVVP9alLxzWk4wAA6CtqSAC4v1BDAgD64m6usAIA3A9i4lvF9/Dh2NssXNiY6IX9c5Kkc9VEkvTo4LokaaBWQ/NJbS/Hl/4t8xYMA0sax7prMdHtTF487KaRrrXeoqGbMHcvJsAdxD777VAvz3ybSePFQhPFTteu4aBJ4x85gyLNC4/B0Ns+TIaHJsql1gAAAACAk6GGBAD0TPHKmwAAAAAAAAAAAACrwxVWANATOYJj5czvTPY80daev7XjTtdmQZI2o43DMCJng+gHca5odaHw7S7GxLdfajyldr6YzNNxndS1fogE3MPDHW1V0SIiUnIpTrLOhSaVJ+auT71FhJk/V92WmkVSLqXumH7TDvxxs1VpeOfhAAAAAADcATUkAKBvuMIKAAAAAAAAAAAAa8UVVgDQE/U5T5vNg2/R4/v63obecO6aJOlStStJGpv3Jh9aq4cjHXep8IzCJHcpOdOmdRm0WeznvcULpXnCrog03WYk6CZpcMu5ddvsR4/yvWagYenHujDa9/OMlJxv7+c+HfpzzJbn7QUAAAAAnBA1JACgb7jCCgAAAAAAAAAAAGvFFVYA0BMRKFOE1LR5zlNvf+uhq3pk6Km4RwfXJUlfNbgqSXq42J/v/1zr0bM23vq3rNH15Ot3sy/7UnNRkvRSc14p+oxfbby3+PXG022z5Nu22eb391tPzO01nrabNpUmrSfmpq1vc2MyliRN6mred7ye+bpi6o/Lqf+R4xd2o1s6AAAAAOBeUEMCAPqGH6wAoCfKibdHKKf+xX5/4rcXh/saFd6+4WbrBcHN5F/s61xqkgdL67rWCxeLPY1jv063zbV2c97OoY2io0n++EY9nh+nKzZe2D2/dJyUbT55bqdp/Tg5m8wW20mSRSHUnU67OaCzAwAAAACcADUkAKBvaAkIAAAAAAAAAACAteIKKwDoiWbT02kRVlN72Senfeb8a9Rc9IWjopEkbcYkuQ9XO/P9BzEZ7rlo8VBamrdsmHVJuMgxTNNgnoorI003jSRc9xyF5XlizqLXRI60W50KTWaDpXVdEq4oktouKRdtHcpIxQ13/LmK/Ub5eMMDAAAAADiAGhIA0DdcYQUAAAAAAAAAAIC14gorAOiJXCx35M6DRX6sSZ4/6HqGX283JXnabWieZrtY7i3tP8kD1TFR7m7ypN212G8vDbXX+uS3RaTbmkjL1ZGIm6VS16fer7xuu17lfo45m6oyUnWNr6trf66UTE3ty9INT9AVdezXxShKuo8DAAAAwElQQwIA+oYrrAAAAAAAAAAAALBWXGEFAD2x8eVdSZK9+YLfNp4ga9pi3ke86w3eytddbbZuOU5b+LZbxVStPME2SZ5S20vD+eOu3/jNZixJujz1Y1XRx3yWqnlirkvFtWmRg2iix3gT6bi28ccpm3Lct+g/3rVJz5GKs7ql/zgAAAAAnAA1JACgb/jBCgB6orngLReKmFw26gnduLGhl0czSVJ93r/Yl/FVvc6FUmzYtWqYRQuHSR7Mj30zeUFRR4FRKqmJ1hDT1pcVcczdejRfvlf7MdrUFT7RpiGbZjPfr5sUtys2cjblKJS6pg2tP/28nUNzccwlwAAAAABwAtSQAIC+4b0cAAAAAAAAAAAAa8UVVgDQE6nyLFl0XlAxidTZqNSNiSfWnr7xOknSlbG3XhgVjTZKT851E+B2LR8uVIsJdF+uz/l+te+30ww1i6TcTqThdmZ+m6J1w6SpNIlJcLuUW921bmgLpWjn0KXiFCm5PCukNvbwbhKyuI1TAwAAAACcEDUkAKBvuMIKAAAAAAAAAAAAa8UVVgDQE2kQk8xGgszq6OE9KbW36w28r21sSJKamLh2XNZ6aLgvSTo3mCwd78XZeRXmPcV3Wk++dRPflpY1i17i+81gab86jj2tK7WHJ8WNxyYpRQIuT8rlP+TATLhF/A3ltFvnK7OZAAAAAAD3jhoSANA3XGEFAAAAAAAAAACAteIKKwDoiXLmTboj0KY09jvlpam2tzz59vj2NUnSa8c3JUmzVKmyduk4bWQVBtbOU3Fdr/GUfd0jwx09sXFZknS12ZQkfXH3IUnSpPW03PZwpusTT+V1fcfrdpGSq+X3y+3anziSd6kx5f3oW94up+OKONVyrz7GyAAAAAAADqOGBAD0DVdYAQAAAAAAAAAAYK24wgoA+iJScbmLGqR4nG2eSntpsi1J2ig9XXZpuKtR4Q3LS/Md2rzIKgwiOZei3/d+9uRbYXm+rown6hJ0bfQfL5Q1rppYFr3QY53Zosn4vJN4t+xAb/Fc+rJ2aEt/YzFtuj8PAAAAAHAvqCEBAD3DD1YA0BNFneLWH5fTmJDWsgalFwbTxt/Wr9feZmGaKhW2/LV9u5rN71+b+QS7k9b32629vUNXtEiLCXN3Z0NJi2KjaYt5kTObRRuHaOuQm0K5jsJjErdtV5BIZTdR7qS7jSeLmiON+HgCAAAAgJOghgQA9A0tAQEAAAAAAAAAALBWxA8AoCfajUiedVGDSJI1N4a6GZPRbo48+bYTKbfrsw01kWbbqDxW1wz3JUmjspkfuyo8QVfG7X4zUIpj7teejutScdNIwE2nAzW130+RjlvqwdDY0nnmKno1tKaccizr1nVJv+5xsWgDAQAAAAA4NmpIAEDf3PUVVmZWmtmfmdlvx+NLZvYRM3smbh9a3WkCAAAAAPqEGhIAAADAcRynJeB7JT194PH7JH005/wmSR+NxwCAFWlHhdpRoaLJKposqyWrpWJSKNX+b2cy0s5kpDqVqlOp0pK2BjNtDWYaFq2GRasmF2pyoTqV82MXllVY1vnhROeHE43KRjmbcjaVRVJZJM2aUrOmVNsWattCOZnMfP5bq5KsSv6p0n2yZP9XTEzFxGQz/yfLyqWUSynFv/njyvzfsLzdEAAAgH6hhgSANaKGBAD0zV39YGVmj0v6Hkm/eGDxOyU9FfefkvT9p3tqAAAAAIA+ooYEAAAAcFx3O4fVz0v6SUnnDix7NOf8vCTlnJ83s9febkcze4+k90jSWJsnOFUAeHWz1nt2p65Xd+mPrZFS7fmDwvLSPm0u5j3BN4fem7wyX9DkYt6bfLPydWXsPyvLeb/xUddaPJqDT2bejzylQopzkvm2XfvxPC3nkYi00fUdj00b06HTnPdUL2e+oro51aFNAABAv1BDAsCaUUMCAPrmFX+wMrPvlfRizvmTZva24z5BzvlJSU9K0nm7xGcHANyjXEaR0U0yG0WAJZt/y69bX3hz6hPmbg1n2hj6RLldkbEVhUVhSSm+5U+jtcN+64XEpB3IoiLYq4e+Te1P3EbRUVWtUvJlt0xu25qs7SbBjaJIXZHkBZIkFd2tn6LSwLdJo4oJcwEA6ClqSAC4P1BDAgD65m6usHqrpO8zs++WNJZ03sx+RdJXzOyxSMY9JunFVZ4oAAAAAKAXqCEBAAAAHNsrzmGVc/7pnPPjOecnJL1L0h/knH9I0oclvTs2e7ekD63sLAEAspRlKXsSLkkW/5Tl8TSTUjKltMiVDYtWlfm/SVtp0lbabYbabYZqUqlp/Eu5iH82/7czG2lnNlKdCtVp8XHRTaTbNKVyMv8Xy5TiX5mV459lk+VDWbe8fN6dai/5v+uTlY0jAABYLWpIALg/UEMCAPrmFX+wuoP3S3qHmT0j6R3xGAAAAACA26GGBAAAAHCku2kJOJdz/pikj8X9y5LefvqnBAC4HesmnO1mcjhwW131t/PJ0PuOD6rYeHzrcWbRa7yJNJy0mAx3Fv3E61TOD99NnJtjm7r2/dumVOp6jLexTUzcq9YOzJ4b5x2pvWJqi77jM1v62+b9xzcGdxoKAADQE9SQALA+1JAAgL45yRVWAAAAAAAAAAAAwIkd6worAMD65C5i0KXN8oF13Z1Y2D1OOtT3W1JVeGytUFYyX99EAq5LybWpOHA/UnGtp+K6/uZZUu7ud6m4ZvF8857j3fnWi3X5UFyiaPKhvzELAAAAAHDvqCEBAH3DD1YA0Be3KTIk/4Lejr2AGG3WkqRHz+1Ikp7YvqKtarq0/SMDXzcqaqX4dj9J3j7hpdk5SdJz++c1aX3ZtPWPiunAezDsVb68TcW8qJlOfVk9iY+VbPNCpGvr0A58a5uZrFkuRFJp87/FN7q1SAIAAAAAHAM1JACgZ2gJCAAAAAAAAAAAgLXiCisA6IlcxWSyXQCtPLDygqfizm1OJEmDoj3yOHvtUJIn4rqWDTutT7Q7jYNXRZLiEPMJc2P/svB7dWtqu4ly03KaLbc2b+1gTWQj5hP8miwSc4tJgLt2DtFCYlyRqAAAAACAE6CGBAD0De/lAAAAAAAAAAAAWCuusAKAnkiRjuuSZV1f71xlVSPvDf7QeF+SdH44me/XJI/RVYcScwNrVcvXFYeamo/LWlU8UbduZ+YJujoe24F9cl5Ox1mZF5Ppll2T8VjXmrp5fLuEXxp0/cdj25L+4wAAAABwEtSQAIC+4QorAAAAAAAAAAAArBVXWAFAT5RTj5dZ45EyOxB2S20si8RaEc2+C0vaqqa+fxdPC/WBBuabxUySNCv8Y6HJhWYRYet6lHe3baTeSstqDp2jRbot1YU0T8/Z0o0k5S4u0bUmj3VdgM/q5XMFAAAAABwPNSQAoG+4wgoAAAAAAAAAAABrxRVWANATuegiZFq6zZVUFp4mq+a3HjNrUqndZrR0nOJAX/EuFdfGwdqIqe23A80icdemYr69tOg1vjcZqql9m1Qfyj805v+kRTiuXvRP75J9xXS5p3qKT6VcFqIDOQAAAADcO2pIAEDf8IMVAPSE5WiV0E0y230xH+R5AbBbDyX5hLeSdGEwmRceg/iGv1l6gTGyRtPsB5k20cYhDp6yKUXPhVksm8y38eVlmRaT4na3TRQdyXxiXEmHqwZrJWvslmV+gO4x7RwAAAAA4CSoIQEAfUNLQAAAAAAAAAAAAKwVV1gBQE/M2zkc7nPQSindvvnBNFWqZ5tHHrOcT2rrbtbe+uHmbKwm0nH79cBvZ35b1/7RMZtUStHOYd66oenaM5gU5zRv3dBtk7VoRVHG8xeHzn/5tAAAAAAAx0QNCQDoG66wAgAAAAAAAAAAwFpxhRUA9ESE1eaTy3a3RW1qd/3t/MruchJuVDYal40kaVx5T/IiomeFZTWRYEsRuZtFU/P9ZjFhbpeKm0Uqro5EXG6LRYoteo13fcWtscV5xro8n+g3e3pOWuzf/S1NnNusISAHAAAAACdADQkA6BuusAIAAAAAAAAAAMBacYUVAPRELpcTZV1fbyVTseeJtZ2bY0lSWXjcbGNYa1J4um074mnDwtNyKRfzVFzKfltHIq7Npmmk4ZrW92sj5ZYaf5yTSXUR59KdW6Tj8uL+4VSftEjOzXuSd7vH35jGg1varAMAAAAA7h41JACgb/jBCgB6wlKO225Bd5uVB9EGISagtZgId1CkeeHRFRRN9oKiSYXqVC6tmzT+sTCpKzWpKzL8NsXj3BUWs2LRvqE+3EBrxwgAAAx7SURBVJ5hUVDMJ8W1RfmQD0+Ya8v758IoNgAAAADgBKghAQB9Q0tAAAAAAAAAAAAArBVXWAFAX8yTY8u3kmSzaJ2QltsydGk3SZoWfn9UeTuHNhXzBFw3OW7dLNo55EjMpW5S3WjdoFl3Aotzmp/HPLmXl9Jwd/0nlnGHOAUAAAAAnAw1JACgZ3g7BwAAAAAAAAAAwFpxhRUA9MRiwtlDj7NJhcfU0jRSbiN/e5/aIr42i/vzvuKp0KxZ7j/e9RrP2dTGxLjN1I+Vp5FxiLScNTa/f4ts3Xy5t/Qat1aytDyZbiqXd7f60B8LAAAAADgWakgAQN9whRUAAAAAAAAAAADWiiusAKAvIuhm7aHFZVauInkWCbSy9HRZUSQ10Vs8pUXyTZLMsqrSD9ZtU0dqrWnKeb/xXB9KxbVd7O1gv/FDt8qLXuKH/4xS3p9cUq6it/kg1nVJukGh43cvBwAAAADMUUMCAHqGH6wAoCfqLf8mnob+eP4FvdRi4tr4Ep8OtFkoC68IchQb3WS6khcj0qJG2BjVkqSJSZNoDaEoKKxZLjaskeY9G7qnP9iFoSuOuol+u02LfMs2XWGSuuKjLLgEGAAAAABOgBoSANA3d/VebmYXzezXzeyvzOxpM/s2M7tkZh8xs2fi9qFVnywAAAAA4P5HDQkAAADguO72Cqv/JOl3c87/zMyGkjYl/Yykj+ac329m75P0Pkk/taLzBIBXvaLNywu61For5ci35UjF7d8c+e3ljUUsrds9botpMZ/EVhc8FVcNo73DtFJ5xeN35b4t3XYsSdUk7jfLp2b5wGS4keJL8YlTb0s57pfTOPZ08bf4sQ/9rQAAoG+oIQFgzaghAQB984pXWJnZeUnfIemXJCnnPMs5X5P0TklPxWZPSfr+VZ0kAAAAAKAfqCEBAAAA3Iu7ucLqayS9JOm/mNnfk/RJSe+V9GjO+XlJyjk/b2avXd1pAgDmvb27vt7d8qx5Uq6bDLd7PHyxUnMuJqd9aObbR//vpIHOf84/BkbXDuUX8uIJupSb8nJizdKBdYfO0fItrclVb3eT45rark96pPO6vuNdAtAy6TgAAHqMGhIA7gPUkACAvrmbOawqSd8s6Rdyzt8kaVfeuuGumNl7zOwTZvaJWtN7PE0AAAAAQE9QQwIAAAA4tru5wupZSc/mnP84Hv+6vNj4ipk9Fsm4xyS9eLudc85PSnpSks7bJeIOAHCvDr+DxuNsUhF9vO3q8tv67DWtzr3upiRpVHlz793JUJK0P6k0/XZfV4y8//j+1JuFz17Y1MbzpSRp4+VIrHU9xrvUXHmwX/ihUysW2yU/jJqtuN3OSoM4Zu0btWNfN7noOYrRtUqjo8YBAADc76ghAeB+QA0JAOiZV/zBKuf8gpl9yczenHP+rKS3S/pM/Hu3pPfH7YdWeqYA8CpXTvwbfTnzL+TdF/yiMZXeqUFbX/Av782W3954c5q3eOjmoK1n/tZf7JTK57t1MeHufHLdA70Yjihy7EDLh24yXLvN/1LqlnWFiQ4UJjkKkXbsG9Xn/ICz8yXFBgAAPUUNCQD3B2pIAEDf3M0VVpL0ryT9qpkNJX1B0r+QtxP8oJn9qKQvSvqB1ZwiAAAAAKBnqCEBAAAAHMtd/WCVc/6UpG+5zaq3n+7pAACOkstofTCKRFsVk80OsqyJmFrcDK/5ukf+pFSqLiwd52K0ZbAk6c+9x0IXhhsdbMtwKOrWbWMH2kjMk2/dfgdDdTFLYrPZJd4Wy7s2DmW0oSgi3VfGNBXljO4/AAD0GTUkAKwfNSQAoG+KdZ8AAAAAAAAAAAAAXt0s57NLIJjZS5J2Jb18Zk/66vOIGN9VYnxXjzFeLcZ3tRjf1WJ8V48xXq0HdXy/Ouf8mnWfBB5M1JBn4kF9b7pfML6rxxivFuO7Wozv6jHGq8X4rtaDOr5H1pBn+oOVJJnZJ3LOt2sNgVPA+K4W47t6jPFqMb6rxfiuFuO7eozxajG+wL3hv53VYnxXi/FdPcZ4tRjf1WJ8V48xXi3Gd7VejeNLS0AAAAAAAAAAAACsFT9YAQAAAAAAAAAAYK3W8YPVk2t4zlcTxne1GN/VY4xXi/FdLcZ3tRjf1WOMV4vxBe4N/+2sFuO7Wozv6jHGq8X4rhbju3qM8Woxvqv1qhvfM5/DCgAAAAAAAAAAADiIloAAAAAAAAAAAABYqzP7wcrMvtPMPmtmnzez953V8z7ozOyvzewvzOxTZvaJWHbJzD5iZs/E7UPrPs++MLNfNrMXzewvDyw7cjzN7KfjNf1ZM/vH6znr/jhifH/WzL4cr+FPmdl3H1jH+B6Dmb3BzP7QzJ42s0+b2XtjOa/hU3CH8eU1fErMbGxmf2Jmfx5j/O9iOa/hU3CH8eU1fIrMrDSzPzOz347HvH6Be0QNuRrUkKeLGnK1qCFXixpytaghV48acrWoIc8GNeSyM2kJaGalpM9JeoekZyV9XNIP5pw/s/Inf8CZ2V9L+pac88sHlv17SVdyzu+Pwu6hnPNPresc+8TMvkPSjqT/mnP+hlh22/E0s6+X9GuS3iLpqyT9vqS/nXNu13T6970jxvdnJe3knP/DoW0Z32Mys8ckPZZz/lMzOyfpk5K+X9KPiNfwid1hfP+5eA2fCjMzSVs55x0zG0j6I0nvlfRPxWv4xO4wvt8pXsOnxsz+taRvkXQ+5/y9fI8A7g015OpQQ54uasjVooZcLWrI1aKGXD1qyNWihjwb1JDLzuoKq7dI+nzO+Qs555mkD0h65xk996vROyU9Ffefkn8Y4i7knP+npCuHFh81nu+U9IGc8zTn/P8kfV7+WscRjhjfozC+x5Rzfj7n/Kdx/6akpyW9XryGT8UdxvcojO8xZbcTDwfxL4vX8Km4w/gehfE9JjN7XNL3SPrFA4t5/QL3hhrybFFD3iNqyNWihlwtasjVooZcPWrI1aKGXD1qyFud1Q9Wr5f0pQOPn9Wd36Bx97Kk/2FmnzSz98SyR3POz0v+4SjptWs7uwfDUePJ6/r0/LiZ/R/zdg/dZa6M7wmY2ROSvknSH4vX8Kk7NL4Sr+FTE5fCf0rSi5I+knPmNXyKjhhfidfwafl5ST8pKR1YxusXuDf8N7I61JCrx3v/6vHd5ZRRQ64WNeTqUEOuFjXkylFDHnJWP1jZbZatvhfhq8Nbc87fLOm7JP1YXC6Ps8Hr+nT8gqSvlfSNkp6X9B9jOeN7j8xsW9JvSPqJnPONO216m2WM8Su4zfjyGj5FOec25/yNkh6X9BYz+4Y7bM4YH9MR48tr+BSY2fdKejHn/Mm73eU2yxhfYIH/RlaHGnJ9eF2fDr67nDJqyNWihlwtasjVooZcHWrI2zurH6yelfSGA48fl/TcGT33Ay3n/Fzcvijpt+SXAX4l+uR2/XJfXN8ZPhCOGk9e16cg5/yV+PBLkv6zFpeyMr73IHoK/4akX805/2Ys5jV8Sm43vryGVyPnfE3Sx+S9sXkNn7KD48tr+NS8VdL3xdwwH5D0D8zsV8TrF7hX/DeyItSQZ4L3/hXiu8vpooZcLWrIs0MNuVrUkCtBDXkbZ/WD1cclvcnM3mhmQ0nvkvThM3ruB5aZbcWkjTKzLUn/SNJfysf23bHZuyV9aD1n+MA4ajw/LOldZjYyszdKepOkP1nD+fVa9wYc/on8NSwxvscWk2H+kqSnc84/d2AVr+FTcNT48ho+PWb2GjO7GPc3JP1DSX8lXsOn4qjx5TV8OnLOP51zfjzn/IT8u+4f5Jx/SLx+gXtFDbkC1JBnhvf+FeK7y+mhhlwtasjVo4ZcLWrI1aKGvL3qLJ4k59yY2Y9L+j1JpaRfzjl/+iye+wH3qKTf8s8/VZL+e875d83s45I+aGY/KumLkn5gjefYK2b2a5LeJukRM3tW0r+V9H7dZjxzzp82sw9K+oykRtKP5ZzbtZx4Txwxvm8zs2+UX8L615L+pcT43qO3SvphSX8R/YUl6WfEa/i0HDW+P8hr+NQ8JukpMyvloZoP5px/28z+l3gNn4ajxve/8RpeKd6DgXtADbky1JCnjBpytaghV44acrWoIVePGnK1qCHX41X9Hmw5P3BtDgEAAAAAAAAAANAjZ9USEAAAAAAAAAAAALgtfrACAAAAAAAAAADAWvGDFQAAAAAAAAAAANaKH6wAAAAAAAAAAACwVvxgBQAAAAAAAAAAgLXiBysAAAAAAAAAAACsFT9YAQAAAAAAAAAAYK34wQoAAAAAAAAAAABr9f8BFzEmNEAtofIAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x, y = train_loader.dataset[0]\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "plt.figure(0, figsize=(30, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x.detach().numpy()[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(noisier(x.cuda()).detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1872 10\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "# model_func = get_model_from_name(\"esc_wideresnet28_8\")\n",
    "model = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 24, 64, 431]             240\n         MaxPool2d-2          [-1, 24, 16, 215]               0\n       BatchNorm2d-3          [-1, 24, 16, 215]              48\n             ReLU6-4          [-1, 24, 16, 215]               0\n            Conv2d-5          [-1, 48, 16, 215]          10,416\n         MaxPool2d-6           [-1, 48, 4, 107]               0\n       BatchNorm2d-7           [-1, 48, 4, 107]              96\n             ReLU6-8           [-1, 48, 4, 107]               0\n            Conv2d-9           [-1, 72, 4, 107]          31,176\n        MaxPool2d-10            [-1, 72, 2, 53]               0\n      BatchNorm2d-11            [-1, 72, 2, 53]             144\n            ReLU6-12            [-1, 72, 2, 53]               0\n           Conv2d-13            [-1, 72, 2, 53]          46,728\n        MaxPool2d-14            [-1, 72, 1, 26]               0\n      BatchNorm2d-15            [-1, 72, 1, 26]             144\n            ReLU6-16            [-1, 72, 1, 26]               0\n           Conv2d-17            [-1, 72, 1, 26]          46,728\n            ReLU6-18            [-1, 72, 1, 26]               0\n          Flatten-19                 [-1, 1872]               0\n          Dropout-20                 [-1, 1872]               0\n           Linear-21                   [-1, 10]          18,730\n================================================================\nTotal params: 154,450\nTrainable params: 154,450\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.11\nForward/backward pass size (MB): 9.24\nParams size (MB): 0.59\nEstimated Total Size (MB): 9.93\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(model, input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../tensorboard/esc10/supervised/cnn03/1.0S/2020-11-09_11:14:58_cnn03_1.0S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "tensorboard_title = \"%s/%sS/%s_%s_%.1fS\" % title_element\n",
    "\n",
    "title_element = (model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % title_element\n",
    "\n",
    "tensorboard = mSummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))\n",
    "\n",
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_params = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    tensorboard_params[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard.add_hparams(tensorboard_params, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(args.dataset, \"supervised\", model=model, learning_rate=args.learning_rate)\n",
    "callbacks = load_callbacks(args.dataset, \"supervised\", optimizer=optimizer, nb_epoch=args.nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# Metrics\n",
    "fscore_fn = FScore()\n",
    "acc_fn = CategoricalAccuracy()\n",
    "avg = ContinueAverage()\n",
    "\n",
    "maximum_tracker = track_maximum()\n",
    "\n",
    "reset_metrics = lambda : [m.reset() for m in [fscore_fn, acc_fn, avg]]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \".               \", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"metrics: \", \"acc\", \"F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "noisier = NoiseLayer(ratio=0.0, init_snr=6)\n",
    "\n",
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        X = noisier(X)\n",
    "        \n",
    "        logits_X = model(X)        \n",
    "        # logits_X_ = model(X_)\n",
    "\n",
    "        loss = loss_ce(logits_X, y)\n",
    "        # loss_aug = aug_loss(logits_X, logits_X_)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # aug_opti.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        # loss_aug.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # aug_opti.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = torch.softmax(logits_X, dim=1)\n",
    "            pred_arg = torch.argmax(logits_X, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_ce(logits, y)\n",
    "\n",
    "            # metrics\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc\", acc, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc\", maximum_tracker(\"acc\", acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/f1\", maximum_tracker(\"f1\", fscore), epoch )\n",
    "\n",
    "    checkpoint.step(acc)\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n",
      "\n",
      "Training 1      - 100    -          2.0345 -           0.2656    | 0.0965   - 2.5676\n",
      "\u001b[1;4mValidati 1      - 100    -          2.5470 -           0.2188    | 0.3068   - 0.4729\u001b[0m\n",
      "Training 2      - 100    -          1.2253 -           0.5219    | 0.3648   - 0.4483\n",
      "\u001b[1;4mValidati 2      - 100    -          3.4100 -           0.2969    | 0.3063   - 0.0172\u001b[0m\n",
      "Training 3      - 100    -          0.9396 -           0.6406    | 0.5896   - 0.4315\n",
      "\u001b[1;4mValidati 3      - 100    -          3.9036 -           0.3438    | 0.3872   - 0.0182\u001b[0m\n",
      "Training 4      - 100    -          0.6590 -           0.7656    | 0.7444   - 0.4383\n",
      "\u001b[1;4mValidati 4      - 100    -          4.0962 -           0.3984    | 0.3919   - 0.0156\u001b[0m\n",
      "Training 5      - 100    -          0.4959 -           0.8250    | 0.8114   - 0.4311\n",
      "\u001b[1;4mValidati 5      - 100    -          2.1161 -           0.5938    | 0.6129   - 0.0158\u001b[0m\n",
      "Training 6      - 100    -          0.3715 -           0.8813    | 0.8736   - 0.4343\n",
      "\u001b[1;4mValidati 6      - 100    -          1.0987 -           0.7500    | 0.7579   - 0.0161\u001b[0m\n",
      "Training 7      - 100    -          0.3525 -           0.8719    | 0.8773   - 0.4725\n",
      "\u001b[1;4mValidati 7      - 100    -          0.7913 -           0.7969    | 0.7853   - 0.0270\u001b[0m\n",
      "Training 8      - 100    -          0.2745 -           0.9000    | 0.9113   - 0.4236\n",
      "\u001b[1;4mValidati 8      - 100    -          0.9991 -           0.7188    | 0.7183   - 0.0132\u001b[0m\n",
      "Training 9      - 100    -          0.1977 -           0.9406    | 0.9399   - 0.4123\n",
      "\u001b[1;4mValidati 9      - 100    -          1.2517 -           0.7422    | 0.7470   - 0.0222\u001b[0m\n",
      "Training 10     - 100    -          0.1827 -           0.9344    | 0.9333   - 0.4100\n",
      "\u001b[1;4mValidati 10     - 100    -          1.8346 -           0.6875    | 0.6905   - 0.0149\u001b[0m\n",
      "Training 11     - 100    -          0.2142 -           0.9187    | 0.9114   - 0.4012\n",
      "\u001b[1;4mValidati 11     - 100    -          2.1991 -           0.6484    | 0.6585   - 0.0157\u001b[0m\n",
      "Training 12     - 100    -          0.1652 -           0.9406    | 0.9349   - 0.4084\n",
      "\u001b[1;4mValidati 12     - 100    -          1.6951 -           0.7734    | 0.7766   - 0.0207\u001b[0m\n",
      "Training 13     - 100    -          0.2162 -           0.9438    | 0.9453   - 0.4161\n",
      "\u001b[1;4mValidati 13     - 100    -          1.1206 -           0.7656    | 0.7485   - 0.0140\u001b[0m\n",
      "Training 14     - 100    -          0.1895 -           0.9344    | 0.9339   - 0.4007\n",
      "\u001b[1;4mValidati 14     - 100    -          1.8157 -           0.8281    | 0.8100   - 0.0141\u001b[0m\n",
      "Training 15     - 100    -          0.2167 -           0.9219    | 0.9228   - 0.4060\n",
      "\u001b[1;4mValidati 15     - 100    -          0.9895 -           0.7891    | 0.7793   - 0.0149\u001b[0m\n",
      "Training 16     - 100    -          0.1854 -           0.9375    | 0.9323   - 0.4124\n",
      "\u001b[1;4mValidati 16     - 100    -          2.3423 -           0.7109    | 0.7057   - 0.0174\u001b[0m\n",
      "Training 17     - 100    -          0.1291 -           0.9563    | 0.9527   - 0.4065\n",
      "\u001b[1;4mValidati 17     - 100    -          1.6368 -           0.7500    | 0.7532   - 0.0153\u001b[0m\n",
      "Training 18     - 100    -          0.0716 -           0.9719    | 0.9749   - 0.4093\n",
      "\u001b[1;4mValidati 18     - 100    -          2.0973 -           0.7031    | 0.7062   - 0.0153\u001b[0m\n",
      "Training 19     - 100    -          0.0860 -           0.9750    | 0.9779   - 0.4067\n",
      "\u001b[1;4mValidati 19     - 100    -          1.3537 -           0.8047    | 0.8081   - 0.0145\u001b[0m\n",
      "Training 20     - 100    -          0.0541 -           0.9812    | 0.9812   - 0.4229\n",
      "\u001b[1;4mValidati 20     - 100    -          1.0622 -           0.8359    | 0.8393   - 0.0191\u001b[0m\n",
      "Training 21     - 100    -          0.0339 -           0.9969    | 0.9969   - 0.4151\n",
      "\u001b[1;4mValidati 21     - 100    -          2.1520 -           0.7969    | 0.7767   - 0.0145\u001b[0m\n",
      "Training 22     - 100    -          0.0557 -           0.9844    | 0.9828   - 0.3977\n",
      "\u001b[1;4mValidati 22     - 100    -          2.2416 -           0.7500    | 0.7500   - 0.0148\u001b[0m\n",
      "Training 23     - 100    -          0.0507 -           0.9875    | 0.9875   - 0.3956\n",
      "\u001b[1;4mValidati 23     - 100    -          1.0598 -           0.8281    | 0.8281   - 0.0133\u001b[0m\n",
      "Training 24     - 100    -          0.0414 -           0.9938    | 0.9922   - 0.4056\n",
      "\u001b[1;4mValidati 24     - 100    -          1.2190 -           0.7891    | 0.7891   - 0.0140\u001b[0m\n",
      "Training 25     - 100    -          0.0447 -           0.9906    | 0.9906   - 0.4063\n",
      "\u001b[1;4mValidati 25     - 100    -          1.0651 -           0.7891    | 0.7798   - 0.0151\u001b[0m\n",
      "Training 26     - 100    -          0.0367 -           0.9781    | 0.9797   - 0.4181\n",
      "\u001b[1;4mValidati 26     - 100    -          0.6240 -           0.8828    | 0.8894   - 0.0177\u001b[0m\n",
      "Training 27     - 100    -          0.0563 -           0.9812    | 0.9812   - 0.4052\n",
      "\u001b[1;4mValidati 27     - 100    -          0.6287 -           0.8672    | 0.8625   - 0.0141\u001b[0m\n",
      "Training 28     - 100    -          0.0246 -           0.9938    | 0.9938   - 0.4065\n",
      "\u001b[1;4mValidati 28     - 100    -          0.7144 -           0.8438    | 0.8502   - 0.0145\u001b[0m\n",
      "Training 29     - 100    -          0.0455 -           0.9812    | 0.9828   - 0.4041\n",
      "\u001b[1;4mValidati 29     - 100    -          0.6946 -           0.8906    | 0.8906   - 0.0146\u001b[0m\n",
      "Training 30     - 100    -          0.0951 -           0.9656    | 0.9686   - 0.4068\n",
      "\u001b[1;4mValidati 30     - 100    -          1.8767 -           0.6484    | 0.6518   - 0.0142\u001b[0m\n",
      "Training 31     - 100    -          0.1329 -           0.9656    | 0.9656   - 0.4139\n",
      "\u001b[1;4mValidati 31     - 100    -          1.2678 -           0.8047    | 0.8110   - 0.0149\u001b[0m\n",
      "Training 32     - 100    -          0.0339 -           0.9875    | 0.9875   - 0.4145\n",
      "\u001b[1;4mValidati 32     - 100    -          1.5412 -           0.7500    | 0.7527   - 0.0177\u001b[0m\n",
      "Training 33     - 100    -          0.1110 -           0.9656    | 0.9655   - 0.4045\n",
      "\u001b[1;4mValidati 33     - 100    -          2.1169 -           0.6797    | 0.6826   - 0.0134\u001b[0m\n",
      "Training 34     - 100    -          0.0367 -           0.9812    | 0.9812   - 0.4154\n",
      "\u001b[1;4mValidati 34     - 100    -          1.0818 -           0.8125    | 0.8078   - 0.0155\u001b[0m\n",
      "Training 35     - 100    -          0.0623 -           0.9812    | 0.9795   - 0.4355\n",
      "\n",
      "Training 36     - 100    -          0.0390 -           0.9906    | 0.9906   - 0.4193\n",
      "\u001b[1;4mValidati 36     - 100    -          0.9526 -           0.8828    | 0.8782   - 0.0151\u001b[0m\n",
      "Training 37     - 100    -          0.0238 -           0.9938    | 0.9938   - 0.4337\n",
      "\u001b[1;4mValidati 37     - 100    -          1.4019 -           0.8281    | 0.8281   - 0.0144\u001b[0m\n",
      "Training 38     - 100    -          0.0259 -           0.9906    | 0.9922   - 0.4385\n",
      "\u001b[1;4mValidati 38     - 100    -          0.7262 -           0.8750    | 0.8740   - 0.0181\u001b[0m\n",
      "Training 39     - 100    -          0.0377 -           0.9938    | 0.9922   - 0.4334\n",
      "\u001b[1;4mValidati 39     - 100    -          0.8013 -           0.8516    | 0.8469   - 0.0141\u001b[0m\n",
      "Training 40     - 100    -          0.0217 -           0.9938    | 0.9938   - 0.4126\n",
      "\u001b[1;4mValidati 40     - 100    -          1.3242 -           0.7891    | 0.8012   - 0.0159\u001b[0m\n",
      "Training 41     - 100    -          0.0156 -           0.9938    | 0.9938   - 0.4262\n",
      "\u001b[1;4mValidati 41     - 100    -          1.2164 -           0.7891    | 0.7891   - 0.0184\u001b[0m\n",
      "Training 42     - 100    -          0.0105 -           1.0000    | 1.0000   - 0.4150\n",
      "\u001b[1;4mValidati 42     - 100    -          1.0929 -           0.8203    | 0.8203   - 0.0146\u001b[0m\n",
      "Training 43     - 100    -          0.0129 -           0.9938    | 0.9938   - 0.4047\n",
      "\u001b[1;4mValidati 43     - 100    -          0.7709 -           0.8438    | 0.8438   - 0.0156\u001b[0m\n",
      "Training 44     - 100    -          0.0071 -           1.0000    | 1.0000   - 0.3971\n",
      "\u001b[1;4mValidati 44     - 100    -          0.8085 -           0.8672    | 0.8703   - 0.0158\u001b[0m\n",
      "Training 45     - 100    -          0.0078 -           0.9969    | 0.9969   - 0.3994\n",
      "\u001b[1;4mValidati 45     - 100    -          0.7811 -           0.8516    | 0.8516   - 0.0138\u001b[0m\n",
      "Training 46     - 100    -          0.0226 -           0.9906    | 0.9906   - 0.3974\n",
      "\u001b[1;4mValidati 46     - 100    -          0.7107 -           0.8672    | 0.8672   - 0.0139\u001b[0m\n",
      "Training 47     - 100    -          0.0092 -           0.9969    | 0.9969   - 0.4008\n",
      "\u001b[1;4mValidati 47     - 100    -          1.1931 -           0.7812    | 0.7812   - 0.0136\u001b[0m\n",
      "Training 48     - 100    -          0.0055 -           1.0000    | 1.0000   - 0.4129\n",
      "\u001b[1;4mValidati 48     - 100    -          0.8493 -           0.8359    | 0.8359   - 0.0147\u001b[0m\n",
      "Training 49     - 100    -          0.0087 -           1.0000    | 1.0000   - 0.3963\n",
      "\u001b[1;4mValidati 49     - 100    -          1.0499 -           0.8672    | 0.8672   - 0.0149\u001b[0m\n",
      "Training 50     - 100    -          0.0027 -           1.0000    | 1.0000   - 0.4043\n",
      "\u001b[1;4mValidati 50     - 100    -          1.1625 -           0.8672    | 0.8672   - 0.0140\u001b[0m\n",
      "Training 51     - 100    -          0.0027 -           1.0000    | 1.0000   - 0.4380\n",
      "\u001b[1;4mValidati 51     - 100    -          1.0415 -           0.8906    | 0.8906   - 0.0203\u001b[0m\n",
      "Training 52     - 100    -          0.0050 -           0.9969    | 0.9969   - 0.3992\n",
      "\u001b[1;4mValidati 52     - 100    -          0.9886 -           0.8906    | 0.8906   - 0.0138\u001b[0m\n",
      "Training 53     - 100    -          0.0017 -           1.0000    | 1.0000   - 0.4032\n",
      "\u001b[1;4mValidati 53     - 100    -          0.6336 -           0.9141    | 0.9141   - 0.0134\u001b[0m\n",
      "Training 54     - 100    -          0.0057 -           0.9969    | 0.9969   - 0.4018\n",
      "\u001b[1;4mValidati 54     - 100    -          1.1183 -           0.8438    | 0.8438   - 0.0137\u001b[0m\n",
      "Training 55     - 100    -          0.0098 -           0.9969    | 0.9969   - 0.4072\n",
      "\u001b[1;4mValidati 55     - 100    -          0.7643 -           0.8984    | 0.8984   - 0.0156\u001b[0m\n",
      "Training 56     - 100    -          0.0035 -           1.0000    | 1.0000   - 0.4313\n",
      "\u001b[1;4mValidati 56     - 100    -          1.2089 -           0.8516    | 0.8516   - 0.0213\u001b[0m\n",
      "Training 57     - 100    -          0.0045 -           1.0000    | 1.0000   - 0.4056\n",
      "\u001b[1;4mValidati 57     - 100    -          1.3055 -           0.8516    | 0.8516   - 0.0136\u001b[0m\n",
      "Training 58     - 100    -          0.0063 -           0.9969    | 0.9969   - 0.4182\n",
      "\u001b[1;4mValidati 58     - 100    -          0.8620 -           0.8438    | 0.8438   - 0.0145\u001b[0m\n",
      "Training 59     - 100    -          0.0046 -           1.0000    | 1.0000   - 0.4129\n",
      "\u001b[1;4mValidati 59     - 100    -          1.2056 -           0.8359    | 0.8393   - 0.0142\u001b[0m\n",
      "Training 60     - 100    -          0.0023 -           1.0000    | 1.0000   - 0.4291\n",
      "\u001b[1;4mValidati 60     - 100    -          0.9940 -           0.8594    | 0.8627   - 0.0163\u001b[0m\n",
      "Training 61     - 100    -          0.0013 -           1.0000    | 1.0000   - 0.4069\n",
      "\u001b[1;4mValidati 61     - 100    -          0.9744 -           0.8594    | 0.8627   - 0.0139\u001b[0m\n",
      "Training 62     - 100    -          0.0059 -           0.9969    | 0.9969   - 0.4011\n",
      "\u001b[1;4mValidati 62     - 100    -          0.8217 -           0.8672    | 0.8706   - 0.0128\u001b[0m\n",
      "Training 63     - 100    -          0.0025 -           1.0000    | 1.0000   - 0.4006\n",
      "\u001b[1;4mValidati 63     - 100    -          0.8705 -           0.8438    | 0.8393   - 0.0142\u001b[0m\n",
      "Training 64     - 100    -          0.0023 -           1.0000    | 1.0000   - 0.4201\n",
      "\u001b[1;4mValidati 64     - 100    -          1.0910 -           0.8594    | 0.8627   - 0.0150\u001b[0m\n",
      "Training 65     - 100    -          0.0035 -           1.0000    | 1.0000   - 0.4371\n",
      "\u001b[1;4mValidati 65     - 100    -          0.6409 -           0.9062    | 0.9062   - 0.0173\u001b[0m\n",
      "Training 66     - 100    -          0.0008 -           1.0000    | 1.0000   - 0.4088\n",
      "\u001b[1;4mValidati 66     - 100    -          0.9509 -           0.8594    | 0.8594   - 0.0141\u001b[0m\n",
      "Training 67     - 100    -          0.0020 -           1.0000    | 1.0000   - 0.4133\n",
      "\u001b[1;4mValidati 67     - 100    -          1.1381 -           0.8125    | 0.8125   - 0.0139\u001b[0m\n",
      "Training 68     - 100    -          0.0022 -           1.0000    | 1.0000   - 0.4077\n",
      "\u001b[1;4mValidati 68     - 100    -          0.9965 -           0.8125    | 0.8125   - 0.0151\u001b[0m\n",
      "Training 69     - 100    -          0.0025 -           1.0000    | 1.0000   - 0.4057\n",
      "\u001b[1;4mValidati 69     - 100    -          1.1538 -           0.8359    | 0.8359   - 0.0146\u001b[0m\n",
      "Training 70     - 100    -          0.0078 -           0.9969    | 0.9969   - 0.4126\n",
      "\u001b[1;4mValidati 70     - 100    -          0.7965 -           0.8828    | 0.8828   - 0.0152\u001b[0m\n",
      "Training 71     - 100    -          0.0012 -           1.0000    | 1.0000   - 0.4117\n",
      "\u001b[1;4mValidati 71     - 100    -          0.9186 -           0.8594    | 0.8594   - 0.0147\u001b[0m\n",
      "Training 72     - 100    -          0.0019 -           1.0000    | 1.0000   - 0.4122\n",
      "\u001b[1;4mValidati 72     - 100    -          0.7797 -           0.8906    | 0.8861   - 0.0143\u001b[0m\n",
      "Training 73     - 100    -          0.0013 -           1.0000    | 1.0000   - 0.4209\n",
      "\u001b[1;4mValidati 73     - 100    -          1.2352 -           0.8438    | 0.8393   - 0.0139\u001b[0m\n",
      "Training 74     - 100    -          0.0011 -           1.0000    | 1.0000   - 0.4111\n",
      "\u001b[1;4mValidati 74     - 100    -          1.2377 -           0.8438    | 0.8393   - 0.0146\u001b[0m\n",
      "Training 75     - 100    -          0.0015 -           1.0000    | 1.0000   - 0.4199\n",
      "\u001b[1;4mValidati 75     - 100    -          1.3896 -           0.8203    | 0.8159   - 0.0144\u001b[0m\n",
      "Training 76     - 100    -          0.0012 -           1.0000    | 1.0000   - 0.4093\n",
      "\u001b[1;4mValidati 76     - 100    -          1.6499 -           0.7969    | 0.7926   - 0.0139\u001b[0m\n",
      "Training 77     - 100    -          0.0167 -           0.9969    | 0.9969   - 0.4223\n",
      "\u001b[1;4mValidati 77     - 100    -          1.5174 -           0.8359    | 0.8314   - 0.0179\u001b[0m\n",
      "Training 78     - 100    -          0.0094 -           0.9969    | 0.9969   - 0.4169\n",
      "\u001b[1;4mValidati 78     - 100    -          0.7401 -           0.8828    | 0.8782   - 0.0144\u001b[0m\n",
      "Training 79     - 100    -          0.0063 -           0.9969    | 0.9969   - 0.4099\n",
      "\u001b[1;4mValidati 79     - 100    -          0.7316 -           0.8828    | 0.8657   - 0.0144\u001b[0m\n",
      "Training 80     - 100    -          0.0019 -           1.0000    | 1.0000   - 0.4047\n",
      "\u001b[1;4mValidati 80     - 100    -          1.0051 -           0.8906    | 0.8906   - 0.0141\u001b[0m\n",
      "Training 81     - 100    -          0.0017 -           1.0000    | 1.0000   - 0.4277\n",
      "\n",
      "Training 82     - 100    -          0.0017 -           1.0000    | 1.0000   - 0.4111\n",
      "\u001b[1;4mValidati 82     - 100    -          0.9872 -           0.8438    | 0.8438   - 0.0145\u001b[0m\n",
      "Training 83     - 100    -          0.0032 -           1.0000    | 1.0000   - 0.4053\n",
      "\u001b[1;4mValidati 83     - 100    -          0.6573 -           0.9141    | 0.9141   - 0.0139\u001b[0m\n",
      "Training 84     - 100    -          0.0011 -           1.0000    | 1.0000   - 0.4045\n",
      "\u001b[1;4mValidati 84     - 100    -          0.7695 -           0.8906    | 0.8906   - 0.0141\u001b[0m\n",
      "Training 85     - 100    -          0.0008 -           1.0000    | 1.0000   - 0.4001\n",
      "\u001b[1;4mValidati 85     - 100    -          0.8934 -           0.8750    | 0.8706   - 0.0134\u001b[0m\n",
      "Training 86     - 100    -          0.0020 -           1.0000    | 1.0000   - 0.4025\n",
      "\u001b[1;4mValidati 86     - 100    -          0.7080 -           0.8984    | 0.8984   - 0.0137\u001b[0m\n",
      "Training 87     - 100    -          0.0013 -           1.0000    | 1.0000   - 0.3957\n",
      "\u001b[1;4mValidati 87     - 100    -          1.3882 -           0.8672    | 0.8706   - 0.0145\u001b[0m\n",
      "Training 88     - 100    -          0.0038 -           1.0000    | 1.0000   - 0.3983\n",
      "\u001b[1;4mValidati 88     - 100    -          1.3833 -           0.8516    | 0.8516   - 0.0145\u001b[0m\n",
      "Training 89     - 100    -          0.0009 -           1.0000    | 1.0000   - 0.3981\n",
      "\u001b[1;4mValidati 89     - 100    -          0.9607 -           0.8672    | 0.8706   - 0.0136\u001b[0m\n",
      "Training 90     - 100    -          0.0013 -           1.0000    | 1.0000   - 0.3974\n",
      "\u001b[1;4mValidati 90     - 100    -          1.3984 -           0.8281    | 0.8281   - 0.0147\u001b[0m\n",
      "Training 91     - 100    -          0.0023 -           1.0000    | 1.0000   - 0.3983\n",
      "\u001b[1;4mValidati 91     - 100    -          1.3345 -           0.8516    | 0.8516   - 0.0137\u001b[0m\n",
      "Training 92     - 100    -          0.0008 -           1.0000    | 1.0000   - 0.4006\n",
      "\u001b[1;4mValidati 92     - 100    -          0.7220 -           0.8984    | 0.8984   - 0.0144\u001b[0m\n",
      "Training 93     - 100    -          0.0010 -           1.0000    | 1.0000   - 0.4013\n",
      "\u001b[1;4mValidati 93     - 100    -          0.7298 -           0.8984    | 0.8984   - 0.0144\u001b[0m\n",
      "Training 94     - 100    -          0.0007 -           1.0000    | 1.0000   - 0.3973\n",
      "\u001b[1;4mValidati 94     - 100    -          1.5761 -           0.8516    | 0.8516   - 0.0134\u001b[0m\n",
      "Training 95     - 100    -          0.0017 -           1.0000    | 1.0000   - 0.3989\n",
      "\u001b[1;4mValidati 95     - 100    -          1.0551 -           0.8750    | 0.8706   - 0.0144\u001b[0m\n",
      "Training 96     - 100    -          0.0010 -           1.0000    | 1.0000   - 0.4006\n",
      "\u001b[1;4mValidati 96     - 100    -          0.9759 -           0.8672    | 0.8672   - 0.0140\u001b[0m\n",
      "Training 97     - 100    -          0.0031 -           1.0000    | 1.0000   - 0.4036\n",
      "\u001b[1;4mValidati 97     - 100    -          0.8439 -           0.8750    | 0.8569   - 0.0134\u001b[0m\n",
      "Training 98     - 100    -          0.0009 -           1.0000    | 1.0000   - 0.4004\n",
      "\u001b[1;4mValidati 98     - 100    -          1.3996 -           0.8516    | 0.8516   - 0.0139\u001b[0m\n",
      "Training 99     - 100    -          0.0008 -           1.0000    | 1.0000   - 0.4064\n",
      "\u001b[1;4mValidati 99     - 100    -          1.2445 -           0.8516    | 0.8516   - 0.0145\u001b[0m\n",
      "Training 100    - 100    -          0.0012 -           1.0000    | 1.0000   - 0.4106\n"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "args.nb_epoch = 100\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()"
   ]
  },
  {
   "source": [
    "# Optimize augmentation function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor noise snr\n",
    "def get_snr():\n",
    "    for p in noisier.parameters():\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisier = NoiseLayer(ratio=1.0, init_snr=20)\n",
    "\n",
    "\n",
    "aug_loss = nn.MSELoss(reduction=\"mean\")\n",
    "aug_opti = torch.optim.Adam(noisier.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} {:<6.6} - {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} {:<6.4f} - {:<6.4f}\"\n",
    "header = header_form.format(\".               \", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"snr\", \"Time\")\n",
    "\n",
    "def aug_train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "    noisier.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        X_ = noisier(X)\n",
    "        \n",
    "        logits_X = model(X)        \n",
    "        logits_X_ = model(X_)\n",
    "\n",
    "        loss_aug = aug_loss(logits_X, logits_X_)\n",
    "        \n",
    "        aug_opti.zero_grad()\n",
    "        loss_aug.backward()\n",
    "        aug_opti.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            avg_ce = avg(loss_aug.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(value_form.format(\n",
    "                \"Augmentation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce, get_snr().item(),\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/aug_ce\", avg_ce, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".        Epoch  - %      - Losses:  ce     snr    - Time  \n",
      "\n",
      "Augmenta 1      - 100    -          8.0982 3.4950 - 0.5970\n",
      "Augmenta 2      - 100    -          9.0293 3.4296 - 0.5533\n",
      "Augmenta 3      - 100    -          9.1993 3.3625 - 0.5516\n",
      "Augmenta 4      - 100    -          8.6274 3.2979 - 0.5523\n",
      "Augmenta 5      - 100    -          8.7221 3.2375 - 0.5355\n",
      "Augmenta 6      - 100    -          8.1128 3.1826 - 0.5438\n",
      "Augmenta 7      - 100    -          8.9913 3.1321 - 0.5375\n",
      "Augmenta 8      - 100    -          8.2906 3.0815 - 0.5343\n",
      "Augmenta 9      - 100    -          8.6872 3.0272 - 0.5409\n",
      "Augmenta 10     - 100    -          8.7007 2.9723 - 0.5430\n",
      "Augmenta 11     - 100    -          9.2414 2.9202 - 0.5488\n",
      "Augmenta 12     - 100    -          8.0495 2.8678 - 0.5296\n",
      "Augmenta 13     - 100    -          8.2616 2.8129 - 0.5371\n",
      "Augmenta 14     - 100    -          8.5471 2.7626 - 0.5386\n",
      "Augmenta 15     - 100    -          8.0313 2.7096 - 0.5383\n",
      "Augmenta 16     - 100    -          8.5107 2.6599 - 0.5346\n",
      "Augmenta 17     - 100    -          7.6866 2.6152 - 0.5376\n",
      "Augmenta 18     - 100    -          7.6773 2.5786 - 0.5512\n",
      "Augmenta 19     - 100    -          7.9633 2.5469 - 0.5467\n",
      "Augmenta 20     - 100    -          8.0880 2.5164 - 0.5352\n",
      "Augmenta 21     - 100    -          8.2513 2.4855 - 0.5332\n",
      "Augmenta 22     - 100    -          7.2932 2.4505 - 0.5320\n",
      "Augmenta 23     - 100    -          7.9305 2.4123 - 0.5291\n",
      "Augmenta 24     - 100    -          7.9961 2.3774 - 0.5366\n",
      "Augmenta 25     - 100    -          8.4236 2.3466 - 0.5295\n",
      "Augmenta 26     - 100    -          8.1216 2.3115 - 0.5318\n",
      "Augmenta 27     - 100    -          8.2629 2.2730 - 0.5278\n",
      "Augmenta 28     - 100    -          8.1228 2.2342 - 0.5337\n",
      "Augmenta 29     - 100    -          8.4323 2.1963 - 0.5293\n",
      "Augmenta 30     - 100    -          8.2027 2.1592 - 0.5394\n",
      "Augmenta 31     - 100    -          8.7391 2.1184 - 0.5422\n",
      "Augmenta 32     - 100    -          7.6919 2.0847 - 0.5260\n",
      "Augmenta 33     - 100    -          7.7908 2.0549 - 0.5304\n",
      "Augmenta 34     - 100    -          8.2735 2.0280 - 0.5358\n",
      "Augmenta 35     - 100    -          7.8796 2.0011 - 0.5289\n",
      "Augmenta 36     - 100    -          8.1598 1.9725 - 0.5256\n",
      "Augmenta 37     - 100    -          7.9551 1.9467 - 0.5346\n",
      "Augmenta 38     - 100    -          8.2078 1.9186 - 0.5344\n",
      "Augmenta 39     - 100    -          8.0902 1.8881 - 0.5353\n",
      "Augmenta 40     - 100    -          8.3221 1.8551 - 0.5408\n",
      "Augmenta 41     - 100    -          7.5283 1.8244 - 0.5402\n",
      "Augmenta 42     - 100    -          8.2777 1.7942 - 0.5407\n",
      "Augmenta 43     - 100    -          8.1487 1.7613 - 0.5421\n",
      "Augmenta 44     - 100    -          8.2754 1.7345 - 0.5387\n",
      "Augmenta 45     - 100    -          7.6812 1.7092 - 0.5472\n",
      "Augmenta 46     - 100    -          8.0696 1.6849 - 0.5529\n",
      "Augmenta 47     - 100    -          8.0639 1.6607 - 0.5374\n",
      "Augmenta 48     - 100    -          8.3383 1.6352 - 0.5549\n",
      "Augmenta 49     - 100    -          7.7322 1.6097 - 0.5644\n",
      "Augmenta 50     - 100    -          7.7670 1.5871 - 0.5556\n",
      "Augmenta 51     - 100    -          7.4299 1.5662 - 0.5401\n",
      "Augmenta 52     - 100    -          7.9983 1.5438 - 0.5580\n",
      "Augmenta 53     - 100    -          7.6186 1.5200 - 0.5458\n",
      "Augmenta 54     - 100    -          8.0912 1.4927 - 0.5381\n",
      "Augmenta 55     - 100    -          7.8500 1.4692 - 0.5424\n",
      "Augmenta 56     - 100    -          8.2732 1.4435 - 0.5419\n",
      "Augmenta 57     - 100    -          8.1414 1.4230 - 0.5520\n",
      "Augmenta 58     - 100    -          8.4832 1.4041 - 0.5404\n",
      "Augmenta 59     - 100    -          7.7806 1.3820 - 0.5348\n",
      "Augmenta 60     - 100    -          7.8936 1.3611 - 0.5328\n",
      "Augmenta 61     - 100    -          7.9514 1.3416 - 0.5333\n",
      "Augmenta 62     - 100    -          7.8609 1.3199 - 0.5315\n",
      "Augmenta 63     - 100    -          8.3341 1.2983 - 0.5339\n",
      "Augmenta 64     - 100    -          7.9210 1.2772 - 0.5397\n",
      "Augmenta 65     - 100    -          8.2231 1.2584 - 0.5404\n",
      "Augmenta 66     - 100    -          7.4143 1.2445 - 0.5337\n",
      "Augmenta 67     - 100    -          8.3183 1.2365 - 0.5389\n",
      "Augmenta 68     - 100    -          7.8296 1.2246 - 0.5616\n",
      "Augmenta 69     - 100    -          8.1981 1.2111 - 0.5356\n",
      "Augmenta 70     - 100    -          8.1660 1.1984 - 0.5340\n",
      "Augmenta 71     - 100    -          8.1430 1.1835 - 0.5292\n",
      "Augmenta 72     - 100    -          8.3827 1.1724 - 0.5278\n",
      "Augmenta 73     - 100    -          7.9446 1.1595 - 0.5292\n",
      "Augmenta 74     - 100    -          8.5525 1.1470 - 0.5441\n",
      "Augmenta 75     - 100    -          7.5938 1.1406 - 0.5318\n",
      "Augmenta 76     - 100    -          8.2524 1.1334 - 0.5410\n",
      "Augmenta 77     - 100    -          7.6088 1.1226 - 0.5430\n",
      "Augmenta 78     - 100    -          8.2207 1.1140 - 0.5388\n",
      "Augmenta 79     - 100    -          7.7856 1.1044 - 0.5445\n",
      "Augmenta 80     - 100    -          8.0319 1.0939 - 0.5402\n",
      "Augmenta 81     - 100    -          7.9010 1.0772 - 0.5421\n",
      "Augmenta 82     - 100    -          7.8792 1.0634 - 0.5399\n",
      "Augmenta 83     - 100    -          7.7641 1.0491 - 0.5433\n",
      "Augmenta 84     - 100    -          7.7039 1.0378 - 0.5359\n",
      "Augmenta 85     - 100    -          8.0741 1.0300 - 0.5337\n",
      "Augmenta 86     - 100    -          7.8001 1.0247 - 0.5293\n",
      "Augmenta 87     - 100    -          7.3212 1.0214 - 0.5408\n",
      "Augmenta 88     - 100    -          8.4723 1.0162 - 0.5409\n",
      "Augmenta 89     - 100    -          7.6702 1.0125 - 0.5417\n",
      "Augmenta 90     - 100    -          8.0871 1.0108 - 0.5386\n",
      "Augmenta 91     - 100    -          8.1218 1.0050 - 0.5416\n",
      "Augmenta 92     - 100    -          7.7318 0.9991 - 0.5427\n",
      "Augmenta 93     - 100    -          7.4535 0.9951 - 0.5426\n",
      "Augmenta 94     - 100    -          7.6433 0.9866 - 0.5384\n",
      "Augmenta 95     - 100    -          7.8107 0.9746 - 0.5370\n",
      "Augmenta 96     - 100    -          7.7558 0.9616 - 0.5345\n",
      "Augmenta 97     - 100    -          7.9472 0.9439 - 0.5371\n",
      "Augmenta 98     - 100    -          8.3818 0.9212 - 0.5341\n",
      "Augmenta 99     - 100    -          7.6373 0.9000 - 0.5563\n"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(100):\n",
    "    aug_train(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor(0.8853, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in noisier.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (0,)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ac41ec0e1a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val/acc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val/f1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-ac41ec0e1a4f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{k} = {max(tensorboard.history[k])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mspp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{k} = {max(tensorboard.history[k])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2792\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2794\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m   2796\u001b[0m         is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    270\u001b[0m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (0,)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"val/acc\")\n",
    "pp(\"val/f1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "pp(\"max/acc\")\n",
    "pp(\"max/f1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "# pp(\"train/Lce\")\n",
    "pp(\"train/aug_ce\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(2, 3, 4)\n",
    "# pp(\"train/snr\")\n",
    "# plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .llll||=||llll."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.0 64-bit ('dct': conda)",
   "display_name": "Python 3.8.0 64-bit ('dct': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4213439f5b613b420c7569d9a47a12bf897408d7580af00afce4972bcc867ad9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}