{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1703.01780.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/samova/lcances/.miniconda3/envs/pytorch-dev/bin/python'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.mixup import MixUpBatchShuffle\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, DotDict, get_train_format, DotDict\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.loss import JensenShanon\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"ComParE2021_PRS\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=128, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=125000, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "group_t.add_argument(\"--num_classes\", default=5, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_s = parser.add_argument_group(\"Student teacher parameters\")\n",
    "group_s.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_s.add_argument(\"--warmup_length\", default=50, type=int)\n",
    "group_s.add_argument(\"--lambda_cost_max\", default=1, type=float)\n",
    "group_s.add_argument(\"--teacher_noise\", default=0, type=float)\n",
    "group_s.add_argument(\"--ccost_softmax\", action=\"store_true\", default=False)\n",
    "group_s.add_argument(\"--ccost_method\", type=str, default=\"mse\")\n",
    "\n",
    "group_mixup = parser.add_argument_group(\"Mixup parameters\")\n",
    "group_mixup.add_argument(\"--mixup\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_alpha\", type=float, default=0.4)\n",
    "group_mixup.add_argument(\"--mixup_max\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_label\", action=\"store_true\", default=False)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args=parser.parse_args([\"--mixup\", \"--mixup_max\", \"--ccost_softmax\"])\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DotDict(\n",
    "    hardware=DotDict(\n",
    "        nb_cpu=0,\n",
    "        nb_gpu=1,\n",
    "    ),\n",
    "    \n",
    "    dataset=DotDict(\n",
    "        dataset='ComParE2021_PRS',\n",
    "        num_classes=5,\n",
    "    ),\n",
    "    \n",
    "    model=DotDict(\n",
    "        model=\"MobileNetV2\",\n",
    "    ),\n",
    "    \n",
    "    train_param=DotDict(\n",
    "        supervised_ratio=0.1,\n",
    "        batch_size=128,\n",
    "        nb_iteration=75000,\n",
    "        learning_rate=0.001,\n",
    "        seed=1234,\n",
    "        resume=False,\n",
    "        \n",
    "        train_folds=None,\n",
    "        val_folds=None,\n",
    "    ),\n",
    "    \n",
    "    mt=DotDict(\n",
    "        alpha=0.999,\n",
    "        warmup_length=50,\n",
    "        lambda_ccost_max=1,\n",
    "        use_softmax=True,\n",
    "        ccost_method='mse',\n",
    "    ),\n",
    "    \n",
    "    mixup=DotDict(\n",
    "        use=False,\n",
    "        alpha=1.0,\n",
    "        max=True,\n",
    "        label=True\n",
    "    ),\n",
    "    \n",
    "    specaugment=DotDict(\n",
    "        use=False,\n",
    "        time_drop_width=32,\n",
    "        time_stripe_num=1,\n",
    "        freq_drop_width=4,\n",
    "        freq_stripe_num=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "cfg['path'] = DotDict(\n",
    "    dataset_root='../../datasets',\n",
    "    checkpoint_root='../../model_save',\n",
    "    tensorboard_root='../../tensorboard',\n",
    ")\n",
    "cfg.path['checkpoint_path']=f'{cfg.path.checkpoint_root}/{cfg.dataset.dataset}/fixmatch'\n",
    "cfg.path['tensorboard_path']=f'{cfg.path.tensorboard_root}/{cfg.dataset.dataset}/fixmatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'ccost_method': 'mse',\n",
      " 'ccost_softmax': True,\n",
      " 'checkpoint_path': 'mean-teacher_mixup',\n",
      " 'checkpoint_root': '../../model_save/',\n",
      " 'dataset': 'ComParE2021_PRS',\n",
      " 'dataset_root': '../../datasets',\n",
      " 'ema_alpha': 0.999,\n",
      " 'from_config': '',\n",
      " 'lambda_cost_max': 1,\n",
      " 'learning_rate': 0.001,\n",
      " 'mixup': True,\n",
      " 'mixup_alpha': 0.4,\n",
      " 'mixup_label': False,\n",
      " 'mixup_max': True,\n",
      " 'model': 'wideresnet28_2',\n",
      " 'nb_epoch': 125000,\n",
      " 'num_classes': 5,\n",
      " 'resume': False,\n",
      " 'seed': 1234,\n",
      " 'supervised_ratio': 0.1,\n",
      " 'teacher_noise': 0,\n",
      " 'tensorboard_path': 'mean-teacher_mixup',\n",
      " 'tensorboard_root': '../../tensorboard/',\n",
      " 'tensorboard_sufix': '',\n",
      " 'train_folds': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      " 'val_folds': [10],\n",
      " 'warmup_length': 50}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(cfg.train_param.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/samova/lcances/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): PadUpTo()\n",
       "  (1): MelSpectrogram(\n",
       "    (spectrogram): Spectrogram()\n",
       "    (mel_scale): MelScale()\n",
       "  )\n",
       "  (2): AmplitudeToDB()\n",
       "  (3): Squeeze()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(cfg.dataset.dataset, \"mean-teacher\")\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/samova/lcances/semi-supervised/SSL/dataset/speechcommands.py\n",
      "<bound method cache_feature.<locals>.decorator of <SSL.dataset_loader.ComParE2021_PRS.ComParE2021_PRS object at 0x7f5b3728efa0>>\n",
      "cache path:  .ComParE2021_PRS/.cache_batch_size=128_seed=1234\n",
      "split ready, loading cache file\n",
      "s_idx:  693\n",
      "u_idx:  6222\n",
      "Sort the classes\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    cfg.dataset.dataset,\n",
    "    \"mean-teacher\",\n",
    "    \n",
    "    dataset_root = cfg.path.dataset_root,\n",
    "    supervised_ratio = cfg.train_param.supervised_ratio,\n",
    "    batch_size = cfg.train_param.batch_size,\n",
    "    train_folds = cfg.train_param.train_folds,\n",
    "    val_folds = cfg.train_param.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=cfg.hardware.nb_cpu,\n",
    "    pin_memory=False,\n",
    "\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 94])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = train_loader._iterables[0].dataset[0][0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(cfg.dataset.dataset, cfg.model.model)\n",
    "\n",
    "student = model_func(input_shape=input_shape, num_classes = cfg.dataset.num_classes)\n",
    "teacher = model_func(input_shape=input_shape, num_classes = cfg.dataset.num_classes)\n",
    "\n",
    "student = student.cuda()\n",
    "teacher = teacher.cuda()\n",
    "\n",
    "# We do not need gradient for the teacher model\n",
    "for p in teacher.parameters():\n",
    "    p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 94]             288\n",
      "         AvgPool2d-2           [-1, 32, 32, 47]               0\n",
      "       BatchNorm2d-3           [-1, 32, 32, 47]              64\n",
      "             ReLU6-4           [-1, 32, 32, 47]               0\n",
      "            Conv2d-5           [-1, 32, 32, 47]             288\n",
      "         AvgPool2d-6           [-1, 32, 32, 47]               0\n",
      "       BatchNorm2d-7           [-1, 32, 32, 47]              64\n",
      "             ReLU6-8           [-1, 32, 32, 47]               0\n",
      "            Conv2d-9           [-1, 16, 32, 47]             512\n",
      "      BatchNorm2d-10           [-1, 16, 32, 47]              32\n",
      " InvertedResidual-11           [-1, 16, 32, 47]               0\n",
      "           Conv2d-12           [-1, 96, 32, 47]           1,536\n",
      "      BatchNorm2d-13           [-1, 96, 32, 47]             192\n",
      "            ReLU6-14           [-1, 96, 32, 47]               0\n",
      "           Conv2d-15           [-1, 96, 32, 47]             864\n",
      "        AvgPool2d-16           [-1, 96, 16, 23]               0\n",
      "      BatchNorm2d-17           [-1, 96, 16, 23]             192\n",
      "            ReLU6-18           [-1, 96, 16, 23]               0\n",
      "           Conv2d-19           [-1, 24, 16, 23]           2,304\n",
      "      BatchNorm2d-20           [-1, 24, 16, 23]              48\n",
      " InvertedResidual-21           [-1, 24, 16, 23]               0\n",
      "           Conv2d-22          [-1, 144, 16, 23]           3,456\n",
      "      BatchNorm2d-23          [-1, 144, 16, 23]             288\n",
      "            ReLU6-24          [-1, 144, 16, 23]               0\n",
      "           Conv2d-25          [-1, 144, 16, 23]           1,296\n",
      "        AvgPool2d-26          [-1, 144, 16, 23]               0\n",
      "      BatchNorm2d-27          [-1, 144, 16, 23]             288\n",
      "            ReLU6-28          [-1, 144, 16, 23]               0\n",
      "           Conv2d-29           [-1, 24, 16, 23]           3,456\n",
      "      BatchNorm2d-30           [-1, 24, 16, 23]              48\n",
      " InvertedResidual-31           [-1, 24, 16, 23]               0\n",
      "           Conv2d-32          [-1, 144, 16, 23]           3,456\n",
      "      BatchNorm2d-33          [-1, 144, 16, 23]             288\n",
      "            ReLU6-34          [-1, 144, 16, 23]               0\n",
      "           Conv2d-35          [-1, 144, 16, 23]           1,296\n",
      "        AvgPool2d-36           [-1, 144, 8, 11]               0\n",
      "      BatchNorm2d-37           [-1, 144, 8, 11]             288\n",
      "            ReLU6-38           [-1, 144, 8, 11]               0\n",
      "           Conv2d-39            [-1, 32, 8, 11]           4,608\n",
      "      BatchNorm2d-40            [-1, 32, 8, 11]              64\n",
      " InvertedResidual-41            [-1, 32, 8, 11]               0\n",
      "           Conv2d-42           [-1, 192, 8, 11]           6,144\n",
      "      BatchNorm2d-43           [-1, 192, 8, 11]             384\n",
      "            ReLU6-44           [-1, 192, 8, 11]               0\n",
      "           Conv2d-45           [-1, 192, 8, 11]           1,728\n",
      "        AvgPool2d-46           [-1, 192, 8, 11]               0\n",
      "      BatchNorm2d-47           [-1, 192, 8, 11]             384\n",
      "            ReLU6-48           [-1, 192, 8, 11]               0\n",
      "           Conv2d-49            [-1, 32, 8, 11]           6,144\n",
      "      BatchNorm2d-50            [-1, 32, 8, 11]              64\n",
      " InvertedResidual-51            [-1, 32, 8, 11]               0\n",
      "           Conv2d-52           [-1, 192, 8, 11]           6,144\n",
      "      BatchNorm2d-53           [-1, 192, 8, 11]             384\n",
      "            ReLU6-54           [-1, 192, 8, 11]               0\n",
      "           Conv2d-55           [-1, 192, 8, 11]           1,728\n",
      "        AvgPool2d-56           [-1, 192, 8, 11]               0\n",
      "      BatchNorm2d-57           [-1, 192, 8, 11]             384\n",
      "            ReLU6-58           [-1, 192, 8, 11]               0\n",
      "           Conv2d-59            [-1, 32, 8, 11]           6,144\n",
      "      BatchNorm2d-60            [-1, 32, 8, 11]              64\n",
      " InvertedResidual-61            [-1, 32, 8, 11]               0\n",
      "           Conv2d-62           [-1, 192, 8, 11]           6,144\n",
      "      BatchNorm2d-63           [-1, 192, 8, 11]             384\n",
      "            ReLU6-64           [-1, 192, 8, 11]               0\n",
      "           Conv2d-65           [-1, 192, 8, 11]           1,728\n",
      "        AvgPool2d-66            [-1, 192, 4, 5]               0\n",
      "      BatchNorm2d-67            [-1, 192, 4, 5]             384\n",
      "            ReLU6-68            [-1, 192, 4, 5]               0\n",
      "           Conv2d-69             [-1, 64, 4, 5]          12,288\n",
      "      BatchNorm2d-70             [-1, 64, 4, 5]             128\n",
      " InvertedResidual-71             [-1, 64, 4, 5]               0\n",
      "           Conv2d-72            [-1, 384, 4, 5]          24,576\n",
      "      BatchNorm2d-73            [-1, 384, 4, 5]             768\n",
      "            ReLU6-74            [-1, 384, 4, 5]               0\n",
      "           Conv2d-75            [-1, 384, 4, 5]           3,456\n",
      "        AvgPool2d-76            [-1, 384, 4, 5]               0\n",
      "      BatchNorm2d-77            [-1, 384, 4, 5]             768\n",
      "            ReLU6-78            [-1, 384, 4, 5]               0\n",
      "           Conv2d-79             [-1, 64, 4, 5]          24,576\n",
      "      BatchNorm2d-80             [-1, 64, 4, 5]             128\n",
      " InvertedResidual-81             [-1, 64, 4, 5]               0\n",
      "           Conv2d-82            [-1, 384, 4, 5]          24,576\n",
      "      BatchNorm2d-83            [-1, 384, 4, 5]             768\n",
      "            ReLU6-84            [-1, 384, 4, 5]               0\n",
      "           Conv2d-85            [-1, 384, 4, 5]           3,456\n",
      "        AvgPool2d-86            [-1, 384, 4, 5]               0\n",
      "      BatchNorm2d-87            [-1, 384, 4, 5]             768\n",
      "            ReLU6-88            [-1, 384, 4, 5]               0\n",
      "           Conv2d-89             [-1, 64, 4, 5]          24,576\n",
      "      BatchNorm2d-90             [-1, 64, 4, 5]             128\n",
      " InvertedResidual-91             [-1, 64, 4, 5]               0\n",
      "           Conv2d-92            [-1, 384, 4, 5]          24,576\n",
      "      BatchNorm2d-93            [-1, 384, 4, 5]             768\n",
      "            ReLU6-94            [-1, 384, 4, 5]               0\n",
      "           Conv2d-95            [-1, 384, 4, 5]           3,456\n",
      "        AvgPool2d-96            [-1, 384, 4, 5]               0\n",
      "      BatchNorm2d-97            [-1, 384, 4, 5]             768\n",
      "            ReLU6-98            [-1, 384, 4, 5]               0\n",
      "           Conv2d-99             [-1, 64, 4, 5]          24,576\n",
      "     BatchNorm2d-100             [-1, 64, 4, 5]             128\n",
      "InvertedResidual-101             [-1, 64, 4, 5]               0\n",
      "          Conv2d-102            [-1, 384, 4, 5]          24,576\n",
      "     BatchNorm2d-103            [-1, 384, 4, 5]             768\n",
      "           ReLU6-104            [-1, 384, 4, 5]               0\n",
      "          Conv2d-105            [-1, 384, 4, 5]           3,456\n",
      "       AvgPool2d-106            [-1, 384, 2, 2]               0\n",
      "     BatchNorm2d-107            [-1, 384, 2, 2]             768\n",
      "           ReLU6-108            [-1, 384, 2, 2]               0\n",
      "          Conv2d-109             [-1, 96, 2, 2]          36,864\n",
      "     BatchNorm2d-110             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-111             [-1, 96, 2, 2]               0\n",
      "          Conv2d-112            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-113            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-114            [-1, 576, 2, 2]               0\n",
      "          Conv2d-115            [-1, 576, 2, 2]           5,184\n",
      "       AvgPool2d-116            [-1, 576, 2, 2]               0\n",
      "     BatchNorm2d-117            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-118            [-1, 576, 2, 2]               0\n",
      "          Conv2d-119             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-120             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-121             [-1, 96, 2, 2]               0\n",
      "          Conv2d-122            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-123            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-124            [-1, 576, 2, 2]               0\n",
      "          Conv2d-125            [-1, 576, 2, 2]           5,184\n",
      "       AvgPool2d-126            [-1, 576, 2, 2]               0\n",
      "     BatchNorm2d-127            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-128            [-1, 576, 2, 2]               0\n",
      "          Conv2d-129             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-130             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-131             [-1, 96, 2, 2]               0\n",
      "          Conv2d-132            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-133            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-134            [-1, 576, 2, 2]               0\n",
      "          Conv2d-135            [-1, 576, 2, 2]           5,184\n",
      "       AvgPool2d-136            [-1, 576, 2, 2]               0\n",
      "     BatchNorm2d-137            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-138            [-1, 576, 2, 2]               0\n",
      "          Conv2d-139            [-1, 160, 2, 2]          92,160\n",
      "     BatchNorm2d-140            [-1, 160, 2, 2]             320\n",
      "InvertedResidual-141            [-1, 160, 2, 2]               0\n",
      "          Conv2d-142            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-143            [-1, 960, 2, 2]           1,920\n",
      "           ReLU6-144            [-1, 960, 2, 2]               0\n",
      "          Conv2d-145            [-1, 960, 2, 2]           8,640\n",
      "       AvgPool2d-146            [-1, 960, 2, 2]               0\n",
      "     BatchNorm2d-147            [-1, 960, 2, 2]           1,920\n",
      "           ReLU6-148            [-1, 960, 2, 2]               0\n",
      "          Conv2d-149            [-1, 160, 2, 2]         153,600\n",
      "     BatchNorm2d-150            [-1, 160, 2, 2]             320\n",
      "InvertedResidual-151            [-1, 160, 2, 2]               0\n",
      "          Conv2d-152            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-153            [-1, 960, 2, 2]           1,920\n",
      "           ReLU6-154            [-1, 960, 2, 2]               0\n",
      "          Conv2d-155            [-1, 960, 2, 2]           8,640\n",
      "       AvgPool2d-156            [-1, 960, 2, 2]               0\n",
      "     BatchNorm2d-157            [-1, 960, 2, 2]           1,920\n",
      "           ReLU6-158            [-1, 960, 2, 2]               0\n",
      "          Conv2d-159            [-1, 160, 2, 2]         153,600\n",
      "     BatchNorm2d-160            [-1, 160, 2, 2]             320\n",
      "InvertedResidual-161            [-1, 160, 2, 2]               0\n",
      "          Conv2d-162            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-163            [-1, 960, 2, 2]           1,920\n",
      "           ReLU6-164            [-1, 960, 2, 2]               0\n",
      "          Conv2d-165            [-1, 960, 2, 2]           8,640\n",
      "       AvgPool2d-166            [-1, 960, 2, 2]               0\n",
      "     BatchNorm2d-167            [-1, 960, 2, 2]           1,920\n",
      "           ReLU6-168            [-1, 960, 2, 2]               0\n",
      "          Conv2d-169            [-1, 320, 2, 2]         307,200\n",
      "     BatchNorm2d-170            [-1, 320, 2, 2]             640\n",
      "InvertedResidual-171            [-1, 320, 2, 2]               0\n",
      "          Conv2d-172           [-1, 1280, 2, 2]         409,600\n",
      "     BatchNorm2d-173           [-1, 1280, 2, 2]           2,560\n",
      "           ReLU6-174           [-1, 1280, 2, 2]               0\n",
      "          Linear-175                 [-1, 1024]       1,311,744\n",
      "          Linear-176                    [-1, 5]           5,125\n",
      "================================================================\n",
      "Total params: 3,540,165\n",
      "Trainable params: 3,540,165\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 20.37\n",
      "Params size (MB): 13.50\n",
      "Estimated Total Size (MB): 33.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(student, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_root=f\"{cfg.model.model}/{cfg.train_param.supervised_ratio}/{get_datetime()}_{model_func.__name__}\"\n",
    "checkpoint_root = f\"{cfg.model.model}/{cfg.train_param.supervised_ratio}/{model_func.__name__}\"\n",
    "\n",
    "# mea teacher parameters\n",
    "sufix_title = f'_{cfg.mt.ema_alpha}-emaa'\n",
    "sufix_title += f'_{cfg.mt.warmup_length}-wl'\n",
    "sufix_title += f'_{cfg.mt.lambda_cost_max}-lccm'\n",
    "\n",
    "# mixup parameters\n",
    "if cfg.mixup.use:\n",
    "    sufix_title += \"_mixup\"\n",
    "    if cfg.mixup.mixup_max: sufix_title += \"-max\"\n",
    "    if cfg.mixup.mixup_label: sufix_title += \"-label\"\n",
    "    sufix_title += f\"-{cfg.mixup.mixup_alpha}-a\"\n",
    "    \n",
    "# ccost function and method\n",
    "if cfg.mt.ccost_method: sufix_title += \"_cc-MSE\"\n",
    "if cfg.mt.use_softmax: sufix_title += \"-SOFTMAX\"\n",
    "    \n",
    "# normale training parameters\n",
    "sufix_title += f'_{cfg.train_param.learning_rate}-lr'\n",
    "sufix_title += f'_{cfg.train_param.supervised_ratio}-sr'\n",
    "sufix_title += f'_{cfg.train_param.nb_epoch}-e'\n",
    "sufix_title += f'_{cfg.train_param.batch_size}-bs'\n",
    "sufix_title += f'_{cfg.train_param.seed}-seed'\n",
    "\n",
    "tensorboard_title = tensorboard_root + sufix_title\n",
    "checkpoint_title = checkpoint_root + sufix_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tensorboard/ComParE2021_PRS/mean-teacher_mixup/MobileNetV2/0.1/2021-03-19_10:01:58_MobileNetV2_None-emaa_50-wl_None-lccm_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_None-e_128-bs_1234-seed\n"
     ]
    }
   ],
   "source": [
    "tensorboard = mSummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(cfg.dataset.dataset, \"mean-teacher\", student=student, learning_rate=cfg.train_param.learning_rate)\n",
    "callbacks = load_callbacks(cfg.dataset.dataset, \"mean-teacher\", optimizer=optimizer, nb_epoch=cfg.train_param.nb_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint initialise at:  /users/samova/lcances/semi-supervised/model_save/ComParE2021_PRS/mean-teacher_mixup/MobileNetV2/0.1/MobileNetV2_None-emaa_50-wl_None-lccm_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_None-e_128-bs_1234-seed.torch\n",
      "name:  MobileNetV2_None-emaa_50-wl_None-lccm_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_None-e_128-bs_1234-seed.torch\n",
      "mode:  max\n"
     ]
    }
   ],
   "source": [
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\") # Supervised loss\n",
    "\n",
    "if cfg.mt.ccost_method == \"mse\":\n",
    "    consistency_cost = nn.MSELoss(reduction=\"mean\") # Unsupervised loss\n",
    "elif cfg.mt.ccost_method == \"js\":\n",
    "    consistency_cost = JensenShanon\n",
    "        \n",
    "lambda_cost = Warmup(cfg.mt.lambda_ccost_max, cfg.mt.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cost]\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint([student, teacher], optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_ = lambda x: x.mean(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculator():\n",
    "    def c(logits, y):\n",
    "        with torch.no_grad():\n",
    "            y_one_hot = F.one_hot(y, num_classes=cfg.dataset.num_classes)\n",
    "            \n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            arg = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            acc = m_(c.fn.acc(arg, y))\n",
    "            f1 = m_(c.fn.f1(pred, y_one_hot))\n",
    "            \n",
    "            return acc, f1,\n",
    "            \n",
    "    c.fn = DotDict(\n",
    "        acc = CategoricalAccuracy(),\n",
    "        f1 = FScore(),\n",
    "    )\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = DotDict(\n",
    "    calc_student_s_metrics = metrics_calculator(),\n",
    "    calc_student_u_metrics = metrics_calculator(),\n",
    "    calc_teacher_s_metrics = metrics_calculator(),\n",
    "    calc_teacher_u_metrics = metrics_calculator(),\n",
    "    avg_Sce = ContinueAverage(),\n",
    "    avg_Tce = ContinueAverage(),\n",
    "    avg_ccost = ContinueAverage(),\n",
    ")\n",
    "\n",
    "val_metrics = DotDict(\n",
    "    calc_student_s_metrics = metrics_calculator(),\n",
    "    calc_student_u_metrics = metrics_calculator(),\n",
    "    calc_teacher_s_metrics = metrics_calculator(),\n",
    "    calc_teacher_u_metrics = metrics_calculator(),\n",
    "    student_mAP = MAP(),\n",
    "    teacher_mAP = MAP(),\n",
    "    avg_Sce = ContinueAverage(),\n",
    "    avg_Tce = ContinueAverage(),\n",
    "    avg_ccost = ContinueAverage(),\n",
    ")\n",
    "\n",
    "softmax_fn = lambda x: x\n",
    "if cfg.mt.ccost_softmax:\n",
    "    softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "def reset_metrics(dd):\n",
    "    for m in dd.values():\n",
    "        if not isinstance(m, (ContinueAverage, MAP)):\n",
    "            for m_ in m.fn.values():\n",
    "                m_.reset()\n",
    "                \n",
    "        else:\n",
    "            m.reset()\n",
    "    \n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} | {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} - {:<8.6}\"\n",
    "value_form = \"{:<8.8} {:<6d} - {:<6d} - {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} | {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} - {:<8.4f}\"\n",
    "header = header_form.format(\".               \", \"Epoch\",  \"%\", \"Student:\", \"ce\", \"ccost\",\n",
    "                            \"acc_s\", \"f1_s\", \"mAP\", \"AUR\", \"Teacher:\", \"ce\", \"acc_s\", \"f1_s\", \"mAP\", \"AUR\", \"Time\")\n",
    "\n",
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "train_format = value_form\n",
    "val_format = UNDERLINE_SEQ + value_form + RESET_SEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "    \n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "    \n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data,  alpha = 1-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = MixUpBatchShuffle(alpha=cfg.mt.alpha, apply_max=cfg.mt.max, mix_labels=cfg.mt.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch, S, U, start_time):\n",
    "     # aliases\n",
    "    M = metrics\n",
    "    T = tensorboard.add_scalar\n",
    "    \n",
    "    student.train()\n",
    "    \n",
    "    x_s, y_s = S\n",
    "    x_u, y_u = U\n",
    "\n",
    "    # Apply mixup if needed, otherwise no mixup.\n",
    "    n_x_s, n_y_s, n_x_u, n_y_u = x_s, y_s, x_u, y_u\n",
    "    if cfg.mixup.use:\n",
    "        n_x_s, n_y_s = mixup_fn(x_s, y_s)\n",
    "        n_x_u, n_y_u = mixup_fn(x_u, y_u)\n",
    "\n",
    "    n_x_s, n_x_u = n_x_s.cuda(), n_x_u.cuda()\n",
    "    x_s, x_u = x_s.cuda(), x_u.cuda()\n",
    "    y_s, y_u = y_s.cuda(), y_u.cuda()\n",
    "\n",
    "    # Predictions\n",
    "    student_s_logits = student(x_s)        \n",
    "    student_u_logits = student(x_u)\n",
    "    teacher_s_logits = teacher(n_x_s)\n",
    "    teacher_u_logits = teacher(n_x_u)\n",
    "\n",
    "    # Calculate supervised loss (only student on S)\n",
    "    loss = loss_ce(student_s_logits, y_s)\n",
    "\n",
    "    # Calculate consistency cost (mse(student(x), teacher(x))) x is S + U\n",
    "    student_logits = torch.cat((student_s_logits, student_u_logits), dim=0)\n",
    "    teacher_logits = torch.cat((teacher_s_logits, teacher_u_logits), dim=0)\n",
    "    ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "\n",
    "    total_loss = loss + lambda_cost() * ccost\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Teacher prediction (for metrics purpose)\n",
    "        _teacher_loss = loss_ce(teacher_s_logits, y_s)\n",
    "\n",
    "        # Update teacher\n",
    "        update_teacher_model(student, teacher, args.ema_alpha, epoch)\n",
    "\n",
    "        # Compute the metrics for the student\n",
    "        student_s_metrics = M.calc_student_s_metrics(student_s_logits, y_s)\n",
    "        student_u_metrics = M.calc_student_u_metrics(student_u_logits, y_u)\n",
    "        student_s_acc, student_s_f1, student_u_acc, student_u_f1 = *student_s_metrics, *student_u_metrics\n",
    "\n",
    "        # Compute the metrics for the teacher\n",
    "        teacher_s_metrics = M.calc_teacher_s_metrics(teacher_s_logits, y_s)\n",
    "        teacher_u_metrics = M.calc_teacher_u_metrics(teacher_u_logits, y_u)\n",
    "        teacher_s_acc, teacher_s_f1, teacher_u_acc, teacher_u_f1 = *teacher_s_metrics, *teacher_u_metrics\n",
    "\n",
    "        # Running average of the two losses\n",
    "        student_running_loss = m_(M.avg_Sce(loss.item()))\n",
    "        teacher_running_loss = m_(M.avg_Tce(_teacher_loss.item()))\n",
    "        running_ccost = m_(M.avg_ccost(ccost.item()))\n",
    "\n",
    "        # logs\n",
    "        print(train_format.format(\n",
    "            \"Training: \", epoch + 1, cfg.train_param.nb_iteration,\n",
    "            \"\", student_running_loss, running_ccost, student_s_acc, student_s_f1, 0.0, 0.0,\n",
    "            \"\", teacher_running_loss, teacher_s_acc, teacher_s_f1, 0.0, 0.0,\n",
    "            time.time() - start_time),\n",
    "            end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_acc_s\", student_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_acc_u\", student_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_s\", student_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_u\", student_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/teacher_acc_s\", teacher_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_u\", teacher_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_s\", teacher_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_u\", teacher_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/consistency_cost\", running_ccost, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    # aliases\n",
    "    M = val_metrics\n",
    "    uar_avg = ContinueAverage()\n",
    "    mAP_avg = ContinueAverage()\n",
    "        \n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics(val_metrics)\n",
    "    student.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Predictions\n",
    "            student_logits = student(X)        \n",
    "            teacher_logits = teacher(X)\n",
    "\n",
    "            # Calculate supervised loss (only student on S)\n",
    "            loss = loss_ce(student_logits, y)\n",
    "            _teacher_loss = loss_ce(teacher_logits, y) # for metrics only\n",
    "            ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "            \n",
    "            # Compute the metrics\n",
    "            y_one_hot = F.one_hot(y, num_classes=cfg.dataset.num_classes)\n",
    "            \n",
    "            # ---- student ----\n",
    "            student_metrics = M.calc_student_s_metrics(student_logits, y)\n",
    "            student_acc, student_f1 = student_metrics\n",
    "            \n",
    "            # ---- teacher ----\n",
    "            teacher_metrics = M.calc_teacher_s_metrics(teacher_logits, y)\n",
    "            teacher_acc, teacher_f1 = teacher_metrics\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = m_(M.avg_Sce(loss.item()))\n",
    "            teacher_running_loss = m_(M.avg_Tce(_teacher_loss.item()))\n",
    "            running_ccost = m_(M.avg_ccost(ccost.item()))\n",
    "            \n",
    "            student_pred = F.one_hot(torch.argmax(student_logits, dim=1) ,num_classes=cfg.dataset.num_classes)\n",
    "            teacher_pred = F.one_hot(torch.argmax(teacher_logits, dim=1) ,num_classes=cfg.dataset.num_classes)\n",
    "            val_student_mAP = m_(M.student_mAP(student_pred.cpu().reshape(-1), y_one_hot.cpu().reshape(-1)))\n",
    "            val_teacher_mAP = m_(M.teacher_mAP(teacher_pred.cpu().reshape(-1), y_one_hot.cpu().reshape(-1)))\n",
    "\n",
    "            # logs\n",
    "            print(val_format.format(\n",
    "                \"Validation: \", epoch + 1, int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", student_running_loss, running_ccost, student_acc, student_f1, val_student_mAP, 0.0,\n",
    "                \"\", teacher_running_loss, teacher_acc, teacher_f1, val_teacher_mAP, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/student_acc\", student_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_f1\", student_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_mAP\", val_student_mAP, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_acc\", teacher_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_f1\", teacher_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_mAP\", val_teacher_mAP, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/consistency_cost\", running_ccost, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    tensorboard.add_scalar(\"hyperparameters/lambda_cost_max\", lambda_cost(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/student_acc\", maximum_tracker(\"student_acc\", student_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_acc\", maximum_tracker(\"teacher_acc\", teacher_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/student_f1\", maximum_tracker(\"student_f1\", student_f1), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_f1\", maximum_tracker(\"teacher_f1\", teacher_f1), epoch )\n",
    "\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "        \n",
    "    return val_teacher_mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     mAP      AUR      | Teacher:   ce       acc_s    f1_s     mAP      AUR      - Time    \n",
      "Training 500    - 75000  -            0.7739   1.8919   0.7008   0.6787   0.0000   0.0000   |            0.7235   0.7200   0.6834   0.0000   0.0000   - 180.3780\n",
      "\u001b[1;4mValidati 501    - 100    -            0.5515   1.0535   0.3412   0.3225   0.2544   0.0000   |            0.6353   0.2921   0.2755   0.2075   0.0000   - 2.0570  \u001b[0m\n",
      "\n",
      " better performance: saving ...\n",
      "Training 1000   - 75000  -            0.5407   1.7824   0.7958   0.7877   0.0000   0.0000   |            0.5141   0.8192   0.8061   0.0000   0.0000   - 267.8175\n",
      "\u001b[1;4mValidati 1001   - 100    -            0.4890   0.9291   0.3555   0.3449   0.2699   0.0000   |            0.5698   0.3317   0.3150   0.2449   0.0000   - 2.0784  \u001b[0m\n",
      "\n",
      " better performance: saving ...\n",
      "Training 1500   - 75000  -            0.4026   1.6589   0.8767   0.8706   0.0000   0.0000   |            0.3829   0.8692   0.8683   0.0000   0.0000   - 355.4184\n",
      "\u001b[1;4mValidati 1501   - 100    -            0.4712   1.0211   0.3896   0.3863   0.3088   0.0000   |            0.5211   0.3568   0.3550   0.2720   0.0000   - 2.0242  \u001b[0m\n",
      "\n",
      " better performance: saving ...\n",
      "Training 1738   - 75000  -            0.4051   1.5954   0.8625   0.8685   0.0000   0.0000   |            0.3072   0.8992   0.8989   0.0000   0.0000   - 398.4651\r"
     ]
    }
   ],
   "source": [
    "if cfg.train_param.resume:\n",
    "    checkpoint.load_last()\n",
    "\n",
    "start_iteration = checkpoint.epoch_counter\n",
    "end_iteration = cfg.train_param.nb_iteration\n",
    "\n",
    "train_iterator = iter(train_loader)\n",
    "start_time = time.time()\n",
    "\n",
    "print(header)\n",
    "for e in range(start_iteration, end_iteration):\n",
    "    # Validation every 500 iteration\n",
    "    if e % 500 == 0:\n",
    "        val_teacher_mAP = val(e)\n",
    "        print('')\n",
    "        checkpoint.step(val_teacher_mAP)\n",
    "        tensorboard.flush()\n",
    "\n",
    "    train(e, *next(train_iterator), start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the hyper parameters and the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1aec3e38df9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m final_metrics = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc_student\": maximum_tracker.max[\"student_acc\"],\n",
    "    \"max_f1_student\": maximum_tracker.max[\"student_f1\"],\n",
    "    \"max_acc_teacher\": maximum_tracker.max[\"teacher_acc\"],\n",
    "    \"max_f1_teacher\": maximum_tracker.max[\"teacher_f1\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSL.dataset_loader.ComParE2021_PRS import ComParE2021_PRS\n",
    "from torch.utils.data import DataLoader\n",
    "from metric_utils.metrics import MAP\n",
    "from sklearn.metrics import recall_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ComParE2021_PRS(root='../../datasets', subset='test', transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=cfg.hardware.nb_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state_dict', 'optimizer', 'epoch', 'best_metric']\n"
     ]
    }
   ],
   "source": [
    "checkpoint.load_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 / 55\r"
     ]
    }
   ],
   "source": [
    "student.eval()\n",
    "S = nn.Softmax(dim=1)\n",
    "nb_batch = len(val_loader)\n",
    "\n",
    "all_pred = []\n",
    "all_targets = []\n",
    "\n",
    "for i, (x, y) in enumerate(val_loader):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "    pred = torch.argmax(S(student(x)), dim=1)\n",
    "    pred = F.one_hot(pred, num_classes=cfg.dataset.num_classes)\n",
    "    y = F.one_hot(y, num_classes=cfg.dataset.num_classes)\n",
    "    \n",
    "    all_pred.append(pred.detach().cpu())\n",
    "    all_targets.append(y.detach().cpu())\n",
    "    \n",
    "    print(f'{i} / {nb_batch}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_ = torch.vstack(all_pred)\n",
    "all_targets_ = torch.vstack(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "uar = recall_score(all_targets_, all_pred_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = average_precision_score(all_targets_, all_pred_, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uar:  0.5746229771610213\n",
      "mAP:  0.4657394601030007\n"
     ]
    }
   ],
   "source": [
    "print('uar: ', uar)\n",
    "print('mAP: ', mAP.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
